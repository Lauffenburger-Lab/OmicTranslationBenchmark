{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a038ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division\n",
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from trainingUtils import MultipleOptimizer, MultipleScheduler, compute_kernel, compute_mmd\n",
    "from models import Decoder, SimpleEncoder,LocalDiscriminator,PriorDiscriminator,Classifier,SpeciesCovariate\n",
    "# import argparse\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "import random\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import silhouette_score,confusion_matrix\n",
    "from scipy.stats import spearmanr,pearsonr\n",
    "from evaluationUtils import r_square,get_cindex,pearson_r,pseudoAccuracy\n",
    "import seaborn as sns\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dfd3c5f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "80cbcf0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1d7fd6e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c0620a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a train generators\n",
    "def getSamples(N, batchSize):\n",
    "    order = np.random.permutation(N)\n",
    "    outList = []\n",
    "    while len(order)>0:\n",
    "        outList.append(order[0:batchSize])\n",
    "        order = order[batchSize:]\n",
    "    return outList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4ac83d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradients(output, input):\n",
    "    grads = torch.autograd.grad(output, input, create_graph=True)\n",
    "    grads = grads[0].pow(2).mean()\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c08c515",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "371016b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data\n",
    "cmap = pd.read_csv('../preprocessing/preprocessed_data/all_cmap_landmarks.csv',index_col = 0)\n",
    "gene_size = len(cmap.columns)\n",
    "samples = cmap.index.values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "009666d8",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "13985346",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_params = {'encoder_1_hiddens':[640,384],\n",
    "                'encoder_2_hiddens':[640,384],\n",
    "                'latent_dim': 292,\n",
    "                #'decoder_hiddens':[384,640],\n",
    "                'decoder_1_hiddens':[384,640],\n",
    "                'decoder_2_hiddens':[384,640],\n",
    "                'dropout_decoder':0.2,\n",
    "                'dropout_encoder':0.1,\n",
    "                'encoder_activation':torch.nn.ELU(),\n",
    "                'decoder_activation':torch.nn.ELU(),\n",
    "                'intermediateEncoder1':[292],\n",
    "                'intermediateEncoder2':[292],\n",
    "                'intermediate_latent':256,\n",
    "                'intermediate_dropout':0.1,\n",
    "                'state_class_hidden':[256,128,64],\n",
    "                'state_class_drop_in':0.5,\n",
    "                'state_class_drop':0.25,\n",
    "                'no_states':2,\n",
    "                'adv_class_hidden':[256,128,64],\n",
    "                'adv_class_drop_in':0.3,\n",
    "                'adv_class_drop':0.1,\n",
    "                'no_adv_class':2,\n",
    "                'adv_lr':0.001,\n",
    "                'schedule_step_adv':200,\n",
    "                'gamma_adv':0.5,\n",
    "                'adv_penalnty':100,\n",
    "                'reg_adv':1000,\n",
    "                'encoding_lr':0.001,\n",
    "                'schedule_step_enc':200,\n",
    "                'gamma_enc':0.8,\n",
    "                'batch_size_1':150,\n",
    "                'batch_size_2':150,\n",
    "                'batch_size_paired':90,\n",
    "                'epochs':1000,\n",
    "                'prior_beta':1.0,\n",
    "                'no_folds':10,\n",
    "                'intermediate_reg':1e-02,\n",
    "                'state_class_reg':1e-02,\n",
    "                'enc_l2_reg':0.01,\n",
    "                'dec_l2_reg':0.01,\n",
    "                'lambda_mi_loss':100,\n",
    "                'cosine_loss': 10,\n",
    "                'reg_classifier': 1000,\n",
    "                'similarity_reg' : 10,\n",
    "                'autoencoder_wd':0,\n",
    "                'adversary_wd':0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "21f76da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_criterion = torch.nn.CrossEntropyLoss()\n",
    "NUM_EPOCHS= model_params['epochs']\n",
    "bs_1 = model_params['batch_size_1']\n",
    "bs_2 =  model_params['batch_size_2']\n",
    "bs_paired =  model_params['batch_size_paired']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "91ed7d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pretrained_adv_class = torch.load('../results/MI_results/models/CPA_approach/pre_trained_classifier_adverse_0.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1144bf5d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split 1: Epoch=1/1000, r2_1=-4.3338, pearson_1=0.1514, MSE_1=3.1442, r2_2=-3.9001, pearson_2=0.1425, MSE_2=2.8816, MI Loss=1.0680, Prior Loss=0.5935, Entropy Loss=0.6696, Adverse Entropy=0.6156, Cosine Loss=0.0130, loss=10002.3945, F1 latent=0.6411, F1 basal=0.7869, F1 basal trained=0.7417, MI Loss intermediate=1.2805\n",
      "Split 1: Epoch=501/1000, r2_1=0.4322, pearson_1=0.7873, MSE_1=1.2150, r2_2=0.3814, pearson_2=0.8010, MSE_2=1.2360, MI Loss=-0.6544, Prior Loss=0.0000, Entropy Loss=0.3136, Adverse Entropy=0.6356, Cosine Loss=0.5249, loss=2045.3291, F1 latent=1.0000, F1 basal=0.8681, F1 basal trained=0.8491, MI Loss intermediate=-0.6542\n",
      "Split 1: Epoch=1000/1000, r2_1=0.4804, pearson_1=0.8005, MSE_1=1.1638, r2_2=0.5421, pearson_2=0.7957, MSE_2=0.9534, MI Loss=-0.6692, Prior Loss=0.0000, Entropy Loss=0.3138, Adverse Entropy=0.6389, Cosine Loss=0.6336, loss=1690.0963, F1 latent=1.0000, F1 basal=0.8421, F1 basal trained=0.8591, MI Loss intermediate=-0.6624\n",
      "Classification accuracy: 1.0\n",
      "Classification F1 score: 1.0\n",
      "Pearson correlation 1: 0.7954875826835632\n",
      "Spearman correlation 1: 0.72670057843199\n",
      "Pseudo-Accuracy 1: 0.7730231765507838\n",
      "Pearson correlation 2: 0.8108107447624207\n",
      "Spearman correlation 2: 0.7471460521628527\n",
      "Pseudo-Accuracy 2: 0.7832961517010597\n",
      "Pearson of direct translation: 0.5206855535507202\n",
      "Pearson correlation 1 to 2: 0.7449458241462708\n",
      "Pseudo accuracy 1 to 2: 0.7304959100204499\n",
      "Pearson correlation 2 to 1: 0.647800862789154\n",
      "Pseudo accuracy 2 to 1: 0.714340490797546\n",
      "Split 2: Epoch=1/1000, r2_1=-5.0083, pearson_1=0.1700, MSE_1=3.5634, r2_2=-4.3174, pearson_2=0.1901, MSE_2=3.1405, MI Loss=1.0681, Prior Loss=0.6839, Entropy Loss=0.6343, Adverse Entropy=0.7355, Cosine Loss=0.0614, loss=10029.1836, F1 latent=0.7424, F1 basal=0.5474, F1 basal trained=0.5464, MI Loss intermediate=1.0419\n",
      "Split 2: Epoch=501/1000, r2_1=0.4750, pearson_1=0.8068, MSE_1=1.1555, r2_2=0.4152, pearson_2=0.7706, MSE_2=1.1597, MI Loss=-0.6640, Prior Loss=0.0000, Entropy Loss=0.3135, Adverse Entropy=0.6195, Cosine Loss=0.5313, loss=1937.3058, F1 latent=1.0000, F1 basal=0.8651, F1 basal trained=0.8819, MI Loss intermediate=-0.6563\n",
      "Split 2: Epoch=1000/1000, r2_1=0.5029, pearson_1=0.8048, MSE_1=1.1085, r2_2=0.5152, pearson_2=0.7901, MSE_2=0.9601, MI Loss=-0.6680, Prior Loss=0.0000, Entropy Loss=0.3133, Adverse Entropy=0.6387, Cosine Loss=0.5837, loss=1643.9802, F1 latent=1.0000, F1 basal=0.8562, F1 basal trained=0.8581, MI Loss intermediate=-0.6609\n",
      "Classification accuracy: 1.0\n",
      "Classification F1 score: 1.0\n",
      "Pearson correlation 1: 0.7844271063804626\n",
      "Spearman correlation 1: 0.7094851282879543\n",
      "Pseudo-Accuracy 1: 0.7651584867075665\n",
      "Pearson correlation 2: 0.8011834621429443\n",
      "Spearman correlation 2: 0.7299796430167284\n",
      "Pseudo-Accuracy 2: 0.7757482803495075\n",
      "Pearson of direct translation: 0.4794166088104248\n",
      "Pearson correlation 1 to 2: 0.6937435865402222\n",
      "Pseudo accuracy 1 to 2: 0.7102505112474438\n",
      "Pearson correlation 2 to 1: 0.685378909111023\n",
      "Pseudo accuracy 2 to 1: 0.7130879345603272\n",
      "Split 3: Epoch=1/1000, r2_1=-4.6697, pearson_1=0.1582, MSE_1=3.3752, r2_2=-4.2634, pearson_2=0.1881, MSE_2=3.1018, MI Loss=1.0498, Prior Loss=0.6698, Entropy Loss=0.5958, Adverse Entropy=0.6392, Cosine Loss=0.0590, loss=10849.7881, F1 latent=0.7848, F1 basal=0.7479, F1 basal trained=0.7386, MI Loss intermediate=1.1901\n",
      "Split 3: Epoch=501/1000, r2_1=0.4686, pearson_1=0.8012, MSE_1=1.2236, r2_2=0.4711, pearson_2=0.8115, MSE_2=1.0636, MI Loss=-0.6593, Prior Loss=0.0000, Entropy Loss=0.3140, Adverse Entropy=0.6178, Cosine Loss=0.4967, loss=1910.5889, F1 latent=1.0000, F1 basal=0.8639, F1 basal trained=0.8542, MI Loss intermediate=-0.6623\n",
      "Split 3: Epoch=1000/1000, r2_1=0.4820, pearson_1=0.8008, MSE_1=1.1226, r2_2=0.4432, pearson_2=0.8026, MSE_2=1.0335, MI Loss=-0.6668, Prior Loss=0.0000, Entropy Loss=0.3185, Adverse Entropy=0.6423, Cosine Loss=0.3946, loss=1744.0679, F1 latent=0.9962, F1 basal=0.8649, F1 basal trained=0.8522, MI Loss intermediate=-0.6613\n",
      "Classification accuracy: 1.0\n",
      "Classification F1 score: 1.0\n",
      "Pearson correlation 1: 0.8004616498947144\n",
      "Spearman correlation 1: 0.7308798769146719\n",
      "Pseudo-Accuracy 1: 0.7741990456714383\n",
      "Pearson correlation 2: 0.7732523679733276\n",
      "Spearman correlation 2: 0.7117098232293813\n",
      "Pseudo-Accuracy 2: 0.7646681539319576\n",
      "Pearson of direct translation: 0.5004222393035889\n",
      "Pearson correlation 1 to 2: 0.7893423438072205\n",
      "Pseudo accuracy 1 to 2: 0.743021472392638\n",
      "Pearson correlation 2 to 1: 0.7432382702827454\n",
      "Pseudo accuracy 2 to 1: 0.7408997955010225\n",
      "Split 4: Epoch=1/1000, r2_1=-4.5273, pearson_1=0.1788, MSE_1=3.3134, r2_2=-4.4165, pearson_2=0.1559, MSE_2=3.1600, MI Loss=1.0179, Prior Loss=0.6475, Entropy Loss=0.6902, Adverse Entropy=0.5769, Cosine Loss=0.0580, loss=10205.0498, F1 latent=0.5930, F1 basal=0.8519, F1 basal trained=0.8382, MI Loss intermediate=1.2298\n",
      "Split 4: Epoch=501/1000, r2_1=0.3791, pearson_1=0.7579, MSE_1=1.2649, r2_2=0.4431, pearson_2=0.7791, MSE_2=1.0992, MI Loss=-0.6536, Prior Loss=0.0000, Entropy Loss=0.3137, Adverse Entropy=0.6326, Cosine Loss=0.5245, loss=2050.9653, F1 latent=1.0000, F1 basal=0.8581, F1 basal trained=0.8432, MI Loss intermediate=-0.6547\n",
      "Split 4: Epoch=1000/1000, r2_1=0.5201, pearson_1=0.8178, MSE_1=1.0937, r2_2=0.4227, pearson_2=0.8032, MSE_2=1.0731, MI Loss=-0.6678, Prior Loss=0.0000, Entropy Loss=0.3133, Adverse Entropy=0.6309, Cosine Loss=0.5161, loss=1802.9017, F1 latent=1.0000, F1 basal=0.8669, F1 basal trained=0.8552, MI Loss intermediate=-0.6644\n",
      "Classification accuracy: 1.0\n",
      "Classification F1 score: 1.0\n",
      "Pearson correlation 1: 0.791576087474823\n",
      "Spearman correlation 1: 0.708253937593625\n",
      "Pseudo-Accuracy 1: 0.7658145875937287\n",
      "Pearson correlation 2: 0.7880823016166687\n",
      "Spearman correlation 2: 0.7115292747312498\n",
      "Pseudo-Accuracy 2: 0.7698364008179959\n",
      "Pearson of direct translation: 0.44552382826805115\n",
      "Pearson correlation 1 to 2: 0.6943235397338867\n",
      "Pseudo accuracy 1 to 2: 0.7035787321063396\n",
      "Pearson correlation 2 to 1: 0.6643461585044861\n",
      "Pseudo accuracy 2 to 1: 0.7061860940695297\n",
      "Split 5: Epoch=1/1000, r2_1=-4.6284, pearson_1=0.1819, MSE_1=3.4407, r2_2=-5.4170, pearson_2=0.2042, MSE_2=3.9396, MI Loss=1.0851, Prior Loss=0.6639, Entropy Loss=0.6354, Adverse Entropy=0.7365, Cosine Loss=0.0683, loss=10944.4316, F1 latent=0.7130, F1 basal=0.4946, F1 basal trained=0.4783, MI Loss intermediate=1.2370\n",
      "Split 5: Epoch=501/1000, r2_1=0.4576, pearson_1=0.8095, MSE_1=1.2244, r2_2=0.4157, pearson_2=0.7817, MSE_2=1.1912, MI Loss=-0.6592, Prior Loss=0.0000, Entropy Loss=0.3136, Adverse Entropy=0.6325, Cosine Loss=0.6427, loss=2038.4771, F1 latent=1.0000, F1 basal=0.8542, F1 basal trained=0.8414, MI Loss intermediate=-0.6550\n",
      "Split 5: Epoch=1000/1000, r2_1=0.5228, pearson_1=0.8163, MSE_1=1.1041, r2_2=0.5333, pearson_2=0.7733, MSE_2=1.0144, MI Loss=-0.6690, Prior Loss=0.0000, Entropy Loss=0.3135, Adverse Entropy=0.6309, Cosine Loss=0.5296, loss=1707.1865, F1 latent=1.0000, F1 basal=0.8385, F1 basal trained=0.8483, MI Loss intermediate=-0.6608\n",
      "Classification accuracy: 1.0\n",
      "Classification F1 score: 1.0\n",
      "Pearson correlation 1: 0.7982794046401978\n",
      "Spearman correlation 1: 0.7183996481057885\n",
      "Pseudo-Accuracy 1: 0.769972733469666\n",
      "Pearson correlation 2: 0.8126439452171326\n",
      "Spearman correlation 2: 0.7586412443492127\n",
      "Pseudo-Accuracy 2: 0.7955846811675031\n",
      "Pearson of direct translation: 0.5428716540336609\n",
      "Pearson correlation 1 to 2: 0.7360649108886719\n",
      "Pseudo accuracy 1 to 2: 0.7237985685071575\n",
      "Pearson correlation 2 to 1: 0.704152524471283\n",
      "Pseudo accuracy 2 to 1: 0.7151329243353783\n",
      "Split 6: Epoch=1/1000, r2_1=-5.2955, pearson_1=0.1800, MSE_1=3.7943, r2_2=-4.1339, pearson_2=0.1814, MSE_2=3.1088, MI Loss=1.1183, Prior Loss=0.7021, Entropy Loss=0.6472, Adverse Entropy=0.7432, Cosine Loss=0.0761, loss=10230.5312, F1 latent=0.7189, F1 basal=0.5492, F1 basal trained=0.4396, MI Loss intermediate=1.4481\n",
      "Split 6: Epoch=501/1000, r2_1=0.4358, pearson_1=0.7910, MSE_1=1.2592, r2_2=0.4244, pearson_2=0.8017, MSE_2=1.2405, MI Loss=-0.6573, Prior Loss=0.0000, Entropy Loss=0.3146, Adverse Entropy=0.6255, Cosine Loss=0.5739, loss=2146.0891, F1 latent=1.0000, F1 basal=0.8643, F1 basal trained=0.8662, MI Loss intermediate=-0.6528\n",
      "Split 6: Epoch=1000/1000, r2_1=0.4955, pearson_1=0.8132, MSE_1=1.1815, r2_2=0.5079, pearson_2=0.7968, MSE_2=1.0328, MI Loss=-0.6712, Prior Loss=0.0000, Entropy Loss=0.3136, Adverse Entropy=0.6262, Cosine Loss=0.5744, loss=1808.1831, F1 latent=1.0000, F1 basal=0.8532, F1 basal trained=0.8814, MI Loss intermediate=-0.6677\n",
      "Classification accuracy: 1.0\n",
      "Classification F1 score: 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson correlation 1: 0.8018448352813721\n",
      "Spearman correlation 1: 0.7378217895920429\n",
      "Pseudo-Accuracy 1: 0.7819444444444446\n",
      "Pearson correlation 2: 0.8123874664306641\n",
      "Spearman correlation 2: 0.7528041325355981\n",
      "Pseudo-Accuracy 2: 0.7941160066926938\n",
      "Pearson of direct translation: 0.45602622628211975\n",
      "Pearson correlation 1 to 2: 0.6820381879806519\n",
      "Pseudo accuracy 1 to 2: 0.7231339468302659\n",
      "Pearson correlation 2 to 1: 0.6436372995376587\n",
      "Pseudo accuracy 2 to 1: 0.7098670756646216\n",
      "Split 7: Epoch=1/1000, r2_1=-5.2122, pearson_1=0.1580, MSE_1=3.7346, r2_2=-4.6452, pearson_2=0.1960, MSE_2=3.4156, MI Loss=1.0880, Prior Loss=0.6546, Entropy Loss=0.7006, Adverse Entropy=0.6934, Cosine Loss=0.0549, loss=11163.3682, F1 latent=0.5941, F1 basal=0.6757, F1 basal trained=0.5962, MI Loss intermediate=1.2161\n",
      "Split 7: Epoch=501/1000, r2_1=0.3724, pearson_1=0.8120, MSE_1=1.4254, r2_2=0.4378, pearson_2=0.7825, MSE_2=1.0924, MI Loss=-0.6553, Prior Loss=0.0000, Entropy Loss=0.3136, Adverse Entropy=0.6202, Cosine Loss=0.5854, loss=2149.8159, F1 latent=1.0000, F1 basal=0.8767, F1 basal trained=0.8729, MI Loss intermediate=-0.6552\n",
      "Split 7: Epoch=1000/1000, r2_1=0.5391, pearson_1=0.8346, MSE_1=1.1818, r2_2=0.4858, pearson_2=0.8031, MSE_2=1.0257, MI Loss=-0.6652, Prior Loss=0.0000, Entropy Loss=0.3133, Adverse Entropy=0.6373, Cosine Loss=0.6044, loss=1778.0183, F1 latent=1.0000, F1 basal=0.8542, F1 basal trained=0.8639, MI Loss intermediate=-0.6578\n",
      "Classification accuracy: 1.0\n",
      "Classification F1 score: 1.0\n",
      "Pearson correlation 1: 0.7866775989532471\n",
      "Spearman correlation 1: 0.725984787043366\n",
      "Pseudo-Accuracy 1: 0.7737389229720518\n",
      "Pearson correlation 2: 0.8130702972412109\n",
      "Spearman correlation 2: 0.7477480580795942\n",
      "Pseudo-Accuracy 2: 0.7912994980479646\n",
      "Pearson of direct translation: 0.3912511467933655\n",
      "Pearson correlation 1 to 2: 0.6458916068077087\n",
      "Pseudo accuracy 1 to 2: 0.6913343558282208\n",
      "Pearson correlation 2 to 1: 0.5686815977096558\n",
      "Pseudo accuracy 2 to 1: 0.6787832310838446\n",
      "Split 8: Epoch=1/1000, r2_1=-5.3022, pearson_1=0.1898, MSE_1=3.8393, r2_2=-4.0829, pearson_2=0.1980, MSE_2=2.9882, MI Loss=1.0704, Prior Loss=0.6368, Entropy Loss=0.6052, Adverse Entropy=0.6147, Cosine Loss=0.0641, loss=10646.4658, F1 latent=0.7851, F1 basal=0.7984, F1 basal trained=0.7717, MI Loss intermediate=1.0828\n",
      "Split 8: Epoch=501/1000, r2_1=0.4174, pearson_1=0.7822, MSE_1=1.2683, r2_2=0.4591, pearson_2=0.7981, MSE_2=1.1560, MI Loss=-0.6486, Prior Loss=0.0000, Entropy Loss=0.3134, Adverse Entropy=0.6240, Cosine Loss=0.5720, loss=2074.5361, F1 latent=1.0000, F1 basal=0.8582, F1 basal trained=0.8521, MI Loss intermediate=-0.6510\n",
      "Split 8: Epoch=1000/1000, r2_1=0.5085, pearson_1=0.8193, MSE_1=1.1613, r2_2=0.5166, pearson_2=0.8218, MSE_2=1.0377, MI Loss=-0.6702, Prior Loss=0.0000, Entropy Loss=0.3135, Adverse Entropy=0.6398, Cosine Loss=0.6222, loss=1777.5924, F1 latent=1.0000, F1 basal=0.8562, F1 basal trained=0.8493, MI Loss intermediate=-0.6639\n",
      "Classification accuracy: 1.0\n",
      "Classification F1 score: 1.0\n",
      "Pearson correlation 1: 0.7804854512214661\n",
      "Spearman correlation 1: 0.7287089630397364\n",
      "Pseudo-Accuracy 1: 0.7787576687116565\n",
      "Pearson correlation 2: 0.8276018500328064\n",
      "Spearman correlation 2: 0.7737800727131497\n",
      "Pseudo-Accuracy 2: 0.8064324223833426\n",
      "Pearson of direct translation: 0.4556507170200348\n",
      "Pearson correlation 1 to 2: 0.6947680711746216\n",
      "Pseudo accuracy 1 to 2: 0.7307515337423313\n",
      "Pearson correlation 2 to 1: 0.6482517719268799\n",
      "Pseudo accuracy 2 to 1: 0.7143149284253579\n",
      "Split 9: Epoch=1/1000, r2_1=-4.0824, pearson_1=0.1905, MSE_1=3.2096, r2_2=-3.7538, pearson_2=0.2017, MSE_2=2.8570, MI Loss=1.2218, Prior Loss=0.7738, Entropy Loss=0.6679, Adverse Entropy=0.8136, Cosine Loss=0.0083, loss=9509.9707, F1 latent=0.6699, F1 basal=0.4253, F1 basal trained=0.3392, MI Loss intermediate=1.1675\n",
      "Split 9: Epoch=501/1000, r2_1=0.4343, pearson_1=0.7886, MSE_1=1.2633, r2_2=0.4075, pearson_2=0.7642, MSE_2=1.2155, MI Loss=-0.6650, Prior Loss=0.0000, Entropy Loss=0.3134, Adverse Entropy=0.6329, Cosine Loss=0.6414, loss=2079.0042, F1 latent=1.0000, F1 basal=0.8621, F1 basal trained=0.8542, MI Loss intermediate=-0.6550\n",
      "Split 9: Epoch=1000/1000, r2_1=0.5035, pearson_1=0.7932, MSE_1=1.1279, r2_2=0.4980, pearson_2=0.8169, MSE_2=1.0840, MI Loss=-0.6699, Prior Loss=0.0000, Entropy Loss=0.3133, Adverse Entropy=0.6426, Cosine Loss=0.5543, loss=1810.3420, F1 latent=1.0000, F1 basal=0.8591, F1 basal trained=0.8767, MI Loss intermediate=-0.6652\n",
      "Classification accuracy: 1.0\n",
      "Classification F1 score: 1.0\n",
      "Pearson correlation 1: 0.7852794528007507\n",
      "Spearman correlation 1: 0.732775589610356\n",
      "Pseudo-Accuracy 1: 0.7776073619631901\n",
      "Pearson correlation 2: 0.7977802753448486\n",
      "Spearman correlation 2: 0.7440233624344126\n",
      "Pseudo-Accuracy 2: 0.7857036623907792\n",
      "Pearson of direct translation: 0.3736864924430847\n",
      "Pearson correlation 1 to 2: 0.6403435468673706\n",
      "Pseudo accuracy 1 to 2: 0.6774795501022496\n",
      "Pearson correlation 2 to 1: 0.5761745572090149\n",
      "Pseudo accuracy 2 to 1: 0.6740797546012269\n",
      "Split 10: Epoch=1/1000, r2_1=-3.9507, pearson_1=0.1406, MSE_1=2.9416, r2_2=-3.4385, pearson_2=0.1730, MSE_2=2.6292, MI Loss=1.0512, Prior Loss=0.6772, Entropy Loss=0.6010, Adverse Entropy=0.6317, Cosine Loss=0.0129, loss=9736.8408, F1 latent=0.7773, F1 basal=0.7325, F1 basal trained=0.7206, MI Loss intermediate=1.1577\n",
      "Split 10: Epoch=501/1000, r2_1=0.3958, pearson_1=0.7987, MSE_1=1.3225, r2_2=0.3605, pearson_2=0.7698, MSE_2=1.3280, MI Loss=-0.6504, Prior Loss=0.0000, Entropy Loss=0.3140, Adverse Entropy=0.6340, Cosine Loss=0.6191, loss=2266.3672, F1 latent=1.0000, F1 basal=0.8201, F1 basal trained=0.8581, MI Loss intermediate=-0.6556\n",
      "Split 10: Epoch=1000/1000, r2_1=0.4895, pearson_1=0.8100, MSE_1=1.1783, r2_2=0.4868, pearson_2=0.7791, MSE_2=1.0075, MI Loss=-0.6696, Prior Loss=0.0000, Entropy Loss=0.3134, Adverse Entropy=0.6400, Cosine Loss=0.5996, loss=1790.8160, F1 latent=1.0000, F1 basal=0.8532, F1 basal trained=0.8658, MI Loss intermediate=-0.6659\n",
      "Classification accuracy: 1.0\n",
      "Classification F1 score: 1.0\n",
      "Pearson correlation 1: 0.770980715751648\n",
      "Spearman correlation 1: 0.717916326490269\n",
      "Pseudo-Accuracy 1: 0.7706714383094752\n",
      "Pearson correlation 2: 0.7859881520271301\n",
      "Spearman correlation 2: 0.7242630984384772\n",
      "Pseudo-Accuracy 2: 0.7724298196690834\n",
      "Pearson of direct translation: 0.4139716327190399\n",
      "Pearson correlation 1 to 2: 0.63853520154953\n",
      "Pseudo accuracy 1 to 2: 0.69079754601227\n",
      "Pearson correlation 2 to 1: 0.6018168330192566\n",
      "Pseudo accuracy 2 to 1: 0.6923057259713701\n"
     ]
    }
   ],
   "source": [
    "valR2 = []\n",
    "valPear = []\n",
    "valMSE =[]\n",
    "valSpear = []\n",
    "valAccuracy = []\n",
    "\n",
    "\n",
    "valPearDirect = []\n",
    "valSpearDirect = []\n",
    "valAccDirect = []\n",
    "\n",
    "valR2_1 = []\n",
    "valPear_1 = []\n",
    "valMSE_1 =[]\n",
    "valSpear_1 = []\n",
    "valAccuracy_1 = []\n",
    "\n",
    "valR2_2 = []\n",
    "valPear_2 = []\n",
    "valMSE_2 =[]\n",
    "valSpear_2 = []\n",
    "valAccuracy_2 = []\n",
    "\n",
    "crossCorrelation = []\n",
    "\n",
    "valF1 = []\n",
    "valClassAcc = []\n",
    "\n",
    "for i in range(10):\n",
    "    # Network\n",
    "    decoder_1 = Decoder(model_params['intermediate_latent'],model_params['decoder_1_hiddens'],gene_size,\n",
    "                        dropRate=model_params['dropout_decoder'], \n",
    "                        activation=model_params['decoder_activation']).to(device)\n",
    "    decoder_2 = Decoder(model_params['intermediate_latent'],model_params['decoder_2_hiddens'],gene_size,\n",
    "                        dropRate=model_params['dropout_decoder'], \n",
    "                        activation=model_params['decoder_activation']).to(device)\n",
    "    #decoder = Decoder(model_params['intermediate_latent'],model_params['decoder_hiddens'],gene_size,\n",
    "    #                    dropRate=model_params['dropout_decoder'], \n",
    "    #                    activation=model_params['decoder_activation']).to(device)\n",
    "    encoder_1 = SimpleEncoder(gene_size,model_params['encoder_1_hiddens'],model_params['latent_dim'],\n",
    "                              dropRate=model_params['dropout_encoder'], \n",
    "                              activation=model_params['encoder_activation']).to(device)\n",
    "    encoder_2 = SimpleEncoder(gene_size,model_params['encoder_2_hiddens'],model_params['latent_dim'],\n",
    "                                  dropRate=model_params['dropout_encoder'], \n",
    "                                  activation=model_params['encoder_activation']).to(device)\n",
    "    prior_d = PriorDiscriminator(model_params['latent_dim']).to(device)\n",
    "    local_d = LocalDiscriminator(model_params['latent_dim'],model_params['latent_dim']).to(device)\n",
    "    classifier = Classifier(in_channel=model_params['intermediate_latent'],\n",
    "                            hidden_layers=model_params['state_class_hidden'],\n",
    "                            num_classes=model_params['no_states'],\n",
    "                            drop_in=model_params['state_class_drop_in'],\n",
    "                            drop=model_params['state_class_drop']).to(device)\n",
    "    encoder_interm_1 = SimpleEncoder(model_params['latent_dim'],\n",
    "                                     model_params['intermediateEncoder1'],\n",
    "                                     model_params['intermediate_latent'],\n",
    "                                     dropRate=model_params['intermediate_dropout'], \n",
    "                                     activation=model_params['encoder_activation']).to(device)\n",
    "    encoder_interm_2 = SimpleEncoder(model_params['latent_dim'],\n",
    "                                     model_params['intermediateEncoder2'],\n",
    "                                     model_params['intermediate_latent'],\n",
    "                                     dropRate=model_params['intermediate_dropout'], \n",
    "                                     activation=model_params['encoder_activation']).to(device)\n",
    "    prior_d_2 = PriorDiscriminator(model_params['intermediate_latent']).to(device)\n",
    "    local_d_2 = LocalDiscriminator(model_params['intermediate_latent'],model_params['intermediate_latent']).to(device)\n",
    "    adverse_classifier = Classifier(in_channel=model_params['latent_dim'],\n",
    "                                    hidden_layers=model_params['adv_class_hidden'],\n",
    "                                    num_classes=model_params['no_adv_class'],\n",
    "                                    drop_in=model_params['adv_class_drop_in'],\n",
    "                                    drop=model_params['adv_class_drop']).to(device)\n",
    "    #adverse_classifier.load_state_dict(pretrained_adv_class.state_dict())\n",
    "    \n",
    "    trainInfo_paired = pd.read_csv('../preprocessing/preprocessed_data/10fold_validation_spit/train_paired_%s.csv'%i,index_col=0)\n",
    "    trainInfo_1 = pd.read_csv('../preprocessing/preprocessed_data/10fold_validation_spit/train_a375_%s.csv'%i,index_col=0)\n",
    "    trainInfo_2 = pd.read_csv('../preprocessing/preprocessed_data/10fold_validation_spit/train_ht29_%s.csv'%i,index_col=0)\n",
    "    \n",
    "    valInfo_paired = pd.read_csv('../preprocessing/preprocessed_data/10fold_validation_spit/val_paired_%s.csv'%i,index_col=0)\n",
    "    valInfo_1 = pd.read_csv('../preprocessing/preprocessed_data/10fold_validation_spit/val_a375_%s.csv'%i,index_col=0)\n",
    "    valInfo_2 = pd.read_csv('../preprocessing/preprocessed_data/10fold_validation_spit/val_ht29_%s.csv'%i,index_col=0)\n",
    "    \n",
    "    N_paired = len(trainInfo_paired)\n",
    "    N_1 = len(trainInfo_1)\n",
    "    N_2 = len(trainInfo_2)\n",
    "    N = N_1\n",
    "    if N_2>N:\n",
    "        N=N_2\n",
    "    \n",
    "    allParams = list(decoder_1.parameters()) +list(decoder_2.parameters())\n",
    "    #allParams = list(decoder.parameters())\n",
    "    allParams = allParams + list(encoder_1.parameters()) +list(encoder_2.parameters())\n",
    "    allParams = allParams + list(encoder_interm_1.parameters()) +list(encoder_interm_2.parameters())\n",
    "    allParams = allParams + list(prior_d.parameters()) + list(local_d.parameters())\n",
    "    allParams = allParams + list(prior_d_2.parameters()) + list(local_d_2.parameters())\n",
    "    allParams = allParams + list(classifier.parameters())\n",
    "    optimizer = torch.optim.Adam(allParams, lr= model_params['encoding_lr'],\n",
    "                                 weight_decay=model_params['autoencoder_wd'])\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n",
    "                                                step_size=model_params['schedule_step_enc'],\n",
    "                                                gamma=model_params['gamma_enc'])\n",
    "    optimizer_adv = torch.optim.Adam(adverse_classifier.parameters(), lr= model_params['adv_lr'], \n",
    "                                     weight_decay=model_params['adversary_wd'])\n",
    "    \n",
    "    scheduler_adv = torch.optim.lr_scheduler.StepLR(optimizer_adv,\n",
    "                                                    step_size=model_params['schedule_step_adv'],\n",
    "                                                    gamma=model_params['gamma_adv'])\n",
    "    for e in range(0, NUM_EPOCHS):\n",
    "        decoder_1.train()\n",
    "        decoder_2.train()\n",
    "        encoder_1.train()\n",
    "        encoder_2.train()\n",
    "        encoder_interm_1.train()\n",
    "        encoder_interm_2.train()\n",
    "        prior_d.train()\n",
    "        local_d.train()\n",
    "        classifier.train()\n",
    "        adverse_classifier.train()\n",
    "        prior_d_2.train()\n",
    "        local_d_2.train()\n",
    "        \n",
    "        trainloader_1 = getSamples(N_1, bs_1)\n",
    "        len_1 = len(trainloader_1)\n",
    "        trainloader_2 = getSamples(N_2, bs_2)\n",
    "        len_2 = len(trainloader_2)\n",
    "        trainloader_paired = getSamples(N_paired, bs_paired)\n",
    "        len_paired = len(trainloader_paired)\n",
    "\n",
    "        lens = [len_1,len_2,len_paired]\n",
    "        maxLen = np.max(lens)\n",
    "        \n",
    "        iteration = 1\n",
    "\n",
    "        if maxLen>lens[0]:\n",
    "            trainloader_suppl = getSamples(N_1, bs_1)\n",
    "            for jj in range(maxLen-lens[0]):\n",
    "                trainloader_1.insert(jj,trainloader_suppl[jj])\n",
    "        \n",
    "        if maxLen>lens[1]:\n",
    "            trainloader_suppl = getSamples(N_2, bs_2)\n",
    "            for jj in range(maxLen-lens[1]):\n",
    "                trainloader_2.insert(jj,trainloader_suppl[jj])\n",
    "        \n",
    "        if maxLen>lens[2]:\n",
    "            trainloader_suppl = getSamples(N_paired, bs_paired)\n",
    "            for jj in range(maxLen-lens[2]):\n",
    "                trainloader_paired.insert(jj,trainloader_suppl[jj])\n",
    "        #for dataIndex in trainloader:\n",
    "        for j in range(maxLen):\n",
    "            dataIndex_1 = trainloader_1[j]\n",
    "            dataIndex_2 = trainloader_2[j]\n",
    "            dataIndex_paired = trainloader_paired[j]\n",
    "            \n",
    "            df_pairs = trainInfo_paired.iloc[dataIndex_paired,:]\n",
    "            df_1 = trainInfo_1.iloc[dataIndex_1,:]\n",
    "            df_2 = trainInfo_2.iloc[dataIndex_2,:]\n",
    "            paired_inds = len(df_pairs)\n",
    "            \n",
    "            \n",
    "            X_1 = torch.tensor(np.concatenate((cmap.loc[df_pairs['sig_id.x']].values,\n",
    "                                                 cmap.loc[df_1.sig_id].values))).float().to(device)\n",
    "            X_2 = torch.tensor(np.concatenate((cmap.loc[df_pairs['sig_id.y']].values,\n",
    "                                                 cmap.loc[df_2.sig_id].values))).float().to(device)\n",
    "            \n",
    "            \n",
    "            conditions = np.concatenate((df_pairs.conditionId.values,\n",
    "                                         df_1.conditionId.values,\n",
    "                                         df_pairs.conditionId.values,\n",
    "                                         df_2.conditionId.values))\n",
    "            size = conditions.size\n",
    "            conditions = conditions.reshape(size,1)\n",
    "            conditions = conditions == conditions.transpose()\n",
    "            conditions = conditions*1\n",
    "            mask = torch.tensor(conditions).to(device).detach()\n",
    "            pos_mask = mask\n",
    "            neg_mask = 1 - mask\n",
    "            log_2 = math.log(2.)\n",
    "            optimizer.zero_grad()\n",
    "            optimizer_adv.zero_grad()\n",
    "            \n",
    "            z_base_1 = encoder_1(X_1)\n",
    "            z_base_2 = encoder_2(X_2)\n",
    "            latent_base_vectors = torch.cat((z_base_1, z_base_2), 0)\n",
    "            labels_adv = adverse_classifier(latent_base_vectors)\n",
    "            true_labels = torch.cat((torch.ones(z_base_1.shape[0]),\n",
    "                                         torch.zeros(z_base_2.shape[0])),0).long().to(device)\n",
    "            _, predicted = torch.max(labels_adv, 1)\n",
    "            predicted = predicted.cpu().numpy()\n",
    "            cf_matrix = confusion_matrix(true_labels.cpu().numpy(),predicted)\n",
    "            tn, fp, fn, tp = cf_matrix.ravel()\n",
    "            f1_basal_trained = 2*tp/(2*tp+fp+fn)\n",
    "            adv_entropy = class_criterion(labels_adv,true_labels)\n",
    "            adversary_drugs_penalty = compute_gradients(labels_adv.sum(), latent_base_vectors)\n",
    "            loss_adv = adv_entropy + model_params['adv_penalnty'] * adversary_drugs_penalty\n",
    "            loss_adv.backward()\n",
    "            optimizer_adv.step()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            z_base_1 = encoder_1(X_1)\n",
    "            z_base_2 = encoder_2(X_2)\n",
    "            latent_base_vectors = torch.cat((z_base_1, z_base_2), 0)\n",
    "            \n",
    "            z_un = local_d(latent_base_vectors)\n",
    "            res_un = torch.matmul(z_un, z_un.t())\n",
    "            \n",
    "            z_1 = encoder_interm_1(z_base_1)\n",
    "            z_2 = encoder_interm_2(z_base_2)\n",
    "            latent_vectors = torch.cat((z_1, z_2), 0)\n",
    "            z_un_interm = local_d_2(latent_vectors)\n",
    "            res_un_interm = torch.matmul(z_un_interm, z_un_interm.t())\n",
    "            \n",
    "            y_pred_1 = decoder_1(z_1)\n",
    "            fitLoss_1 = torch.mean(torch.sum((y_pred_1 - X_1)**2,dim=1))\n",
    "            L2Loss_1 = encoder_1.L2Regularization(model_params['enc_l2_reg']) + decoder_1.L2Regularization(model_params['dec_l2_reg'])\n",
    "            loss_1 = fitLoss_1 + L2Loss_1\n",
    "            \n",
    "            y_pred_2 = decoder_2(z_2)\n",
    "            fitLoss_2 = torch.mean(torch.sum((y_pred_2 - X_2)**2,dim=1))\n",
    "            L2Loss_2 = encoder_2.L2Regularization(model_params['enc_l2_reg']) + decoder_2.L2Regularization(model_params['dec_l2_reg'])\n",
    "            loss_2 = fitLoss_2 + L2Loss_2\n",
    "            \n",
    "            silimalityLoss = torch.mean(torch.sum((z_base_1[0:paired_inds,:] - z_base_2[0:paired_inds,:])**2,dim=-1))\n",
    "            cosineLoss = torch.nn.functional.cosine_similarity(z_base_1[0:paired_inds,:],z_base_2[0:paired_inds,:],dim=-1).mean()\n",
    "            \n",
    "            p_samples = res_un * pos_mask.float()\n",
    "            q_samples = res_un * neg_mask.float()\n",
    "            Ep = log_2 - F.softplus(- p_samples)\n",
    "            Eq = F.softplus(-q_samples) + q_samples - log_2\n",
    "            Ep = (Ep * pos_mask.float()).sum() / pos_mask.float().sum()\n",
    "            Eq = (Eq * neg_mask.float()).sum() / neg_mask.float().sum()\n",
    "            mi_loss = Eq - Ep\n",
    "            prior = torch.rand_like(latent_base_vectors)\n",
    "            term_a = torch.log(prior_d(prior)).mean()\n",
    "            term_b = torch.log(1.0 - prior_d(latent_base_vectors)).mean()\n",
    "            prior_loss = -(term_a + term_b) * model_params['prior_beta']\n",
    "            \n",
    "            p_samples_interm = res_un_interm * pos_mask.float()\n",
    "            q_samples_interm = res_un_interm * neg_mask.float()\n",
    "            Ep_interm = log_2 - F.softplus(- p_samples_interm)\n",
    "            Eq_interm = F.softplus(-q_samples_interm) + q_samples_interm - log_2\n",
    "            Ep_interm = (Ep_interm * pos_mask.float()).sum() / pos_mask.float().sum()\n",
    "            Eq_interm = (Eq_interm * neg_mask.float()).sum() / neg_mask.float().sum()\n",
    "            mi_loss_interm = Eq_interm - Ep_interm\n",
    "            prior_interm = torch.rand_like(latent_vectors)\n",
    "            term_a_interm = torch.log(prior_d_2(prior_interm)).mean()\n",
    "            term_b_interm = torch.log(1.0 - prior_d_2(latent_vectors)).mean()\n",
    "            prior_loss_interm = -(term_a_interm + term_b_interm) * model_params['prior_beta']\n",
    "\n",
    "            # Classification loss\n",
    "            labels = classifier(latent_vectors)\n",
    "            true_labels = torch.cat((torch.ones(z_1.shape[0]),\n",
    "                                     torch.zeros(z_2.shape[0])),0).long().to(device)\n",
    "            entropy = class_criterion(labels,true_labels)\n",
    "            _, predicted = torch.max(labels, 1)\n",
    "            predicted = predicted.cpu().numpy()\n",
    "            cf_matrix = confusion_matrix(true_labels.cpu().numpy(),predicted)\n",
    "            tn, fp, fn, tp = cf_matrix.ravel()\n",
    "            f1_latent = 2*tp/(2*tp+fp+fn)\n",
    "            \n",
    "            # Remove signal from z_basal\n",
    "            labels_adv = adverse_classifier(latent_base_vectors)\n",
    "            true_labels = torch.cat((torch.ones(z_base_1.shape[0]),\n",
    "                                     torch.zeros(z_base_2.shape[0])),0).long().to(device)\n",
    "            adv_entropy = class_criterion(labels_adv,true_labels)\n",
    "            _, predicted = torch.max(labels_adv, 1)\n",
    "            predicted = predicted.cpu().numpy()\n",
    "            cf_matrix = confusion_matrix(true_labels.cpu().numpy(),predicted)\n",
    "            tn, fp, fn, tp = cf_matrix.ravel()\n",
    "            f1_basal = 2*tp/(2*tp+fp+fn)\n",
    "            \n",
    "            #decoder.L2Regularization(model_params['dec_l2_reg'])\n",
    "            loss = loss_1 + loss_2 + model_params['similarity_reg'] * silimalityLoss +\\\n",
    "                   model_params['lambda_mi_loss']*mi_loss_interm + prior_loss_interm +\\\n",
    "                   model_params['lambda_mi_loss']*mi_loss + prior_loss +\\\n",
    "                   model_params['reg_classifier']*entropy - model_params['reg_adv']*adv_entropy +\\\n",
    "                   classifier.L2Regularization(model_params['state_class_reg']) +\\\n",
    "                   encoder_interm_1.L2Regularization(model_params['intermediate_reg'])+\\\n",
    "                   encoder_interm_2.L2Regularization(model_params['intermediate_reg'])-\\\n",
    "                   model_params['cosine_loss'] * cosineLoss\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        \n",
    "            pearson_1 = pearson_r(y_pred_1.detach().flatten(), X_1.detach().flatten())\n",
    "            r2_1 = r_square(y_pred_1.detach().flatten(), X_1.detach().flatten())\n",
    "            mse_1 = torch.mean(torch.mean((y_pred_1.detach() - X_1.detach())**2,dim=1))\n",
    "        \n",
    "            pearson_2 = pearson_r(y_pred_2.detach().flatten(), X_2.detach().flatten())\n",
    "            r2_2 = r_square(y_pred_2.detach().flatten(), X_2.detach().flatten())\n",
    "            mse_2 = torch.mean(torch.mean((y_pred_2.detach() - X_2.detach())**2,dim=1))\n",
    "        \n",
    "        scheduler_adv.step()\n",
    "        if (e>=0):\n",
    "            scheduler.step()\n",
    "            outString = 'Split {:.0f}: Epoch={:.0f}/{:.0f}'.format(i+1,e+1,NUM_EPOCHS)\n",
    "            outString += ', r2_1={:.4f}'.format(r2_1.item())\n",
    "            outString += ', pearson_1={:.4f}'.format(pearson_1.item())\n",
    "            outString += ', MSE_1={:.4f}'.format(mse_1.item())\n",
    "            outString += ', r2_2={:.4f}'.format(r2_2.item())\n",
    "            outString += ', pearson_2={:.4f}'.format(pearson_2.item())\n",
    "            outString += ', MSE_2={:.4f}'.format(mse_2.item())\n",
    "            outString += ', MI Loss={:.4f}'.format(mi_loss.item())\n",
    "            outString += ', Prior Loss={:.4f}'.format(prior_loss.item())\n",
    "            outString += ', Entropy Loss={:.4f}'.format(entropy.item())\n",
    "            outString += ', Adverse Entropy={:.4f}'.format(adv_entropy.item())\n",
    "            outString += ', Cosine Loss={:.4f}'.format(cosineLoss.item())\n",
    "            outString += ', loss={:.4f}'.format(loss.item())\n",
    "            outString += ', F1 latent={:.4f}'.format(f1_latent)\n",
    "            outString += ', F1 basal={:.4f}'.format(f1_basal)\n",
    "            outString += ', F1 basal trained={:.4f}'.format(f1_basal_trained)\n",
    "            outString += ', MI Loss intermediate={:.4f}'.format(mi_loss_interm.item())\n",
    "        if (e==0 or (e%500==0 and e>0)):\n",
    "            print(outString)\n",
    "    print(outString)\n",
    "    #trainLoss.append(splitLoss)\n",
    "    decoder_1.eval()\n",
    "    decoder_2.eval()\n",
    "    encoder_1.eval()\n",
    "    encoder_2.eval()\n",
    "    encoder_interm_1.eval()\n",
    "    encoder_interm_2.eval()\n",
    "    prior_d.eval()\n",
    "    local_d.eval()\n",
    "    classifier.eval()\n",
    "    adverse_classifier.eval()\n",
    "    prior_d_2.eval()\n",
    "    local_d_2.eval()\n",
    "    \n",
    "    paired_val_inds = len(valInfo_paired)\n",
    "    x_1 = torch.tensor(np.concatenate((cmap.loc[valInfo_paired['sig_id.x']].values,\n",
    "                                          cmap.loc[valInfo_1.sig_id].values))).float().to(device)\n",
    "    x_2 = torch.tensor(np.concatenate((cmap.loc[valInfo_paired['sig_id.y']].values,\n",
    "                                          cmap.loc[valInfo_2.sig_id].values))).float().to(device)\n",
    "    \n",
    "    z_latent_base_1 = encoder_1(x_1)\n",
    "    z_latent_base_2 = encoder_2(x_2)\n",
    "    \n",
    "    z_latent_1 = encoder_interm_1(z_latent_base_1)\n",
    "    z_latent_2 = encoder_interm_2(z_latent_base_2)\n",
    "    \n",
    "    labels = classifier(torch.cat((z_latent_1, z_latent_2), 0))\n",
    "    true_labels = torch.cat((torch.ones(z_latent_1.shape[0]).view(z_latent_1.shape[0],1),\n",
    "                             torch.zeros(z_latent_2.shape[0]).view(z_latent_2.shape[0],1)),0).long()\n",
    "    _, predicted = torch.max(labels, 1)\n",
    "    predicted = predicted.cpu().numpy()\n",
    "    cf_matrix = confusion_matrix(true_labels.numpy(),predicted)\n",
    "    tn, fp, fn, tp = cf_matrix.ravel()\n",
    "    class_acc = (tp+tn)/predicted.size\n",
    "    f1 = 2*tp/(2*tp+fp+fn)\n",
    "    \n",
    "    valF1.append(f1)\n",
    "    valClassAcc.append(class_acc)\n",
    "    \n",
    "    print('Classification accuracy: %s'%class_acc)\n",
    "    print('Classification F1 score: %s'%f1)\n",
    "\n",
    "    xhat_1 = decoder_1(z_latent_1)\n",
    "    xhat_2 = decoder_2(z_latent_2)\n",
    "\n",
    "    \n",
    "    r2_1 = r_square(xhat_1.detach().flatten(), x_1.detach().flatten())\n",
    "    pearson_1 = pearson_r(xhat_1.detach().flatten(), x_1.detach().flatten())\n",
    "    mse_1 = torch.mean(torch.mean((xhat_1 - x_1)**2,dim=1))\n",
    "    r2_2 = r_square(xhat_2.detach().flatten(), x_2.detach().flatten())\n",
    "    pearson_2 = pearson_r(xhat_2.detach().flatten(), x_2.detach().flatten())\n",
    "    mse_2 = torch.mean(torch.mean((xhat_2 - x_2)**2,dim=1))\n",
    "    rhos = []\n",
    "    for jj in range(xhat_1.shape[0]):\n",
    "        rho,p = spearmanr(x_1[jj,:].detach().cpu().numpy(),xhat_1[jj,:].detach().cpu().numpy())\n",
    "        rhos.append(rho)\n",
    "    valSpear_1.append(np.mean(rhos))\n",
    "    acc = pseudoAccuracy(x_1.detach().cpu(),xhat_1.detach().cpu(),eps=1e-6)\n",
    "    valAccuracy_1.append(np.mean(acc))\n",
    "    rhos = []\n",
    "    for jj in range(xhat_2.shape[0]):\n",
    "        rho,p = spearmanr(x_2[jj,:].detach().cpu().numpy(),xhat_2[jj,:].detach().cpu().numpy())\n",
    "        rhos.append(rho)\n",
    "    valSpear_2.append(np.mean(rhos))\n",
    "    acc = pseudoAccuracy(x_2.detach().cpu(),xhat_2.detach().cpu(),eps=1e-6)\n",
    "    valAccuracy_2.append(np.mean(acc))\n",
    "    \n",
    "    valR2_1.append(r2_1.item())\n",
    "    valPear_1.append(pearson_1.item())\n",
    "    valMSE_1.append(mse_1.item())\n",
    "    valR2_2.append(r2_2.item())\n",
    "    valPear_2.append(pearson_2.item())\n",
    "    valMSE_2.append(mse_2.item())\n",
    "    #print('R^2 1: %s'%r2_1.item())\n",
    "    print('Pearson correlation 1: %s'%pearson_1.item())\n",
    "    print('Spearman correlation 1: %s'%valSpear_1[i])\n",
    "    print('Pseudo-Accuracy 1: %s'%valAccuracy_1[i])\n",
    "    #print('R^2 2: %s'%r2_2.item())\n",
    "    print('Pearson correlation 2: %s'%pearson_2.item())\n",
    "    print('Spearman correlation 2: %s'%valSpear_2[i])\n",
    "    print('Pseudo-Accuracy 2: %s'%valAccuracy_2[i])\n",
    "    \n",
    "    x_1_equivalent = x_1[0:paired_val_inds,:]\n",
    "    x_2_equivalent = x_2[0:paired_val_inds,:]\n",
    "        \n",
    "    pearDirect = pearson_r(x_1_equivalent.detach().flatten(), x_2_equivalent.detach().flatten())\n",
    "    rhos = []\n",
    "    for jj in range(x_1_equivalent.shape[0]):\n",
    "        rho,p = spearmanr(x_1_equivalent[jj,:].detach().cpu().numpy(),x_2_equivalent[jj,:].detach().cpu().numpy())\n",
    "        rhos.append(rho)\n",
    "    spearDirect = np.mean(rhos)\n",
    "    accDirect_2 = np.mean(pseudoAccuracy(x_2_equivalent.detach().cpu(),x_1_equivalent.detach().cpu(),eps=1e-6))\n",
    "    accDirect_1 = np.mean(pseudoAccuracy(x_1_equivalent.detach().cpu(),x_2_equivalent.detach().cpu(),eps=1e-6))\n",
    "\n",
    "    z_latent_base_1_equivalent  = encoder_1(x_1_equivalent)\n",
    "    z_latent_1_equivalent = encoder_interm_2(z_latent_base_1_equivalent)\n",
    "    x_hat_2_equivalent = decoder_2(z_latent_1_equivalent).detach()\n",
    "    r2_2 = r_square(x_hat_2_equivalent.detach().flatten(), x_2_equivalent.detach().flatten())\n",
    "    pearson_2 = pearson_r(x_hat_2_equivalent.detach().flatten(), x_2_equivalent.detach().flatten())\n",
    "    mse_2 = torch.mean(torch.mean((x_hat_2_equivalent - x_2_equivalent)**2,dim=1))\n",
    "    rhos = []\n",
    "    for jj in range(x_hat_2_equivalent.shape[0]):\n",
    "        rho,p = spearmanr(x_2_equivalent[jj,:].detach().cpu().numpy(),x_hat_2_equivalent[jj,:].detach().cpu().numpy())\n",
    "        rhos.append(rho)\n",
    "    rho_2 = np.mean(rhos)\n",
    "    acc_2 = np.mean(pseudoAccuracy(x_2_equivalent.detach().cpu(),x_hat_2_equivalent.detach().cpu(),eps=1e-6))\n",
    "    print('Pearson of direct translation: %s'%pearDirect.item())\n",
    "    print('Pearson correlation 1 to 2: %s'%pearson_2.item())\n",
    "    print('Pseudo accuracy 1 to 2: %s'%acc_2)\n",
    "\n",
    "    z_latent_base_2_equivalent  = encoder_2(x_2_equivalent)\n",
    "    z_latent_2_equivalent = encoder_interm_1(z_latent_base_2_equivalent)\n",
    "    x_hat_1_equivalent = decoder_1(z_latent_2_equivalent).detach()\n",
    "    r2_1 = r_square(x_hat_1_equivalent.detach().flatten(), x_1_equivalent.detach().flatten())\n",
    "    pearson_1 = pearson_r(x_hat_1_equivalent.detach().flatten(), x_1_equivalent.detach().flatten())\n",
    "    mse_1 = torch.mean(torch.mean((x_hat_1_equivalent - x_1_equivalent)**2,dim=1))\n",
    "    rhos = []\n",
    "    for jj in range(x_hat_1_equivalent.shape[0]):\n",
    "        rho,p = spearmanr(x_1_equivalent[jj,:].detach().cpu().numpy(),x_hat_1_equivalent[jj,:].detach().cpu().numpy())\n",
    "        rhos.append(rho)\n",
    "    rho_1 = np.mean(rhos)\n",
    "    acc_1 = np.mean(pseudoAccuracy(x_1_equivalent.detach().cpu(),x_hat_1_equivalent.detach().cpu(),eps=1e-6))\n",
    "    print('Pearson correlation 2 to 1: %s'%pearson_1.item())\n",
    "    print('Pseudo accuracy 2 to 1: %s'%acc_1)\n",
    "    \n",
    "    \n",
    "    valPear.append([pearson_2.item(),pearson_1.item()])\n",
    "    valSpear.append([rho_2,rho_1])\n",
    "    valAccuracy.append([acc_2,acc_1])\n",
    "    \n",
    "    valPearDirect.append(pearDirect.item())\n",
    "    valSpearDirect.append(spearDirect)\n",
    "    valAccDirect.append([accDirect_2,accDirect_1])\n",
    "    \n",
    "    torch.save(decoder_1,'../results/MI_results/models/IntermediateTranslationLandmarks2Dec_1000ep/decoder_a375_%s.pt'%i)\n",
    "    torch.save(decoder_2,'../results/MI_results/models/IntermediateTranslationLandmarks2Dec_1000ep/decoder_ht29_%s.pt'%i)\n",
    "    torch.save(prior_d,'../results/MI_results/models/IntermediateTranslationLandmarks2Dec_1000ep/priorDiscr_%s.pt'%i)\n",
    "    torch.save(local_d,'../results/MI_results/models/IntermediateTranslationLandmarks2Dec_1000ep/localDiscr_%s.pt'%i)\n",
    "    torch.save(prior_d_2,'../results/MI_results/models/IntermediateTranslationLandmarks2Dec_1000ep/priorDiscr_interm_%s.pt'%i)\n",
    "    torch.save(local_d_2,'../results/MI_results/models/IntermediateTranslationLandmarks2Dec_1000ep/localDiscr_interm_%s.pt'%i)\n",
    "    torch.save(encoder_1,'../results/MI_results/models/IntermediateTranslationLandmarks2Dec_1000ep/encoder_a375_%s.pt'%i)\n",
    "    torch.save(encoder_2,'../results/MI_results/models/IntermediateTranslationLandmarks2Dec_1000ep/encoder_ht29_%s.pt'%i)\n",
    "    torch.save(encoder_interm_1,'../results/MI_results/models/IntermediateTranslationLandmarks2Dec_1000ep/encoder_interm_a375_%s.pt'%i)\n",
    "    torch.save(encoder_interm_2,'../results/MI_results/models/IntermediateTranslationLandmarks2Dec_1000ep/encoder_interm_ht29_%s.pt'%i)\n",
    "    torch.save(classifier,'../results/MI_results/models/IntermediateTranslationLandmarks2Dec_1000ep/classifier_%s.pt'%i)\n",
    "    torch.save(adverse_classifier,'../results/MI_results/models/IntermediateTranslationLandmarks2Dec_1000ep/classifier_adverse_%s.pt'%i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "07c4c0eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<seaborn.axisgrid.FacetGrid at 0x1e33edd3070>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.displot(torch.cat((z_1, z_2), 0).flatten().detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "040a245b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<seaborn.axisgrid.FacetGrid at 0x1e3be085fd0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.displot(torch.cat((z_base_1, z_base_2), 0).flatten().detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "46e3612e",
   "metadata": {},
   "outputs": [],
   "source": [
    "valPear = np.array(valPear)\n",
    "valPearDirect = np.array(valPearDirect)\n",
    "crossCorrelation = np.array(crossCorrelation)\n",
    "valSpear = np.array(valSpear)\n",
    "valAccuracy= np.array(valAccuracy)\n",
    "valSpearDirect= np.array(valSpearDirect)\n",
    "valAccDirect= np.array(valAccDirect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b2f16c28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.69599968 0.64834788]\n",
      "0.45795060992240905\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(valPear,axis=0))\n",
    "print(np.mean(valPearDirect))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dbf5c5fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.60461412 0.57966662]\n",
      "0.40821883215419613\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(valSpear,axis=0))\n",
    "print(np.mean(valSpearDirect))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8690386a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.71246421 0.7058998 ]\n",
      "[0.64107618 0.64107618]\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(valAccuracy,axis=0))\n",
    "print(np.mean(valAccDirect,axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ac0688ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(valF1))\n",
    "print(np.mean(valClassAcc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5cd80bed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>F1_score</th>\n",
       "      <th>ClassAccuracy</th>\n",
       "      <th>model_pearsonHT29</th>\n",
       "      <th>model_pearsonA375</th>\n",
       "      <th>model_spearHT29</th>\n",
       "      <th>model_spearA375</th>\n",
       "      <th>model_accHT29</th>\n",
       "      <th>model_accA375</th>\n",
       "      <th>recon_pear_ht29</th>\n",
       "      <th>recon_pear_a375</th>\n",
       "      <th>recon_spear_ht29</th>\n",
       "      <th>recon_spear_a375</th>\n",
       "      <th>recon_acc_ht29</th>\n",
       "      <th>recon_acc_a375</th>\n",
       "      <th>Direct_pearson</th>\n",
       "      <th>Direct_spearman</th>\n",
       "      <th>DirectAcc_ht29</th>\n",
       "      <th>DirectAcc_a375</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.744946</td>\n",
       "      <td>0.647801</td>\n",
       "      <td>0.659541</td>\n",
       "      <td>0.600114</td>\n",
       "      <td>0.730496</td>\n",
       "      <td>0.714340</td>\n",
       "      <td>0.810811</td>\n",
       "      <td>0.795488</td>\n",
       "      <td>0.747146</td>\n",
       "      <td>0.726701</td>\n",
       "      <td>0.783296</td>\n",
       "      <td>0.773023</td>\n",
       "      <td>0.520686</td>\n",
       "      <td>0.463031</td>\n",
       "      <td>0.662730</td>\n",
       "      <td>0.662730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.693744</td>\n",
       "      <td>0.685379</td>\n",
       "      <td>0.598586</td>\n",
       "      <td>0.590397</td>\n",
       "      <td>0.710251</td>\n",
       "      <td>0.713088</td>\n",
       "      <td>0.801183</td>\n",
       "      <td>0.784427</td>\n",
       "      <td>0.729980</td>\n",
       "      <td>0.709485</td>\n",
       "      <td>0.775748</td>\n",
       "      <td>0.765158</td>\n",
       "      <td>0.479417</td>\n",
       "      <td>0.420304</td>\n",
       "      <td>0.649847</td>\n",
       "      <td>0.649847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.789342</td>\n",
       "      <td>0.743238</td>\n",
       "      <td>0.691196</td>\n",
       "      <td>0.666481</td>\n",
       "      <td>0.743021</td>\n",
       "      <td>0.740900</td>\n",
       "      <td>0.773252</td>\n",
       "      <td>0.800462</td>\n",
       "      <td>0.711710</td>\n",
       "      <td>0.730880</td>\n",
       "      <td>0.764668</td>\n",
       "      <td>0.774199</td>\n",
       "      <td>0.500422</td>\n",
       "      <td>0.432608</td>\n",
       "      <td>0.644836</td>\n",
       "      <td>0.644836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.694324</td>\n",
       "      <td>0.664346</td>\n",
       "      <td>0.575898</td>\n",
       "      <td>0.570811</td>\n",
       "      <td>0.703579</td>\n",
       "      <td>0.706186</td>\n",
       "      <td>0.788082</td>\n",
       "      <td>0.791576</td>\n",
       "      <td>0.711529</td>\n",
       "      <td>0.708254</td>\n",
       "      <td>0.769836</td>\n",
       "      <td>0.765815</td>\n",
       "      <td>0.445524</td>\n",
       "      <td>0.393393</td>\n",
       "      <td>0.635890</td>\n",
       "      <td>0.635890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.736065</td>\n",
       "      <td>0.704153</td>\n",
       "      <td>0.634498</td>\n",
       "      <td>0.618352</td>\n",
       "      <td>0.723799</td>\n",
       "      <td>0.715133</td>\n",
       "      <td>0.812644</td>\n",
       "      <td>0.798279</td>\n",
       "      <td>0.758641</td>\n",
       "      <td>0.718400</td>\n",
       "      <td>0.795585</td>\n",
       "      <td>0.769973</td>\n",
       "      <td>0.542872</td>\n",
       "      <td>0.442503</td>\n",
       "      <td>0.653144</td>\n",
       "      <td>0.653144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.682038</td>\n",
       "      <td>0.643637</td>\n",
       "      <td>0.611858</td>\n",
       "      <td>0.582229</td>\n",
       "      <td>0.723134</td>\n",
       "      <td>0.709867</td>\n",
       "      <td>0.812387</td>\n",
       "      <td>0.801845</td>\n",
       "      <td>0.752804</td>\n",
       "      <td>0.737822</td>\n",
       "      <td>0.794116</td>\n",
       "      <td>0.781944</td>\n",
       "      <td>0.456026</td>\n",
       "      <td>0.433163</td>\n",
       "      <td>0.654525</td>\n",
       "      <td>0.654525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.645892</td>\n",
       "      <td>0.568682</td>\n",
       "      <td>0.555579</td>\n",
       "      <td>0.520047</td>\n",
       "      <td>0.691334</td>\n",
       "      <td>0.678783</td>\n",
       "      <td>0.813070</td>\n",
       "      <td>0.786678</td>\n",
       "      <td>0.747748</td>\n",
       "      <td>0.725985</td>\n",
       "      <td>0.791299</td>\n",
       "      <td>0.773739</td>\n",
       "      <td>0.391251</td>\n",
       "      <td>0.353916</td>\n",
       "      <td>0.618405</td>\n",
       "      <td>0.618405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.694768</td>\n",
       "      <td>0.648252</td>\n",
       "      <td>0.642995</td>\n",
       "      <td>0.606206</td>\n",
       "      <td>0.730752</td>\n",
       "      <td>0.714315</td>\n",
       "      <td>0.827602</td>\n",
       "      <td>0.780485</td>\n",
       "      <td>0.773780</td>\n",
       "      <td>0.728709</td>\n",
       "      <td>0.806432</td>\n",
       "      <td>0.778758</td>\n",
       "      <td>0.455651</td>\n",
       "      <td>0.433182</td>\n",
       "      <td>0.652428</td>\n",
       "      <td>0.652428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.640344</td>\n",
       "      <td>0.576175</td>\n",
       "      <td>0.519874</td>\n",
       "      <td>0.501132</td>\n",
       "      <td>0.677480</td>\n",
       "      <td>0.674080</td>\n",
       "      <td>0.797780</td>\n",
       "      <td>0.785279</td>\n",
       "      <td>0.744023</td>\n",
       "      <td>0.732776</td>\n",
       "      <td>0.785704</td>\n",
       "      <td>0.777607</td>\n",
       "      <td>0.373686</td>\n",
       "      <td>0.337688</td>\n",
       "      <td>0.610148</td>\n",
       "      <td>0.610148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.638535</td>\n",
       "      <td>0.601817</td>\n",
       "      <td>0.556115</td>\n",
       "      <td>0.540897</td>\n",
       "      <td>0.690798</td>\n",
       "      <td>0.692306</td>\n",
       "      <td>0.785988</td>\n",
       "      <td>0.770981</td>\n",
       "      <td>0.724263</td>\n",
       "      <td>0.717916</td>\n",
       "      <td>0.772430</td>\n",
       "      <td>0.770671</td>\n",
       "      <td>0.413972</td>\n",
       "      <td>0.372399</td>\n",
       "      <td>0.628809</td>\n",
       "      <td>0.628809</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   F1_score  ClassAccuracy  model_pearsonHT29  model_pearsonA375  \\\n",
       "0       1.0            1.0           0.744946           0.647801   \n",
       "1       1.0            1.0           0.693744           0.685379   \n",
       "2       1.0            1.0           0.789342           0.743238   \n",
       "3       1.0            1.0           0.694324           0.664346   \n",
       "4       1.0            1.0           0.736065           0.704153   \n",
       "5       1.0            1.0           0.682038           0.643637   \n",
       "6       1.0            1.0           0.645892           0.568682   \n",
       "7       1.0            1.0           0.694768           0.648252   \n",
       "8       1.0            1.0           0.640344           0.576175   \n",
       "9       1.0            1.0           0.638535           0.601817   \n",
       "\n",
       "   model_spearHT29  model_spearA375  model_accHT29  model_accA375  \\\n",
       "0         0.659541         0.600114       0.730496       0.714340   \n",
       "1         0.598586         0.590397       0.710251       0.713088   \n",
       "2         0.691196         0.666481       0.743021       0.740900   \n",
       "3         0.575898         0.570811       0.703579       0.706186   \n",
       "4         0.634498         0.618352       0.723799       0.715133   \n",
       "5         0.611858         0.582229       0.723134       0.709867   \n",
       "6         0.555579         0.520047       0.691334       0.678783   \n",
       "7         0.642995         0.606206       0.730752       0.714315   \n",
       "8         0.519874         0.501132       0.677480       0.674080   \n",
       "9         0.556115         0.540897       0.690798       0.692306   \n",
       "\n",
       "   recon_pear_ht29  recon_pear_a375  recon_spear_ht29  recon_spear_a375  \\\n",
       "0         0.810811         0.795488          0.747146          0.726701   \n",
       "1         0.801183         0.784427          0.729980          0.709485   \n",
       "2         0.773252         0.800462          0.711710          0.730880   \n",
       "3         0.788082         0.791576          0.711529          0.708254   \n",
       "4         0.812644         0.798279          0.758641          0.718400   \n",
       "5         0.812387         0.801845          0.752804          0.737822   \n",
       "6         0.813070         0.786678          0.747748          0.725985   \n",
       "7         0.827602         0.780485          0.773780          0.728709   \n",
       "8         0.797780         0.785279          0.744023          0.732776   \n",
       "9         0.785988         0.770981          0.724263          0.717916   \n",
       "\n",
       "   recon_acc_ht29  recon_acc_a375  Direct_pearson  Direct_spearman  \\\n",
       "0        0.783296        0.773023        0.520686         0.463031   \n",
       "1        0.775748        0.765158        0.479417         0.420304   \n",
       "2        0.764668        0.774199        0.500422         0.432608   \n",
       "3        0.769836        0.765815        0.445524         0.393393   \n",
       "4        0.795585        0.769973        0.542872         0.442503   \n",
       "5        0.794116        0.781944        0.456026         0.433163   \n",
       "6        0.791299        0.773739        0.391251         0.353916   \n",
       "7        0.806432        0.778758        0.455651         0.433182   \n",
       "8        0.785704        0.777607        0.373686         0.337688   \n",
       "9        0.772430        0.770671        0.413972         0.372399   \n",
       "\n",
       "   DirectAcc_ht29  DirectAcc_a375  \n",
       "0        0.662730        0.662730  \n",
       "1        0.649847        0.649847  \n",
       "2        0.644836        0.644836  \n",
       "3        0.635890        0.635890  \n",
       "4        0.653144        0.653144  \n",
       "5        0.654525        0.654525  \n",
       "6        0.618405        0.618405  \n",
       "7        0.652428        0.652428  \n",
       "8        0.610148        0.610148  \n",
       "9        0.628809        0.628809  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_result = pd.DataFrame({'F1_score':valF1,'ClassAccuracy':valClassAcc,\n",
    "                          'model_pearsonHT29':valPear[:,0],'model_pearsonA375':valPear[:,1],\n",
    "                          'model_spearHT29':valSpear[:,0],'model_spearA375':valSpear[:,1],\n",
    "                          'model_accHT29':valAccuracy[:,0],'model_accA375':valAccuracy[:,1],\n",
    "                          'recon_pear_ht29':valPear_2 ,'recon_pear_a375':valPear_1,\n",
    "                          'recon_spear_ht29':valSpear_2 ,'recon_spear_a375':valSpear_1,\n",
    "                          'recon_acc_ht29':valAccuracy_2 ,'recon_acc_a375':valAccuracy_1,\n",
    "                          'Direct_pearson':valPearDirect,'Direct_spearman':valSpearDirect,\n",
    "                          'DirectAcc_ht29':valAccDirect[:,0],'DirectAcc_a375':valAccDirect[:,1]})\n",
    "df_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c19bbfd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result.to_csv('../results/MI_results/landmarks_10foldvalidation_withIntermediate2Dec_1000ep512bs_a375_ht29.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a497b5",
   "metadata": {},
   "source": [
    "## Get predictions and embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bdee479b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    # Network\n",
    "    encoder_1 = torch.load('../results/MI_results/models/IntermediateTranslationLandmarks2Dec_1000ep/encoder_a375_%s.pt'%i)\n",
    "    encoder_2 = torch.load('../results/MI_results/models/IntermediateTranslationLandmarks2Dec_1000ep/encoder_ht29_%s.pt'%i)\n",
    "    encoder_interm_1 = torch.load('../results/MI_results/models/IntermediateTranslationLandmarks2Dec_1000ep/encoder_interm_a375_%s.pt'%i)\n",
    "    encoder_interm_2 = torch.load('../results/MI_results/models/IntermediateTranslationLandmarks2Dec_1000ep/encoder_interm_ht29_%s.pt'%i)\n",
    "    decoder_1 = torch.load('../results/MI_results/models/IntermediateTranslationLandmarks2Dec_1000ep/decoder_a375_%s.pt'%i)\n",
    "    decoder_2 = torch.load('../results/MI_results/models/IntermediateTranslationLandmarks2Dec_1000ep/decoder_ht29_%s.pt'%i)\n",
    "    \n",
    "    trainInfo_paired = pd.read_csv('../preprocessing/preprocessed_data/10fold_validation_spit/train_paired_%s.csv'%i,index_col=0)\n",
    "    trainInfo_1 = pd.read_csv('../preprocessing/preprocessed_data/10fold_validation_spit/train_a375_%s.csv'%i,index_col=0)\n",
    "    trainInfo_2 = pd.read_csv('../preprocessing/preprocessed_data/10fold_validation_spit/train_ht29_%s.csv'%i,index_col=0)\n",
    "    \n",
    "    valInfo_paired = pd.read_csv('../preprocessing/preprocessed_data/10fold_validation_spit/val_paired_%s.csv'%i,index_col=0)\n",
    "    valInfo_1 = pd.read_csv('../preprocessing/preprocessed_data/10fold_validation_spit/val_a375_%s.csv'%i,index_col=0)\n",
    "    valInfo_2 = pd.read_csv('../preprocessing/preprocessed_data/10fold_validation_spit/val_ht29_%s.csv'%i,index_col=0)\n",
    "    \n",
    "    encoder_1.eval()\n",
    "    encoder_2.eval()\n",
    "    encoder_interm_1.eval()\n",
    "    encoder_interm_2.eval()\n",
    "    decoder_1.eval()\n",
    "    decoder_2.eval()\n",
    "    \n",
    "    paired_val_inds = len(valInfo_paired)\n",
    "    x_1 = torch.tensor(np.concatenate((cmap.loc[valInfo_paired['sig_id.x']].values,\n",
    "                                          cmap.loc[valInfo_1.sig_id].values))).float().to(device)\n",
    "    x_2 = torch.tensor(np.concatenate((cmap.loc[valInfo_paired['sig_id.y']].values,\n",
    "                                          cmap.loc[valInfo_2.sig_id].values))).float().to(device)\n",
    "    \n",
    "    z_base_1 = encoder_1(x_1)\n",
    "    z_base_2 = encoder_2(x_2)\n",
    "    \n",
    "    z_latent_1 = encoder_interm_1(z_base_1)\n",
    "    z_latent_2 = encoder_interm_2(z_base_2)\n",
    "    \n",
    "    xhat1 = decoder_1(z_latent_1)\n",
    "    xhat2 = decoder_2(z_latent_2)\n",
    "\n",
    "    valPreds_1 = pd.DataFrame(xhat1.detach().cpu().numpy())\n",
    "    valPreds_1.index = np.concatenate((valInfo_paired['sig_id.x'].values,valInfo_1.sig_id.values))\n",
    "    valPreds_2 = pd.DataFrame(xhat2.detach().cpu().numpy())\n",
    "    valPreds_2.index = np.concatenate((valInfo_paired['sig_id.y'].values,valInfo_2.sig_id.values))\n",
    "    \n",
    "    valEmbs_1 = pd.DataFrame(z_latent_1.detach().cpu().numpy())\n",
    "    valEmbs_1.index = np.concatenate((valInfo_paired['sig_id.x'].values,valInfo_1.sig_id.values))\n",
    "    valEmbs_2 = pd.DataFrame(z_latent_2.detach().cpu().numpy())\n",
    "    valEmbs_2.index = np.concatenate((valInfo_paired['sig_id.y'].values,valInfo_2.sig_id.values))\n",
    "    \n",
    "    valEmbs_base_1 = pd.DataFrame(z_base_1.detach().cpu().numpy())\n",
    "    valEmbs_base_1.index = np.concatenate((valInfo_paired['sig_id.x'].values,valInfo_1.sig_id.values))\n",
    "    valEmbs_base_2 = pd.DataFrame(z_base_2.detach().cpu().numpy())\n",
    "    valEmbs_base_2.index = np.concatenate((valInfo_paired['sig_id.y'].values,valInfo_2.sig_id.values))\n",
    "    \n",
    "    # Training embeddigns\n",
    "    paired_inds = len(trainInfo_paired)\n",
    "    x_1 = torch.tensor(np.concatenate((cmap.loc[trainInfo_paired['sig_id.x']].values,\n",
    "                                          cmap.loc[trainInfo_1.sig_id].values))).float().to(device)\n",
    "    x_2 = torch.tensor(np.concatenate((cmap.loc[trainInfo_paired['sig_id.y']].values,\n",
    "                                          cmap.loc[trainInfo_2.sig_id].values))).float().to(device)\n",
    "    z_base_1 = encoder_1(x_1)\n",
    "    z_base_2 = encoder_2(x_2)\n",
    "    \n",
    "    z_latent_1 = encoder_interm_1(z_base_1)\n",
    "    z_latent_2 = encoder_interm_2(z_base_2)\n",
    "    \n",
    "    # Training predictions\n",
    "    xhat1 = decoder_1(z_latent_1)\n",
    "    xhat2 = decoder_2(z_latent_2)\n",
    "\n",
    "    trainPreds_1 = pd.DataFrame(xhat1.detach().cpu().numpy())\n",
    "    trainPreds_1.index = np.concatenate((trainInfo_paired['sig_id.x'].values,trainInfo_1.sig_id.values))\n",
    "    trainPreds_2 = pd.DataFrame(xhat2.detach().cpu().numpy())\n",
    "    trainPreds_2.index = np.concatenate((trainInfo_paired['sig_id.y'].values,trainInfo_2.sig_id.values))\n",
    "    \n",
    "    trainEmbs_1 = pd.DataFrame(z_latent_1.detach().cpu().numpy())\n",
    "    trainEmbs_1.index = np.concatenate((trainInfo_paired['sig_id.x'].values,trainInfo_1.sig_id.values))\n",
    "    trainEmbs_2 = pd.DataFrame(z_latent_2.detach().cpu().numpy())\n",
    "    trainEmbs_2.index = np.concatenate((trainInfo_paired['sig_id.y'].values,trainInfo_2.sig_id.values))\n",
    "    \n",
    "    trainEmbs_base_1 = pd.DataFrame(z_base_1.detach().cpu().numpy())\n",
    "    trainEmbs_base_1.index = np.concatenate((trainInfo_paired['sig_id.x'].values,trainInfo_1.sig_id.values))\n",
    "    trainEmbs_base_2 = pd.DataFrame(z_base_2.detach().cpu().numpy())\n",
    "    trainEmbs_base_2.index = np.concatenate((trainInfo_paired['sig_id.y'].values,trainInfo_2.sig_id.values))\n",
    "    \n",
    "    valEmbs_1.to_csv('../results/MI_results/embs/IntermediateTranslationLandmarks2Dec_1000ep/validation/valEmbs_%s_a375.csv'%i)\n",
    "    valEmbs_2.to_csv('../results/MI_results/embs/IntermediateTranslationLandmarks2Dec_1000ep/validation/valEmbs_%s_ht29.csv'%i)\n",
    "    trainEmbs_1.to_csv('../results/MI_results/embs/IntermediateTranslationLandmarks2Dec_1000ep/train/trainEmbs_%s_a375.csv'%i)\n",
    "    trainEmbs_2.to_csv('../results/MI_results/embs/IntermediateTranslationLandmarks2Dec_1000ep/train/trainEmbs_%s_ht29.csv'%i)\n",
    "    \n",
    "    valEmbs_base_1.to_csv('../results/MI_results/embs/IntermediateTranslationLandmarks2Dec_1000ep/validation/valEmbs_base_%s_a375.csv'%i)\n",
    "    valEmbs_base_2.to_csv('../results/MI_results/embs/IntermediateTranslationLandmarks2Dec_1000ep/validation/valEmbs_base_%s_ht29.csv'%i)\n",
    "    trainEmbs_base_1.to_csv('../results/MI_results/embs/IntermediateTranslationLandmarks2Dec_1000ep/train/trainEmbs_base_%s_a375.csv'%i)\n",
    "    trainEmbs_base_2.to_csv('../results/MI_results/embs/IntermediateTranslationLandmarks2Dec_1000ep/train/trainEmbs_base_%s_ht29.csv'%i)\n",
    "    \n",
    "    valPreds_1.to_csv('../results/MI_results/embs/IntermediateTranslationLandmarks2Dec_1000ep/validation/valPreds_%s_a375.csv'%i)\n",
    "    valPreds_2.to_csv('../results/MI_results/embs/IntermediateTranslationLandmarks2Dec_1000ep/validation/valPreds_%s_ht29.csv'%i)\n",
    "    trainPreds_1.to_csv('../results/MI_results/embs/IntermediateTranslationLandmarks2Dec_1000ep/train/trainPreds_%s_a375.csv'%i)\n",
    "    trainPreds_2.to_csv('../results/MI_results/embs/IntermediateTranslationLandmarks2Dec_1000ep/train/trainPreds_%s_ht29.csv'%i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e65034bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
