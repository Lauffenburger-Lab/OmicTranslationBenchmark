{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "27832654",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division\n",
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from trainingUtils import MultipleOptimizer, MultipleScheduler, compute_kernel, compute_mmd\n",
    "from models import Decoder,Encoder, SimpleEncoder,LocalDiscriminator,PriorDiscriminator,Classifier,SpeciesCovariate\n",
    "from buildFrame import GlobalAEFramework\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "import random\n",
    "import os\n",
    "from sklearn.metrics import silhouette_score,confusion_matrix,f1_score\n",
    "from scipy.stats import spearmanr\n",
    "from evaluationUtils import r_square,get_cindex,pearson_r,pseudoAccuracy\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1bdc07c",
   "metadata": {},
   "source": [
    "# Load all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0055bf5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda')\n",
    "# Initialize environment and seeds for reproducability\n",
    "torch.backends.cudnn.benchmark = True\n",
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "99ba0748",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>16</th>\n",
       "      <th>23</th>\n",
       "      <th>25</th>\n",
       "      <th>30</th>\n",
       "      <th>39</th>\n",
       "      <th>47</th>\n",
       "      <th>102</th>\n",
       "      <th>128</th>\n",
       "      <th>142</th>\n",
       "      <th>154</th>\n",
       "      <th>...</th>\n",
       "      <th>94239</th>\n",
       "      <th>116832</th>\n",
       "      <th>124583</th>\n",
       "      <th>147179</th>\n",
       "      <th>148022</th>\n",
       "      <th>200081</th>\n",
       "      <th>200734</th>\n",
       "      <th>256364</th>\n",
       "      <th>375346</th>\n",
       "      <th>388650</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>PCL001_HT29_24H:BRD-K42991516:10</th>\n",
       "      <td>0.266452</td>\n",
       "      <td>-0.250874</td>\n",
       "      <td>-0.854204</td>\n",
       "      <td>-0.041545</td>\n",
       "      <td>0.204450</td>\n",
       "      <td>0.709800</td>\n",
       "      <td>-0.328601</td>\n",
       "      <td>-0.498116</td>\n",
       "      <td>-1.454481</td>\n",
       "      <td>0.506321</td>\n",
       "      <td>...</td>\n",
       "      <td>0.536235</td>\n",
       "      <td>0.024452</td>\n",
       "      <td>0.928558</td>\n",
       "      <td>-0.453246</td>\n",
       "      <td>-0.140290</td>\n",
       "      <td>0.205065</td>\n",
       "      <td>1.148706</td>\n",
       "      <td>-1.933820</td>\n",
       "      <td>1.966937</td>\n",
       "      <td>-0.159919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PCL001_HT29_24H:BRD-K50817946:10</th>\n",
       "      <td>6.074023</td>\n",
       "      <td>-0.524075</td>\n",
       "      <td>-0.635742</td>\n",
       "      <td>2.014629</td>\n",
       "      <td>-3.747274</td>\n",
       "      <td>2.109600</td>\n",
       "      <td>0.847576</td>\n",
       "      <td>-2.732549</td>\n",
       "      <td>-5.729352</td>\n",
       "      <td>2.164091</td>\n",
       "      <td>...</td>\n",
       "      <td>0.447939</td>\n",
       "      <td>1.543649</td>\n",
       "      <td>-3.775020</td>\n",
       "      <td>1.827991</td>\n",
       "      <td>-0.088051</td>\n",
       "      <td>0.382848</td>\n",
       "      <td>1.400255</td>\n",
       "      <td>-3.087269</td>\n",
       "      <td>1.392148</td>\n",
       "      <td>1.027263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PCL001_HT29_24H:BRD-K58479490:10</th>\n",
       "      <td>4.145089</td>\n",
       "      <td>-0.881727</td>\n",
       "      <td>-1.720977</td>\n",
       "      <td>1.636901</td>\n",
       "      <td>1.614980</td>\n",
       "      <td>0.092948</td>\n",
       "      <td>0.711952</td>\n",
       "      <td>-0.088671</td>\n",
       "      <td>-1.531390</td>\n",
       "      <td>-0.591393</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.312573</td>\n",
       "      <td>0.095138</td>\n",
       "      <td>2.229333</td>\n",
       "      <td>0.250220</td>\n",
       "      <td>1.523056</td>\n",
       "      <td>-0.394704</td>\n",
       "      <td>-0.167089</td>\n",
       "      <td>0.833252</td>\n",
       "      <td>0.325481</td>\n",
       "      <td>-0.652675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DOSBIO001_A375_24H:BRD-K72343629:10.1316</th>\n",
       "      <td>1.545521</td>\n",
       "      <td>1.061800</td>\n",
       "      <td>1.165320</td>\n",
       "      <td>-1.052685</td>\n",
       "      <td>-3.449826</td>\n",
       "      <td>0.503872</td>\n",
       "      <td>1.850187</td>\n",
       "      <td>-0.426328</td>\n",
       "      <td>0.004190</td>\n",
       "      <td>1.948446</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.748134</td>\n",
       "      <td>-0.636907</td>\n",
       "      <td>1.142301</td>\n",
       "      <td>1.178548</td>\n",
       "      <td>5.263110</td>\n",
       "      <td>-0.141872</td>\n",
       "      <td>-1.490323</td>\n",
       "      <td>0.526244</td>\n",
       "      <td>1.637315</td>\n",
       "      <td>-0.829246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PCL001_HT29_24H:BRD-A89859721:0.12</th>\n",
       "      <td>-0.063263</td>\n",
       "      <td>0.358551</td>\n",
       "      <td>-0.024186</td>\n",
       "      <td>0.695202</td>\n",
       "      <td>-2.394504</td>\n",
       "      <td>0.329883</td>\n",
       "      <td>-0.117662</td>\n",
       "      <td>-0.779904</td>\n",
       "      <td>0.439334</td>\n",
       "      <td>3.228897</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.474038</td>\n",
       "      <td>-0.143447</td>\n",
       "      <td>1.764741</td>\n",
       "      <td>1.436673</td>\n",
       "      <td>0.602154</td>\n",
       "      <td>1.120865</td>\n",
       "      <td>-0.255665</td>\n",
       "      <td>0.316766</td>\n",
       "      <td>0.717193</td>\n",
       "      <td>-0.772269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DOSVAL003_HA1E_24H:BRD-K99481965:20</th>\n",
       "      <td>1.228643</td>\n",
       "      <td>2.201454</td>\n",
       "      <td>1.268494</td>\n",
       "      <td>-2.925642</td>\n",
       "      <td>-5.702180</td>\n",
       "      <td>-1.048154</td>\n",
       "      <td>-0.511689</td>\n",
       "      <td>-1.073713</td>\n",
       "      <td>-0.987551</td>\n",
       "      <td>0.088006</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.521899</td>\n",
       "      <td>-1.388441</td>\n",
       "      <td>0.738075</td>\n",
       "      <td>-0.532254</td>\n",
       "      <td>-0.637348</td>\n",
       "      <td>-0.950856</td>\n",
       "      <td>1.951221</td>\n",
       "      <td>1.633684</td>\n",
       "      <td>0.036308</td>\n",
       "      <td>1.814479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DOSVAL001_HA1E_24H:BRD-K25712227:10.0073</th>\n",
       "      <td>1.240146</td>\n",
       "      <td>-0.854687</td>\n",
       "      <td>-0.066887</td>\n",
       "      <td>-0.824974</td>\n",
       "      <td>-3.184588</td>\n",
       "      <td>-1.167342</td>\n",
       "      <td>-1.893655</td>\n",
       "      <td>-0.301650</td>\n",
       "      <td>-2.429506</td>\n",
       "      <td>2.668036</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.902034</td>\n",
       "      <td>-0.527167</td>\n",
       "      <td>1.294609</td>\n",
       "      <td>-0.512041</td>\n",
       "      <td>1.322372</td>\n",
       "      <td>1.399906</td>\n",
       "      <td>1.182881</td>\n",
       "      <td>0.106317</td>\n",
       "      <td>-0.632841</td>\n",
       "      <td>-1.876167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DOSVAL001_PC3_24H:BRD-K60763357:10.036</th>\n",
       "      <td>5.411014</td>\n",
       "      <td>-0.472251</td>\n",
       "      <td>-0.512716</td>\n",
       "      <td>1.804495</td>\n",
       "      <td>0.990145</td>\n",
       "      <td>0.694528</td>\n",
       "      <td>0.157606</td>\n",
       "      <td>-1.507224</td>\n",
       "      <td>-6.045609</td>\n",
       "      <td>-3.202184</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.563177</td>\n",
       "      <td>-2.870153</td>\n",
       "      <td>1.764212</td>\n",
       "      <td>-0.991561</td>\n",
       "      <td>3.929906</td>\n",
       "      <td>1.254488</td>\n",
       "      <td>-0.011530</td>\n",
       "      <td>-1.532616</td>\n",
       "      <td>-0.706827</td>\n",
       "      <td>0.888209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DOSVAL002_PC3_24H:BRD-K58415436:20</th>\n",
       "      <td>4.105516</td>\n",
       "      <td>0.305956</td>\n",
       "      <td>2.209438</td>\n",
       "      <td>-0.029295</td>\n",
       "      <td>-1.663379</td>\n",
       "      <td>-0.174490</td>\n",
       "      <td>2.631207</td>\n",
       "      <td>-1.057758</td>\n",
       "      <td>-1.062944</td>\n",
       "      <td>1.510586</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.394032</td>\n",
       "      <td>-0.496681</td>\n",
       "      <td>1.682302</td>\n",
       "      <td>-0.722316</td>\n",
       "      <td>0.301925</td>\n",
       "      <td>0.757042</td>\n",
       "      <td>1.770458</td>\n",
       "      <td>-0.691024</td>\n",
       "      <td>-0.490237</td>\n",
       "      <td>0.569134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DOSVAL005_PC3_24H:BRD-K86354270:20</th>\n",
       "      <td>0.711225</td>\n",
       "      <td>-1.162790</td>\n",
       "      <td>-1.024860</td>\n",
       "      <td>-0.046859</td>\n",
       "      <td>1.238543</td>\n",
       "      <td>1.394547</td>\n",
       "      <td>-0.307746</td>\n",
       "      <td>-2.547696</td>\n",
       "      <td>-1.129214</td>\n",
       "      <td>1.428087</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.043247</td>\n",
       "      <td>0.861824</td>\n",
       "      <td>0.920193</td>\n",
       "      <td>-0.000206</td>\n",
       "      <td>1.322194</td>\n",
       "      <td>0.275835</td>\n",
       "      <td>-3.393023</td>\n",
       "      <td>-1.152656</td>\n",
       "      <td>-1.138677</td>\n",
       "      <td>0.496622</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4765 rows × 978 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                16        23        25  \\\n",
       "PCL001_HT29_24H:BRD-K42991516:10          0.266452 -0.250874 -0.854204   \n",
       "PCL001_HT29_24H:BRD-K50817946:10          6.074023 -0.524075 -0.635742   \n",
       "PCL001_HT29_24H:BRD-K58479490:10          4.145089 -0.881727 -1.720977   \n",
       "DOSBIO001_A375_24H:BRD-K72343629:10.1316  1.545521  1.061800  1.165320   \n",
       "PCL001_HT29_24H:BRD-A89859721:0.12       -0.063263  0.358551 -0.024186   \n",
       "...                                            ...       ...       ...   \n",
       "DOSVAL003_HA1E_24H:BRD-K99481965:20       1.228643  2.201454  1.268494   \n",
       "DOSVAL001_HA1E_24H:BRD-K25712227:10.0073  1.240146 -0.854687 -0.066887   \n",
       "DOSVAL001_PC3_24H:BRD-K60763357:10.036    5.411014 -0.472251 -0.512716   \n",
       "DOSVAL002_PC3_24H:BRD-K58415436:20        4.105516  0.305956  2.209438   \n",
       "DOSVAL005_PC3_24H:BRD-K86354270:20        0.711225 -1.162790 -1.024860   \n",
       "\n",
       "                                                30        39        47  \\\n",
       "PCL001_HT29_24H:BRD-K42991516:10         -0.041545  0.204450  0.709800   \n",
       "PCL001_HT29_24H:BRD-K50817946:10          2.014629 -3.747274  2.109600   \n",
       "PCL001_HT29_24H:BRD-K58479490:10          1.636901  1.614980  0.092948   \n",
       "DOSBIO001_A375_24H:BRD-K72343629:10.1316 -1.052685 -3.449826  0.503872   \n",
       "PCL001_HT29_24H:BRD-A89859721:0.12        0.695202 -2.394504  0.329883   \n",
       "...                                            ...       ...       ...   \n",
       "DOSVAL003_HA1E_24H:BRD-K99481965:20      -2.925642 -5.702180 -1.048154   \n",
       "DOSVAL001_HA1E_24H:BRD-K25712227:10.0073 -0.824974 -3.184588 -1.167342   \n",
       "DOSVAL001_PC3_24H:BRD-K60763357:10.036    1.804495  0.990145  0.694528   \n",
       "DOSVAL002_PC3_24H:BRD-K58415436:20       -0.029295 -1.663379 -0.174490   \n",
       "DOSVAL005_PC3_24H:BRD-K86354270:20       -0.046859  1.238543  1.394547   \n",
       "\n",
       "                                               102       128       142  \\\n",
       "PCL001_HT29_24H:BRD-K42991516:10         -0.328601 -0.498116 -1.454481   \n",
       "PCL001_HT29_24H:BRD-K50817946:10          0.847576 -2.732549 -5.729352   \n",
       "PCL001_HT29_24H:BRD-K58479490:10          0.711952 -0.088671 -1.531390   \n",
       "DOSBIO001_A375_24H:BRD-K72343629:10.1316  1.850187 -0.426328  0.004190   \n",
       "PCL001_HT29_24H:BRD-A89859721:0.12       -0.117662 -0.779904  0.439334   \n",
       "...                                            ...       ...       ...   \n",
       "DOSVAL003_HA1E_24H:BRD-K99481965:20      -0.511689 -1.073713 -0.987551   \n",
       "DOSVAL001_HA1E_24H:BRD-K25712227:10.0073 -1.893655 -0.301650 -2.429506   \n",
       "DOSVAL001_PC3_24H:BRD-K60763357:10.036    0.157606 -1.507224 -6.045609   \n",
       "DOSVAL002_PC3_24H:BRD-K58415436:20        2.631207 -1.057758 -1.062944   \n",
       "DOSVAL005_PC3_24H:BRD-K86354270:20       -0.307746 -2.547696 -1.129214   \n",
       "\n",
       "                                               154  ...     94239    116832  \\\n",
       "PCL001_HT29_24H:BRD-K42991516:10          0.506321  ...  0.536235  0.024452   \n",
       "PCL001_HT29_24H:BRD-K50817946:10          2.164091  ...  0.447939  1.543649   \n",
       "PCL001_HT29_24H:BRD-K58479490:10         -0.591393  ... -0.312573  0.095138   \n",
       "DOSBIO001_A375_24H:BRD-K72343629:10.1316  1.948446  ... -1.748134 -0.636907   \n",
       "PCL001_HT29_24H:BRD-A89859721:0.12        3.228897  ... -0.474038 -0.143447   \n",
       "...                                            ...  ...       ...       ...   \n",
       "DOSVAL003_HA1E_24H:BRD-K99481965:20       0.088006  ... -1.521899 -1.388441   \n",
       "DOSVAL001_HA1E_24H:BRD-K25712227:10.0073  2.668036  ... -1.902034 -0.527167   \n",
       "DOSVAL001_PC3_24H:BRD-K60763357:10.036   -3.202184  ... -0.563177 -2.870153   \n",
       "DOSVAL002_PC3_24H:BRD-K58415436:20        1.510586  ... -0.394032 -0.496681   \n",
       "DOSVAL005_PC3_24H:BRD-K86354270:20        1.428087  ... -0.043247  0.861824   \n",
       "\n",
       "                                            124583    147179    148022  \\\n",
       "PCL001_HT29_24H:BRD-K42991516:10          0.928558 -0.453246 -0.140290   \n",
       "PCL001_HT29_24H:BRD-K50817946:10         -3.775020  1.827991 -0.088051   \n",
       "PCL001_HT29_24H:BRD-K58479490:10          2.229333  0.250220  1.523056   \n",
       "DOSBIO001_A375_24H:BRD-K72343629:10.1316  1.142301  1.178548  5.263110   \n",
       "PCL001_HT29_24H:BRD-A89859721:0.12        1.764741  1.436673  0.602154   \n",
       "...                                            ...       ...       ...   \n",
       "DOSVAL003_HA1E_24H:BRD-K99481965:20       0.738075 -0.532254 -0.637348   \n",
       "DOSVAL001_HA1E_24H:BRD-K25712227:10.0073  1.294609 -0.512041  1.322372   \n",
       "DOSVAL001_PC3_24H:BRD-K60763357:10.036    1.764212 -0.991561  3.929906   \n",
       "DOSVAL002_PC3_24H:BRD-K58415436:20        1.682302 -0.722316  0.301925   \n",
       "DOSVAL005_PC3_24H:BRD-K86354270:20        0.920193 -0.000206  1.322194   \n",
       "\n",
       "                                            200081    200734    256364  \\\n",
       "PCL001_HT29_24H:BRD-K42991516:10          0.205065  1.148706 -1.933820   \n",
       "PCL001_HT29_24H:BRD-K50817946:10          0.382848  1.400255 -3.087269   \n",
       "PCL001_HT29_24H:BRD-K58479490:10         -0.394704 -0.167089  0.833252   \n",
       "DOSBIO001_A375_24H:BRD-K72343629:10.1316 -0.141872 -1.490323  0.526244   \n",
       "PCL001_HT29_24H:BRD-A89859721:0.12        1.120865 -0.255665  0.316766   \n",
       "...                                            ...       ...       ...   \n",
       "DOSVAL003_HA1E_24H:BRD-K99481965:20      -0.950856  1.951221  1.633684   \n",
       "DOSVAL001_HA1E_24H:BRD-K25712227:10.0073  1.399906  1.182881  0.106317   \n",
       "DOSVAL001_PC3_24H:BRD-K60763357:10.036    1.254488 -0.011530 -1.532616   \n",
       "DOSVAL002_PC3_24H:BRD-K58415436:20        0.757042  1.770458 -0.691024   \n",
       "DOSVAL005_PC3_24H:BRD-K86354270:20        0.275835 -3.393023 -1.152656   \n",
       "\n",
       "                                            375346    388650  \n",
       "PCL001_HT29_24H:BRD-K42991516:10          1.966937 -0.159919  \n",
       "PCL001_HT29_24H:BRD-K50817946:10          1.392148  1.027263  \n",
       "PCL001_HT29_24H:BRD-K58479490:10          0.325481 -0.652675  \n",
       "DOSBIO001_A375_24H:BRD-K72343629:10.1316  1.637315 -0.829246  \n",
       "PCL001_HT29_24H:BRD-A89859721:0.12        0.717193 -0.772269  \n",
       "...                                            ...       ...  \n",
       "DOSVAL003_HA1E_24H:BRD-K99481965:20       0.036308  1.814479  \n",
       "DOSVAL001_HA1E_24H:BRD-K25712227:10.0073 -0.632841 -1.876167  \n",
       "DOSVAL001_PC3_24H:BRD-K60763357:10.036   -0.706827  0.888209  \n",
       "DOSVAL002_PC3_24H:BRD-K58415436:20       -0.490237  0.569134  \n",
       "DOSVAL005_PC3_24H:BRD-K86354270:20       -1.138677  0.496622  \n",
       "\n",
       "[4765 rows x 978 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cmap1 = pd.read_csv('../preprocessing/preprocessed_data/cmap_landmarks_HT29_A375.csv',index_col = 0)\n",
    "cmap2 = pd.read_csv('../preprocessing/preprocessed_data/cmap_landmarks_HA1E_PC3.csv',index_col = 0)\n",
    "cmap = pd.concat((cmap1,cmap2),axis=0)\n",
    "cmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c3ff902",
   "metadata": {},
   "outputs": [],
   "source": [
    "gene_size = len(cmap.columns)\n",
    "samples = cmap.index.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d99b3ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a train generators\n",
    "def getSamples(N, batchSize):\n",
    "    order = np.random.permutation(N)\n",
    "    outList = []\n",
    "    while len(order)>0:\n",
    "        outList.append(order[0:batchSize])\n",
    "        order = order[batchSize:]\n",
    "    return outList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "93583e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradients(output, input):\n",
    "    grads = torch.autograd.grad(output, input, create_graph=True)\n",
    "    grads = grads[0].pow(2).mean()\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa94965",
   "metadata": {},
   "source": [
    "# Initialize parameters and models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "41edfe10",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_params = {'encoder_hiddens':[640,384],\n",
    "                'latent_dim': 292,\n",
    "                'decoder_hiddens':[384,640],\n",
    "                'dropout_decoder':0.2,\n",
    "                'dropout_encoder':0.1,\n",
    "                'encoder_activation':torch.nn.ELU(),\n",
    "                'decoder_activation':torch.nn.ELU(),\n",
    "                'V_dropout':0.25,\n",
    "                'state_class_hidden':[256,128,64],\n",
    "                'state_class_drop_in':0.5,\n",
    "                'state_class_drop':0.25,\n",
    "                'no_states':4,\n",
    "                'adv_class_hidden':[256,128,64],\n",
    "                'adv_class_drop_in':0.3,\n",
    "                'adv_class_drop':0.1,\n",
    "                'no_adv_class':4,\n",
    "                'encoding_lr':0.01,\n",
    "                'adv_lr':0.001,\n",
    "                'schedule_step_adv':200,\n",
    "                'gamma_adv':0.5,\n",
    "                'schedule_step_enc':400,\n",
    "                'gamma_enc':0.8,\n",
    "                'batch_size_1':178,\n",
    "                'batch_size_2':154,\n",
    "                'batch_size_paired':90,\n",
    "                'epochs':2000,\n",
    "                'prior_beta':1.0,\n",
    "                'no_folds':10,\n",
    "                'v_reg':1e-04,\n",
    "                'state_class_reg':1e-04,\n",
    "                'enc_l2_reg':0.00001,\n",
    "                'dec_l2_reg':0.01,\n",
    "                'lambda_mi_loss':100,\n",
    "                'effsize_reg': 100,\n",
    "                'cosine_loss': 10,\n",
    "                'adv_penalnty':100,\n",
    "                'reg_adv':1000,\n",
    "                'reg_classifier': 1000,\n",
    "                'similarity_reg' : 10,\n",
    "                'adversary_steps':4,\n",
    "                'autoencoder_wd': 0.,\n",
    "                'adversary_wd': 0.}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d5aaf589",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_criterion = torch.nn.CrossEntropyLoss()\n",
    "NUM_EPOCHS= model_params['epochs']\n",
    "bs_1 = model_params['batch_size_1']\n",
    "bs_2 =  model_params['batch_size_2']\n",
    "bs_paired =  model_params['batch_size_paired']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "011ab8b5",
   "metadata": {},
   "source": [
    "## Pre-train adverse classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "035bdc95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# globalFrame = GlobalAEFramework(model_params['no_states'],[[0,1],[0,2],[0,3],[1,2],[1,3],[2,3]],\n",
    "#                                 [gene_size,gene_size,gene_size,gene_size],\n",
    "#                                 [model_params['encoder_hiddens'],model_params['encoder_hiddens'],model_params['encoder_hiddens'],model_params['encoder_hiddens']],\n",
    "#                                 model_params['latent_dim'], \n",
    "#                                 [model_params['decoder_hiddens'],model_params['decoder_hiddens'],model_params['decoder_hiddens'],model_params['decoder_hiddens']] ,\n",
    "#                                 dropRateEnc=model_params['dropout_encoder'], activationEnc=model_params['encoder_activation'],\n",
    "#                                 dropRateDec=model_params['dropout_decoder'], activationDec=model_params['decoder_activation'],\n",
    "#                                 covariateDrop = model_params['V_dropout']).to(device)\n",
    "\n",
    "prior_d = PriorDiscriminator(model_params['latent_dim']).to(device)\n",
    "local_d = LocalDiscriminator(model_params['latent_dim'],model_params['latent_dim']).to(device)\n",
    "\n",
    "adverse_classifier = Classifier(in_channel=model_params['latent_dim'],\n",
    "                                hidden_layers=model_params['adv_class_hidden'],\n",
    "                                num_classes=model_params['no_adv_class'],\n",
    "                                drop_in=0.1,\n",
    "                                drop=0.1).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "454725a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../preprocessing/preprocessed_data/AllfilteredDrugs.csv',index_col=0)\n",
    "#data = data[(data['cell_iname'] == 'PC3')| (data['cell_iname'] == 'HA1E') | (data['cell_iname'] == 'A375') | (data['cell_iname'] == 'HT29')]\n",
    "data = [data[data['cell_iname'] == 'PC3'],data[data['cell_iname'] == 'HA1E'],\n",
    "        data[data['cell_iname'] == 'A375'],data[data['cell_iname'] == 'HT29']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "88bf5bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_of_modalities = globalFrame.num_of_modalities\n",
    "# paired_modalities = globalFrame.paired_modalities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3d165ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_of_modalities = model_params['no_states']\n",
    "paired_modalities = [[0,1],[0,2],[0,3],[1,2],[1,3],[2,3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "89cd4aa4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>conditionId</th>\n",
       "      <th>sig_id_x</th>\n",
       "      <th>cell_iname_x</th>\n",
       "      <th>sig_id_y</th>\n",
       "      <th>cell_iname_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SA-1921085_10 uM_24 h</td>\n",
       "      <td>DOSBIO001_A375_24H:BRD-K58292285:9.838</td>\n",
       "      <td>A375</td>\n",
       "      <td>DOSBIO001_HT29_24H:BRD-K58292285:9.838</td>\n",
       "      <td>HT29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BRD-K88822846_10 uM_24 h</td>\n",
       "      <td>DOSBIO001_A375_24H:BRD-K88822846:10.1074</td>\n",
       "      <td>A375</td>\n",
       "      <td>DOSBIO001_HT29_24H:BRD-K88822846:10.1074</td>\n",
       "      <td>HT29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BRD-K11169037_10 uM_24 h</td>\n",
       "      <td>DOSBIO001_A375_24H:BRD-K11169037:10.0519</td>\n",
       "      <td>A375</td>\n",
       "      <td>DOSBIO001_HT29_24H:BRD-K11169037:10.0519</td>\n",
       "      <td>HT29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>UMB-32_10 uM_24 h</td>\n",
       "      <td>PCL001_A375_24H:BRD-K21532219:10</td>\n",
       "      <td>A375</td>\n",
       "      <td>PCL001_HT29_24H:BRD-K21532219:10</td>\n",
       "      <td>HT29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BRD-K58479490_10 uM_24 h</td>\n",
       "      <td>PCL001_A375_24H:BRD-K58479490:10</td>\n",
       "      <td>A375</td>\n",
       "      <td>PCL001_HT29_24H:BRD-K58479490:10</td>\n",
       "      <td>HT29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>401</th>\n",
       "      <td>BRD-K48853221_10 uM_24 h</td>\n",
       "      <td>DOSVAL001_A375_24H:BRD-K48853221:10</td>\n",
       "      <td>A375</td>\n",
       "      <td>DOSVAL001_HT29_24H:BRD-K48853221:10</td>\n",
       "      <td>HT29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>402</th>\n",
       "      <td>BRD-K66214645_10 uM_24 h</td>\n",
       "      <td>DOSVAL001_A375_24H:BRD-K66214645:10</td>\n",
       "      <td>A375</td>\n",
       "      <td>DOSVAL001_HT29_24H:BRD-K66214645:10</td>\n",
       "      <td>HT29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>403</th>\n",
       "      <td>BRD-K63423329_20 uM_24 h</td>\n",
       "      <td>DOSVAL006_A375_24H:BRD-K63423329:20</td>\n",
       "      <td>A375</td>\n",
       "      <td>DOSVAL006_HT29_24H:BRD-K63423329:20</td>\n",
       "      <td>HT29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>404</th>\n",
       "      <td>BRD-K05885357_10 uM_24 h</td>\n",
       "      <td>DOSVAL001_A375_24H:BRD-K05885357:10.0043</td>\n",
       "      <td>A375</td>\n",
       "      <td>DOSVAL001_HT29_24H:BRD-K05885357:10.0043</td>\n",
       "      <td>HT29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>405</th>\n",
       "      <td>BRD-K06188599_10 uM_24 h</td>\n",
       "      <td>DOSVAL001_A375_24H:BRD-K06188599:10.1209</td>\n",
       "      <td>A375</td>\n",
       "      <td>DOSVAL001_HT29_24H:BRD-K06188599:10.1209</td>\n",
       "      <td>HT29</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>406 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  conditionId                                  sig_id_x  \\\n",
       "0       SA-1921085_10 uM_24 h    DOSBIO001_A375_24H:BRD-K58292285:9.838   \n",
       "1    BRD-K88822846_10 uM_24 h  DOSBIO001_A375_24H:BRD-K88822846:10.1074   \n",
       "2    BRD-K11169037_10 uM_24 h  DOSBIO001_A375_24H:BRD-K11169037:10.0519   \n",
       "3           UMB-32_10 uM_24 h          PCL001_A375_24H:BRD-K21532219:10   \n",
       "4    BRD-K58479490_10 uM_24 h          PCL001_A375_24H:BRD-K58479490:10   \n",
       "..                        ...                                       ...   \n",
       "401  BRD-K48853221_10 uM_24 h       DOSVAL001_A375_24H:BRD-K48853221:10   \n",
       "402  BRD-K66214645_10 uM_24 h       DOSVAL001_A375_24H:BRD-K66214645:10   \n",
       "403  BRD-K63423329_20 uM_24 h       DOSVAL006_A375_24H:BRD-K63423329:20   \n",
       "404  BRD-K05885357_10 uM_24 h  DOSVAL001_A375_24H:BRD-K05885357:10.0043   \n",
       "405  BRD-K06188599_10 uM_24 h  DOSVAL001_A375_24H:BRD-K06188599:10.1209   \n",
       "\n",
       "    cell_iname_x                                  sig_id_y cell_iname_y  \n",
       "0           A375    DOSBIO001_HT29_24H:BRD-K58292285:9.838         HT29  \n",
       "1           A375  DOSBIO001_HT29_24H:BRD-K88822846:10.1074         HT29  \n",
       "2           A375  DOSBIO001_HT29_24H:BRD-K11169037:10.0519         HT29  \n",
       "3           A375          PCL001_HT29_24H:BRD-K21532219:10         HT29  \n",
       "4           A375          PCL001_HT29_24H:BRD-K58479490:10         HT29  \n",
       "..           ...                                       ...          ...  \n",
       "401         A375       DOSVAL001_HT29_24H:BRD-K48853221:10         HT29  \n",
       "402         A375       DOSVAL001_HT29_24H:BRD-K66214645:10         HT29  \n",
       "403         A375       DOSVAL006_HT29_24H:BRD-K63423329:20         HT29  \n",
       "404         A375  DOSVAL001_HT29_24H:BRD-K05885357:10.0043         HT29  \n",
       "405         A375  DOSVAL001_HT29_24H:BRD-K06188599:10.1209         HT29  \n",
       "\n",
       "[406 rows x 5 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(len(paired_modalities)):\n",
    "    sampleInfo1 = data[paired_modalities[i][0]].loc[:,['conditionId','sig_id','cell_iname']]\n",
    "    sampleInfo2 = data[paired_modalities[i][1]].loc[:,['conditionId','sig_id','cell_iname']]\n",
    "    paired = sampleInfo1.merge(sampleInfo2,how='inner',left_on='conditionId',right_on='conditionId')\n",
    "    \n",
    "    if i==0:\n",
    "        pairedSamples = paired.reset_index(drop=True).copy()\n",
    "    else:\n",
    "        pairedSamples = pd.concat((pairedSamples,paired.reset_index(drop=True)),axis=0).reset_index(drop=True)\n",
    "\n",
    "paired_sigs = np.union1d(pairedSamples.sig_id_x.values,pairedSamples.sig_id_y.values)\n",
    "pairedSamples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "833e53d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataUnpaired = []\n",
    "for i,df in enumerate(data):\n",
    "    dataUnpaired.append(df[df.sig_id.isin(paired_sigs)].reset_index(drop=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fd72363d",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_paired = len(pairedSamples)\n",
    "sample_sizes = [len(d) for d in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "75396311",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1189, 1087]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "591db7ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "batchSizes = [256,256,256,256]\n",
    "#bs_paired = 250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "29218aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_criterion = torch.nn.CrossEntropyLoss()\n",
    "NUM_EPOCHS= model_params['epochs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ca771c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiEncoder(torch.nn.Module):\n",
    "    def __init__(self,num_of_modalities,paired_modalities,\n",
    "                 in_channels, enc_hidden_layers, latent_dim ,\n",
    "                 dropRateEnc=0.1, activationEnc=None, biasEnc=True,normalizeLatent=False,\n",
    "                 variational=False):\n",
    "\n",
    "        super(MultiEncoder, self).__init__()\n",
    "\n",
    "        self.num_of_modalities = num_of_modalities\n",
    "        self.paired_modalities = paired_modalities\n",
    "        self.encoders = torch.nn.ModuleList()\n",
    "\n",
    "        for i in range(self.num_of_modalities):\n",
    "            if variational == True:\n",
    "                self.encoders.append(Encoder(in_channels[i], enc_hidden_layers[i], latent_dim,dropRateEnc, activationEnc, biasEnc))\n",
    "            else:\n",
    "                self.encoders.append(SimpleEncoder(in_channels[i], enc_hidden_layers[i], latent_dim, dropRateEnc, activationEnc,normalizeLatent, biasEnc))\n",
    "\n",
    "    def forward(self, x):\n",
    "        z_latents = []\n",
    "        for i in range(self.num_of_modalities):\n",
    "            z = self.encoders[i](x[i])\n",
    "            z_latents.append(z)\n",
    "        return torch.cat(z_latents,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "85790a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_enc = MultiEncoder(model_params['no_states'],[[0,1],[0,2],[0,3],[1,2],[1,3],[2,3]],#[0,2],[0,3],[1,2],[1,3],[2,3]\n",
    "                         [gene_size,gene_size,gene_size,gene_size],\n",
    "                         [model_params['encoder_hiddens'],model_params['encoder_hiddens'],model_params['encoder_hiddens'],model_params['encoder_hiddens']],\n",
    "                         model_params['latent_dim'], \n",
    "                         [model_params['decoder_hiddens'],model_params['decoder_hiddens'],model_params['decoder_hiddens'],model_params['decoder_hiddens']] ,\n",
    "                         dropRateEnc=model_params['dropout_encoder'], activationEnc=model_params['encoder_activation']).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d6cc1479",
   "metadata": {},
   "outputs": [],
   "source": [
    "allParams = list(multi_enc.parameters())\n",
    "allParams = allParams + list(prior_d.parameters()) + list(local_d.parameters())\n",
    "allParams = allParams + list(adverse_classifier.parameters())\n",
    "optimizer = torch.optim.Adam(allParams, lr=model_params['encoding_lr'], weight_decay=0)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n",
    "                                            step_size=model_params['schedule_step_enc'],\n",
    "                                            gamma=model_params['gamma_enc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cdb44cea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch=1/2000, MI Loss=3.7015, Prior Loss=1.3804, Entropy Loss=0.8781, loss=6.0130, F1 score=0.2553\n",
      "Epoch=251/2000, MI Loss=3.6973, Prior Loss=1.3775, Entropy Loss=0.8868, loss=6.0145, F1 score=0.2624\n",
      "Epoch=501/2000, MI Loss=3.6594, Prior Loss=1.3823, Entropy Loss=0.8850, loss=5.9796, F1 score=0.2553\n",
      "Epoch=751/2000, MI Loss=3.7604, Prior Loss=1.3819, Entropy Loss=0.8811, loss=6.0763, F1 score=0.2979\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-efc99f6f2a72>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     81\u001b[0m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels_adv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m         \u001b[0mpredicted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpredicted\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 83\u001b[1;33m         \u001b[0mf1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf1_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrue_labels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpredicted\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maverage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'micro'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     84\u001b[0m         \u001b[0madv_entropy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclass_criterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels_adv\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrue_labels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py\u001b[0m in \u001b[0;36mf1_score\u001b[1;34m(y_true, y_pred, labels, pos_label, average, sample_weight, zero_division)\u001b[0m\n\u001b[0;32m   1121\u001b[0m     \u001b[0mmodified\u001b[0m \u001b[1;32mwith\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mzero_division\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1122\u001b[0m     \"\"\"\n\u001b[1;32m-> 1123\u001b[1;33m     return fbeta_score(\n\u001b[0m\u001b[0;32m   1124\u001b[0m         \u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1125\u001b[0m         \u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py\u001b[0m in \u001b[0;36mfbeta_score\u001b[1;34m(y_true, y_pred, beta, labels, pos_label, average, sample_weight, zero_division)\u001b[0m\n\u001b[0;32m   1259\u001b[0m     \"\"\"\n\u001b[0;32m   1260\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1261\u001b[1;33m     _, _, f, _ = precision_recall_fscore_support(\n\u001b[0m\u001b[0;32m   1262\u001b[0m         \u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1263\u001b[0m         \u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py\u001b[0m in \u001b[0;36mprecision_recall_fscore_support\u001b[1;34m(y_true, y_pred, beta, labels, pos_label, average, warn_for, sample_weight, zero_division)\u001b[0m\n\u001b[0;32m   1542\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mbeta\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1543\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"beta should be >=0 in the F-beta score\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1544\u001b[1;33m     \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_check_set_wise_labels\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpos_label\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1545\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1546\u001b[0m     \u001b[1;31m# Calculate tp_sum, pred_sum, true_sum ###\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py\u001b[0m in \u001b[0;36m_check_set_wise_labels\u001b[1;34m(y_true, y_pred, average, labels, pos_label)\u001b[0m\n\u001b[0;32m   1346\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"average has to be one of \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maverage_options\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1347\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1348\u001b[1;33m     \u001b[0my_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1349\u001b[0m     \u001b[1;31m# Convert to Python primitive type to avoid NumPy type / Python str\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1350\u001b[0m     \u001b[1;31m# comparison. See https://github.com/numpy/numpy/issues/6784\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[1;34m(y_true, y_pred)\u001b[0m\n\u001b[0;32m     82\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0marray\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mindicator\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m     \"\"\"\n\u001b[1;32m---> 84\u001b[1;33m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     85\u001b[0m     \u001b[0mtype_true\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m     \u001b[0mtype_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    328\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    329\u001b[0m     \u001b[0mlengths\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0m_num_samples\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mX\u001b[0m \u001b[1;32min\u001b[0m \u001b[0marrays\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mX\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 330\u001b[1;33m     \u001b[0muniques\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    331\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    332\u001b[0m         raise ValueError(\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36munique\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\numpy\\lib\\arraysetops.py\u001b[0m in \u001b[0;36munique\u001b[1;34m(ar, return_index, return_inverse, return_counts, axis)\u001b[0m\n\u001b[0;32m    259\u001b[0m     \u001b[0mar\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masanyarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mar\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    260\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0maxis\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 261\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_unique1d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mar\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_inverse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_counts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    262\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0m_unpack_tuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mret\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    263\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\numpy\\lib\\arraysetops.py\u001b[0m in \u001b[0;36m_unique1d\u001b[1;34m(ar, return_index, return_inverse, return_counts)\u001b[0m\n\u001b[0;32m    320\u001b[0m         \u001b[0maux\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mar\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mperm\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    321\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 322\u001b[1;33m         \u001b[0mar\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msort\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    323\u001b[0m         \u001b[0maux\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mar\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    324\u001b[0m     \u001b[0mmask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maux\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbool_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for e in range(0, NUM_EPOCHS):\n",
    "    multi_enc.train()\n",
    "    prior_d.train()\n",
    "    local_d.train()\n",
    "    adverse_classifier.train()\n",
    "    \n",
    "    \n",
    "    trainLoadersList = []\n",
    "    lenghts = []\n",
    "    for load_id,bs in enumerate(batchSizes):\n",
    "        trainLoad = getSamples(sample_sizes[load_id],bs)\n",
    "        trainLoadersList.append(trainLoad)\n",
    "        lenghts.append(len(trainLoad))\n",
    "    #trainLoad = getSamples(N_paired,bs_paired)\n",
    "    #trainLoadersList.append(trainLoad)\n",
    "    #lenghts.append(len(trainLoad))\n",
    "    \n",
    "    maxLen = np.max(lenghts)\n",
    "    \n",
    "    for load_id,trainLoad in enumerate(trainLoadersList):\n",
    "        if maxLen>lenghts[load_id]:\n",
    "            trainloader_suppl = getSamples(sample_sizes[load_id],bs)\n",
    "            for jj in range(maxLen-lenghts[load_id]):\n",
    "                trainLoad.insert(jj,trainloader_suppl[jj])\n",
    "        trainLoadersList[load_id] = trainLoad\n",
    "            \n",
    "    for j in range(maxLen):\n",
    "        dataIndices = [trainLoad[j] for trainLoad in trainLoadersList]\n",
    "        #dataIndex_paired = dataIndices[-1]\n",
    "            \n",
    "        #df_pairs = pairedSamples.iloc[dataIndex_paired,:]\n",
    "        #paired_inds = len(df_pairs)\n",
    "        dfs = [data[x].iloc[dataIndices[x],:] for x in range(len(dataIndices))]\n",
    "        \n",
    "        ### Create paired samples DURING TRAINING BELLOW\n",
    "        #for pair_id in range(len(paired_modalities)):\n",
    "        #    sampleInfo1 = dfs[paired_modalities[pair_id][0]].loc[:,['conditionId','sig_id','cell_iname']]\n",
    "        #    sampleInfo2 = dfs[paired_modalities[pair_id][1]].loc[:,['conditionId','sig_id','cell_iname']]\n",
    "        #    paired = sampleInfo1.merge(sampleInfo2,how='inner',left_on='conditionId',right_on='conditionId')\n",
    "        #    if pair_id==0:\n",
    "        #        conditions = np.concatenate((paired.conditionId.values,\n",
    "        #                                     sampleInfo1.conditionId.values,\n",
    "        #                                     paired.conditionId.values,\n",
    "        #                                     sampleInfo2.conditionId.values))\n",
    "        #    else:\n",
    "        #        conditions = np.concatenate((conditions,\n",
    "        #                                     paired.conditionId.values,\n",
    "        #                                     sampleInfo1.conditionId.values,\n",
    "        #                                     paired.conditionId.values,\n",
    "        #                                     sampleInfo2.conditionId.values)) \n",
    "        X_s = []\n",
    "        for df_id,df in enumerate(dfs):\n",
    "            sampleInfo = df.loc[:,['conditionId','sig_id','cell_iname']]\n",
    "            if df_id==0:\n",
    "                conditions = df.conditionId.values\n",
    "                true_labels = torch.ones(len(df)) * df_id\n",
    "            else:\n",
    "                conditions = np.concatenate((conditions,\n",
    "                                             df.conditionId.values))\n",
    "                true_labels = torch.cat((true_labels,torch.ones(len(df)) * df_id),0)\n",
    "            X_s.append(torch.tensor(cmap.loc[df.sig_id.values].values).float().to(device))\n",
    "        true_labels = true_labels.long().to(device)\n",
    "        size = conditions.size\n",
    "        conditions = conditions.reshape(size,1)\n",
    "        conditions = conditions == conditions.transpose()\n",
    "        conditions = conditions*1\n",
    "        mask = torch.tensor(conditions).to(device).detach()\n",
    "        pos_mask = mask\n",
    "        neg_mask = 1 - mask\n",
    "        log_2 = math.log(2.)\n",
    "        #X_s = [torch.tensor(cmap.loc[df.sig_id.values].values).float().to(device) for df in dfs]     \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        #z_latents = []\n",
    "        #for enc_id,enc in enumerate(encoders):\n",
    "        #    z_latents.append(enc(X_s[enc_id]))\n",
    "        #latent_vectors = torch.cat(z_latents, 0)\n",
    "        latent_vectors = multi_enc(X_s)\n",
    "        \n",
    "        labels_adv = adverse_classifier(latent_vectors)\n",
    "        _, predicted = torch.max(labels_adv, 1)\n",
    "        predicted = predicted.cpu().numpy()\n",
    "        f1 = f1_score(true_labels.cpu(), predicted,average='micro')\n",
    "        adv_entropy = class_criterion(labels_adv,true_labels)\n",
    "        \n",
    "        z_un = local_d(latent_vectors)\n",
    "        res_un = torch.matmul(z_un, z_un.t())\n",
    "            \n",
    "        p_samples = res_un * pos_mask.float()\n",
    "        q_samples = res_un * neg_mask.float()\n",
    "\n",
    "        Ep = log_2 - F.softplus(- p_samples)\n",
    "        Eq = F.softplus(-q_samples) + q_samples - log_2\n",
    "\n",
    "        Ep = (Ep * pos_mask.float()).sum() / pos_mask.float().sum()\n",
    "        Eq = (Eq * neg_mask.float()).sum() / neg_mask.float().sum()\n",
    "        mi_loss = Eq - Ep\n",
    "\n",
    "        #prior = torch.rand_like(torch.cat((z_1, z_2), 0))\n",
    "        prior = torch.rand_like(latent_vectors)\n",
    "\n",
    "        term_a = torch.log(prior_d(prior)).mean()\n",
    "        term_b = torch.log(1.0 - prior_d(latent_vectors)).mean()\n",
    "        prior_loss = -(term_a + term_b) * model_params['prior_beta']\n",
    "        \n",
    "        #L2Loss = 0.\n",
    "        #for enc in encoders:\n",
    "        #    L2Loss = L2Loss + enc.L2Regularization(model_params['enc_l2_reg'])\n",
    "        \n",
    "        loss = mi_loss+prior_loss+adv_entropy+adverse_classifier.L2Regularization(model_params['state_class_reg'])#+L2Loss\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "    scheduler.step()\n",
    "    outString = 'Epoch={:.0f}/{:.0f}'.format(e+1,NUM_EPOCHS)\n",
    "    outString += ', MI Loss={:.4f}'.format(mi_loss.item())\n",
    "    outString += ', Prior Loss={:.4f}'.format(prior_loss.item())\n",
    "    outString += ', Entropy Loss={:.4f}'.format(adv_entropy.item())\n",
    "    outString += ', loss={:.4f}'.format(loss.item())\n",
    "    outString += ', F1 score={:.4f}'.format(f1)\n",
    "    if (e==0 or (e%250==0 and e>0)):\n",
    "        print(outString)\n",
    "print(outString)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "790364a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab9dcf28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d6456b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c774a1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2f9f7b3b",
   "metadata": {},
   "source": [
    "# Train the whole framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb18d451",
   "metadata": {},
   "outputs": [],
   "source": [
    "globalFrame = GlobalAEFramework(model_params['no_states'],[[0,1],[0,2],[0,3],[1,2],[1,3],[2,3]],\n",
    "                                [gene_size,gene_size,gene_size,gene_size],\n",
    "                                [model_params['encoder_hiddens'],model_params['encoder_hiddens'],model_params['encoder_hiddens'],model_params['encoder_hiddens']],\n",
    "                                model_params['latent_dim'], \n",
    "                                [model_params['decoder_hiddens'],model_params['decoder_hiddens'],model_params['decoder_hiddens'],model_params['decoder_hiddens']] ,\n",
    "                                dropRateEnc=model_params['dropout_encoder'], activationEnc=model_params['encoder_activation'],\n",
    "                                dropRateDec=model_params['dropout_decoder'], activationDec=model_params['decoder_activation'],\n",
    "                                covariateDrop = model_params['V_dropout']).to(device)\n",
    "\n",
    "prior_d = PriorDiscriminator(model_params['latent_dim']).to(device)\n",
    "local_d = LocalDiscriminator(model_params['latent_dim'],model_params['latent_dim']).to(device)\n",
    "    \n",
    "classifier = Classifier(in_channel=model_params['latent_dim'],\n",
    "                        hidden_layers=model_params['state_class_hidden'],\n",
    "                        num_classes=model_params['no_states'],\n",
    "                        drop_in=model_params['state_class_drop_in'],\n",
    "                        drop=model_params['state_class_drop']).to(device)\n",
    "\n",
    "adverse_classifier = Classifier(in_channel=model_params['latent_dim'],\n",
    "                                hidden_layers=model_params['adv_class_hidden'],\n",
    "                                num_classes=model_params['no_adv_class'],\n",
    "                                drop_in=model_params['adv_class_drop_in'],\n",
    "                                drop=model_params['adv_class_drop']).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0790b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "allParams = list(globalFrame.parameters())\n",
    "allParams = allParams + list(prior_d.parameters()) + list(local_d.parameters())\n",
    "allParams = allParams + list(classifier.parameters())\n",
    "optimizer = torch.optim.Adam(allParams, lr=model_params['encoding_lr'], weight_decay=0)\n",
    "optimizer_adv = torch.optim.Adam(adverse_classifier.parameters(), lr=model_params['adv_lr'], weight_decay=0)\n",
    "if model_params['schedule_step_adv'] is not None:\n",
    "    scheduler_adv = torch.optim.lr_scheduler.StepLR(optimizer_adv,\n",
    "                                                    step_size=model_params['schedule_step_adv'],\n",
    "                                                    gamma=model_params['gamma_adv'])\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n",
    "                                                step_size=model_params['schedule_step_enc'],\n",
    "                                                gamma=model_params['gamma_enc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "78a7ccf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "valR2 = []\n",
    "valPear = []\n",
    "valMSE =[]\n",
    "valSpear = []\n",
    "valAccuracy = []\n",
    "\n",
    "\n",
    "valPearDirect = []\n",
    "valSpearDirect = []\n",
    "valAccDirect = []\n",
    "\n",
    "valR2_1 = []\n",
    "valPear_1 = []\n",
    "valMSE_1 =[]\n",
    "valSpear_1 = []\n",
    "valAccuracy_1 = []\n",
    "\n",
    "valR2_2 = []\n",
    "valPear_2 = []\n",
    "valMSE_2 =[]\n",
    "valSpear_2 = []\n",
    "valAccuracy_2 = []\n",
    "\n",
    "crossCorrelation = []\n",
    "\n",
    "valF1 = []\n",
    "valClassAcc = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d1e166",
   "metadata": {},
   "outputs": [],
   "source": [
    "for e in range(0, NUM_EPOCHS):\n",
    "    decoder_1.train()\n",
    "    decoder_2.train()\n",
    "    encoder_1.train()\n",
    "    encoder_2.train()\n",
    "    prior_d.train()\n",
    "    local_d.train()\n",
    "    classifier.train()\n",
    "    adverse_classifier.train()\n",
    "    Vsp.train()\n",
    "    \n",
    "    trainloader_1 = getSamples(N_1, bs_1)\n",
    "    len_1 = len(trainloader_1)\n",
    "    trainloader_2 = getSamples(N_2, bs_2)\n",
    "    len_2 = len(trainloader_2)\n",
    "    trainloader_paired = getSamples(N_paired, bs_paired)\n",
    "    len_paired = len(trainloader_paired)\n",
    "\n",
    "    lens = [len_1,len_2,len_paired]\n",
    "    maxLen = np.max(lens)\n",
    "    \n",
    "    if maxLen>lens[0]:\n",
    "        trainloader_suppl = getSamples(N_1, bs_1)\n",
    "        for jj in range(maxLen-lens[0]):\n",
    "            trainloader_1.insert(jj,trainloader_suppl[jj])\n",
    "        \n",
    "    if maxLen>lens[1]:\n",
    "        trainloader_suppl = getSamples(N_2, bs_2)\n",
    "        for jj in range(maxLen-lens[1]):\n",
    "            trainloader_2.insert(jj,trainloader_suppl[jj])\n",
    "        \n",
    "    if maxLen>lens[2]:\n",
    "        trainloader_suppl = getSamples(N_paired, bs_paired)\n",
    "        for jj in range(maxLen-lens[2]):\n",
    "            trainloader_paired.insert(jj,trainloader_suppl[jj])\n",
    "            \n",
    "    for j in range(maxLen):\n",
    "        dataIndex_1 = trainloader_1[j]\n",
    "        dataIndex_2 = trainloader_2[j]\n",
    "        dataIndex_paired = trainloader_paired[j]\n",
    "            \n",
    "        df_pairs = trainInfo_paired.iloc[dataIndex_paired,:]\n",
    "        df_1 = trainInfo_1.iloc[dataIndex_1,:]\n",
    "        df_2 = trainInfo_2.iloc[dataIndex_2,:]\n",
    "        paired_inds = len(df_pairs)\n",
    "            \n",
    "            \n",
    "        X_1 = torch.tensor(np.concatenate((cmap.loc[df_pairs['sig_id.x']].values,\n",
    "                                                 cmap.loc[df_1.sig_id].values))).float().to(device)\n",
    "        X_2 = torch.tensor(np.concatenate((cmap.loc[df_pairs['sig_id.y']].values,\n",
    "                                                 cmap.loc[df_2.sig_id].values))).float().to(device)\n",
    "            \n",
    "        z_species_1 = torch.cat((torch.ones(X_1.shape[0],1),\n",
    "                                     torch.zeros(X_1.shape[0],1)),1).to(device)\n",
    "        z_species_2 = torch.cat((torch.zeros(X_2.shape[0],1),\n",
    "                                     torch.ones(X_2.shape[0],1)),1).to(device)\n",
    "            \n",
    "            \n",
    "        conditions = np.concatenate((df_pairs.conditionId.values,\n",
    "                                            df_1.conditionId.values,\n",
    "                                            df_pairs.conditionId.values,\n",
    "                                            df_2.conditionId.values))\n",
    "        size = conditions.size\n",
    "        conditions = conditions.reshape(size,1)\n",
    "        conditions = conditions == conditions.transpose()\n",
    "        conditions = conditions*1\n",
    "        mask = torch.tensor(conditions).to(device).detach()\n",
    "        pos_mask = mask\n",
    "        neg_mask = 1 - mask\n",
    "        log_2 = math.log(2.)\n",
    "        optimizer.zero_grad()\n",
    "        optimizer_adv.zero_grad()\n",
    "                        \n",
    "        #if e % model_params['adversary_steps']==0:\n",
    "        z_base_1 = encoder_1(X_1)\n",
    "        z_base_2 = encoder_2(X_2)\n",
    "        latent_base_vectors = torch.cat((z_base_1, z_base_2), 0)\n",
    "        labels_adv = adverse_classifier(latent_base_vectors)\n",
    "        true_labels = torch.cat((torch.ones(z_base_1.shape[0]),\n",
    "                                 torch.zeros(z_base_2.shape[0])),0).long().to(device)\n",
    "        _, predicted = torch.max(labels_adv, 1)\n",
    "        predicted = predicted.cpu().numpy()\n",
    "        cf_matrix = confusion_matrix(true_labels.cpu().numpy(),predicted)\n",
    "        tn, fp, fn, tp = cf_matrix.ravel()\n",
    "        f1_basal_trained = 2*tp/(2*tp+fp+fn)\n",
    "        adv_entropy = class_criterion(labels_adv,true_labels)\n",
    "        adversary_drugs_penalty = compute_gradients(labels_adv.sum(), latent_base_vectors)\n",
    "        loss_adv = adv_entropy + model_params['adv_penalnty'] * adversary_drugs_penalty\n",
    "        loss_adv.backward()\n",
    "        optimizer_adv.step()\n",
    "        #print(f1_basal_trained)\n",
    "        #else:\n",
    "        optimizer.zero_grad()\n",
    "        #f1_basal_trained = None\n",
    "        z_base_1 = encoder_1(X_1)\n",
    "        z_base_2 = encoder_2(X_2)\n",
    "        latent_base_vectors = torch.cat((z_base_1, z_base_2), 0)\n",
    "            \n",
    "        #z_un = local_d(torch.cat((z_1, z_2), 0))\n",
    "        z_un = local_d(latent_base_vectors)\n",
    "        res_un = torch.matmul(z_un, z_un.t())\n",
    "            \n",
    "        z_1 = Vsp(z_base_1,z_species_1)\n",
    "        z_2 = Vsp(z_base_2,z_species_2)\n",
    "        latent_vectors = torch.cat((z_1, z_2), 0)\n",
    "            \n",
    "        y_pred_1 = decoder_1(z_1)\n",
    "        fitLoss_1 = torch.mean(torch.sum((y_pred_1 - X_1)**2,dim=1))\n",
    "        L2Loss_1 = decoder_1.L2Regularization(model_params['dec_l2_reg']) + encoder_1.L2Regularization(model_params['enc_l2_reg'])\n",
    "        loss_1 = fitLoss_1 + L2Loss_1\n",
    "            \n",
    "        y_pred_2 = decoder_2(z_2)\n",
    "        fitLoss_2 = torch.mean(torch.sum((y_pred_2 - X_2)**2,dim=1))\n",
    "        L2Loss_2 = decoder_2.L2Regularization(model_params['dec_l2_reg']) + encoder_2.L2Regularization(model_params['enc_l2_reg'])\n",
    "        loss_2 = fitLoss_2 + L2Loss_2\n",
    "\n",
    "        silimalityLoss = torch.mean(torch.sum((z_base_1[0:paired_inds,:] - z_base_2[0:paired_inds,:])**2,dim=-1))\n",
    "        cosineLoss = torch.nn.functional.cosine_similarity(z_base_1[0:paired_inds,:],z_base_2[0:paired_inds,:],dim=-1).mean()\n",
    "            \n",
    "        p_samples = res_un * pos_mask.float()\n",
    "        q_samples = res_un * neg_mask.float()\n",
    "    \n",
    "        Ep = log_2 - F.softplus(- p_samples)\n",
    "        Eq = F.softplus(-q_samples) + q_samples - log_2\n",
    "\n",
    "        Ep = (Ep * pos_mask.float()).sum() / pos_mask.float().sum()\n",
    "        Eq = (Eq * neg_mask.float()).sum() / neg_mask.float().sum()\n",
    "        mi_loss = Eq - Ep\n",
    "\n",
    "        prior = torch.rand_like(latent_base_vectors)\n",
    "\n",
    "        term_a = torch.log(prior_d(prior)).mean()\n",
    "        term_b = torch.log(1.0 - prior_d(latent_base_vectors)).mean()\n",
    "        prior_loss = -(term_a + term_b) * model_params['prior_beta']\n",
    "\n",
    "        # Classification loss\n",
    "        labels = classifier(latent_vectors)\n",
    "        true_labels = torch.cat((torch.ones(z_1.shape[0]),\n",
    "                                 torch.zeros(z_2.shape[0])),0).long().to(device)\n",
    "        entropy = class_criterion(labels,true_labels)\n",
    "        _, predicted = torch.max(labels, 1)\n",
    "        predicted = predicted.cpu().numpy()\n",
    "        cf_matrix = confusion_matrix(true_labels.cpu().numpy(),predicted)\n",
    "        tn, fp, fn, tp = cf_matrix.ravel()\n",
    "        f1_latent = 2*tp/(2*tp+fp+fn)\n",
    "            \n",
    "        # Remove signal from z_basal\n",
    "        labels_adv = adverse_classifier(latent_base_vectors)\n",
    "        true_labels = torch.cat((torch.ones(z_base_1.shape[0]),\n",
    "                                 torch.zeros(z_base_2.shape[0])),0).long().to(device)\n",
    "        adv_entropy = class_criterion(labels_adv,true_labels)\n",
    "        _, predicted = torch.max(labels_adv, 1)\n",
    "        predicted = predicted.cpu().numpy()\n",
    "        cf_matrix = confusion_matrix(true_labels.cpu().numpy(),predicted)\n",
    "        tn, fp, fn, tp = cf_matrix.ravel()\n",
    "        f1_basal = 2*tp/(2*tp+fp+fn)\n",
    "            \n",
    "        loss = loss_1 + loss_2 + model_params['similarity_reg'] * silimalityLoss +model_params['lambda_mi_loss']*mi_loss + prior_loss  + model_params['reg_classifier'] * entropy - model_params['reg_adv']*adv_entropy +classifier.L2Regularization(model_params['state_class_reg']) +Vsp.Regularization(model_params['v_reg'])  - model_params['cosine_loss'] * cosineLoss\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "            \n",
    "        \n",
    "        pearson_1 = pearson_r(y_pred_1.detach().flatten(), X_1.detach().flatten())\n",
    "        r2_1 = r_square(y_pred_1.detach().flatten(), X_1.detach().flatten())\n",
    "        mse_1 = torch.mean(torch.mean((y_pred_1.detach() - X_1.detach())**2,dim=1))\n",
    "        \n",
    "        pearson_2 = pearson_r(y_pred_2.detach().flatten(), X_2.detach().flatten())\n",
    "        r2_2 = r_square(y_pred_2.detach().flatten(), X_2.detach().flatten())\n",
    "        mse_2 = torch.mean(torch.mean((y_pred_2.detach() - X_2.detach())**2,dim=1))            \n",
    "            \n",
    "    if model_params['schedule_step_adv'] is not None:\n",
    "        scheduler_adv.step()\n",
    "    if (e>=0):\n",
    "        scheduler.step()\n",
    "        outString = 'Try {:.0f}: Split {:.0f}: Epoch={:.0f}/{:.0f}'.format(fold_id,i,e+1,NUM_EPOCHS)\n",
    "        outString += ', r2_1={:.4f}'.format(r2_1.item())\n",
    "        outString += ', pearson_1={:.4f}'.format(pearson_1.item())\n",
    "        outString += ', MSE_1={:.4f}'.format(mse_1.item())\n",
    "        outString += ', r2_2={:.4f}'.format(r2_2.item())\n",
    "        outString += ', pearson_2={:.4f}'.format(pearson_2.item())\n",
    "        outString += ', MSE_2={:.4f}'.format(mse_2.item())\n",
    "        outString += ', MI Loss={:.4f}'.format(mi_loss.item())\n",
    "        outString += ', Prior Loss={:.4f}'.format(prior_loss.item())\n",
    "        outString += ', Entropy Loss={:.4f}'.format(entropy.item())\n",
    "        outString += ', Adverse Entropy={:.4f}'.format(adv_entropy.item())\n",
    "        outString += ', Cosine Loss={:.4f}'.format(cosineLoss.item())\n",
    "        outString += ', loss={:.4f}'.format(loss.item())\n",
    "        outString += ', F1 latent={:.4f}'.format(f1_latent)\n",
    "        outString += ', F1 basal={:.4f}'.format(f1_basal)\n",
    "        #if e % model_params[\"adversary_steps\"] == 0 and e>0:\n",
    "        outString += ', F1 basal trained={:.4f}'.format(f1_basal_trained)\n",
    "        #else:\n",
    "        #    outString += ', F1 basal trained= %s'%f1_basal_trained\n",
    "    if (e==0 or (e%250==0 and e>0)):\n",
    "        print2log(outString)\n",
    "print2log(outString)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
