{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "913b1b9d",
   "metadata": {},
   "source": [
    "# Load dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "18c35c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import silhouette_score,confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import tree\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "import scipy\n",
    "from scipy.signal import savgol_filter\n",
    "from scipy.spatial.distance import pdist,squareform\n",
    "from scipy.stats import mannwhitneyu\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from matplotlib import pyplot as plt\n",
    "import umap\n",
    "from models import SimpleEncoder,Decoder,PriorDiscriminator,LocalDiscriminator,Classifier\n",
    "from evaluationUtils import r_square,get_cindex,pearson_r,pseudoAccuracy\n",
    "from pathlib import Path\n",
    "from IPython.display import clear_output\n",
    "import seaborn as sns\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3a8500c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(message)s')\n",
    "logger = logging.getLogger()\n",
    "print2log = logger.info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7a4d73ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5ec26083",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a train generators\n",
    "def getSamples(N, batchSize):\n",
    "    order = np.random.permutation(N)\n",
    "    outList = []\n",
    "    while len(order)>0:\n",
    "        outList.append(order[0:batchSize])\n",
    "        order = order[batchSize:]\n",
    "    return outList"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f97e64",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8d9ca653",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>16</th>\n",
       "      <th>23</th>\n",
       "      <th>25</th>\n",
       "      <th>30</th>\n",
       "      <th>39</th>\n",
       "      <th>47</th>\n",
       "      <th>102</th>\n",
       "      <th>128</th>\n",
       "      <th>142</th>\n",
       "      <th>154</th>\n",
       "      <th>...</th>\n",
       "      <th>94239</th>\n",
       "      <th>116832</th>\n",
       "      <th>124583</th>\n",
       "      <th>147179</th>\n",
       "      <th>148022</th>\n",
       "      <th>200081</th>\n",
       "      <th>200734</th>\n",
       "      <th>256364</th>\n",
       "      <th>375346</th>\n",
       "      <th>388650</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>PCL001_HT29_24H:BRD-K42991516:10</th>\n",
       "      <td>0.266452</td>\n",
       "      <td>-0.250874</td>\n",
       "      <td>-0.854204</td>\n",
       "      <td>-0.041545</td>\n",
       "      <td>0.204450</td>\n",
       "      <td>0.709800</td>\n",
       "      <td>-0.328601</td>\n",
       "      <td>-0.498116</td>\n",
       "      <td>-1.454481</td>\n",
       "      <td>0.506321</td>\n",
       "      <td>...</td>\n",
       "      <td>0.536235</td>\n",
       "      <td>0.024452</td>\n",
       "      <td>0.928558</td>\n",
       "      <td>-0.453246</td>\n",
       "      <td>-0.140290</td>\n",
       "      <td>0.205065</td>\n",
       "      <td>1.148706</td>\n",
       "      <td>-1.933820</td>\n",
       "      <td>1.966937</td>\n",
       "      <td>-0.159919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PCL001_HT29_24H:BRD-K50817946:10</th>\n",
       "      <td>6.074023</td>\n",
       "      <td>-0.524075</td>\n",
       "      <td>-0.635742</td>\n",
       "      <td>2.014629</td>\n",
       "      <td>-3.747274</td>\n",
       "      <td>2.109600</td>\n",
       "      <td>0.847576</td>\n",
       "      <td>-2.732549</td>\n",
       "      <td>-5.729352</td>\n",
       "      <td>2.164091</td>\n",
       "      <td>...</td>\n",
       "      <td>0.447939</td>\n",
       "      <td>1.543649</td>\n",
       "      <td>-3.775020</td>\n",
       "      <td>1.827991</td>\n",
       "      <td>-0.088051</td>\n",
       "      <td>0.382848</td>\n",
       "      <td>1.400255</td>\n",
       "      <td>-3.087269</td>\n",
       "      <td>1.392148</td>\n",
       "      <td>1.027263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PCL001_HT29_24H:BRD-K58479490:10</th>\n",
       "      <td>4.145089</td>\n",
       "      <td>-0.881727</td>\n",
       "      <td>-1.720977</td>\n",
       "      <td>1.636901</td>\n",
       "      <td>1.614980</td>\n",
       "      <td>0.092948</td>\n",
       "      <td>0.711952</td>\n",
       "      <td>-0.088671</td>\n",
       "      <td>-1.531390</td>\n",
       "      <td>-0.591393</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.312573</td>\n",
       "      <td>0.095138</td>\n",
       "      <td>2.229333</td>\n",
       "      <td>0.250220</td>\n",
       "      <td>1.523056</td>\n",
       "      <td>-0.394704</td>\n",
       "      <td>-0.167089</td>\n",
       "      <td>0.833252</td>\n",
       "      <td>0.325481</td>\n",
       "      <td>-0.652675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DOSBIO001_A375_24H:BRD-K72343629:10.1316</th>\n",
       "      <td>1.545521</td>\n",
       "      <td>1.061800</td>\n",
       "      <td>1.165320</td>\n",
       "      <td>-1.052685</td>\n",
       "      <td>-3.449826</td>\n",
       "      <td>0.503872</td>\n",
       "      <td>1.850187</td>\n",
       "      <td>-0.426328</td>\n",
       "      <td>0.004190</td>\n",
       "      <td>1.948446</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.748134</td>\n",
       "      <td>-0.636907</td>\n",
       "      <td>1.142301</td>\n",
       "      <td>1.178548</td>\n",
       "      <td>5.263110</td>\n",
       "      <td>-0.141872</td>\n",
       "      <td>-1.490323</td>\n",
       "      <td>0.526244</td>\n",
       "      <td>1.637315</td>\n",
       "      <td>-0.829246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PCL001_HT29_24H:BRD-A89859721:0.12</th>\n",
       "      <td>-0.063263</td>\n",
       "      <td>0.358551</td>\n",
       "      <td>-0.024186</td>\n",
       "      <td>0.695202</td>\n",
       "      <td>-2.394504</td>\n",
       "      <td>0.329883</td>\n",
       "      <td>-0.117662</td>\n",
       "      <td>-0.779904</td>\n",
       "      <td>0.439334</td>\n",
       "      <td>3.228897</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.474038</td>\n",
       "      <td>-0.143447</td>\n",
       "      <td>1.764741</td>\n",
       "      <td>1.436673</td>\n",
       "      <td>0.602154</td>\n",
       "      <td>1.120865</td>\n",
       "      <td>-0.255665</td>\n",
       "      <td>0.316766</td>\n",
       "      <td>0.717193</td>\n",
       "      <td>-0.772269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DOSVAL001_HT29_24H:BRD-K74587389:10</th>\n",
       "      <td>1.385211</td>\n",
       "      <td>0.296959</td>\n",
       "      <td>0.557355</td>\n",
       "      <td>1.468583</td>\n",
       "      <td>-1.779466</td>\n",
       "      <td>-1.511970</td>\n",
       "      <td>3.965060</td>\n",
       "      <td>-0.564973</td>\n",
       "      <td>-3.802075</td>\n",
       "      <td>1.090815</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.506828</td>\n",
       "      <td>-0.669734</td>\n",
       "      <td>3.007058</td>\n",
       "      <td>0.830324</td>\n",
       "      <td>3.094987</td>\n",
       "      <td>1.875227</td>\n",
       "      <td>1.484006</td>\n",
       "      <td>-2.704566</td>\n",
       "      <td>-0.136536</td>\n",
       "      <td>0.028342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DOSVAL001_HT29_24H:BRD-K11432829:10.0841</th>\n",
       "      <td>1.846355</td>\n",
       "      <td>-1.135175</td>\n",
       "      <td>-0.396850</td>\n",
       "      <td>-0.564071</td>\n",
       "      <td>-3.391137</td>\n",
       "      <td>-1.861201</td>\n",
       "      <td>-0.696921</td>\n",
       "      <td>-0.365471</td>\n",
       "      <td>-2.643652</td>\n",
       "      <td>-0.320416</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.298227</td>\n",
       "      <td>-0.076363</td>\n",
       "      <td>-1.810653</td>\n",
       "      <td>0.400317</td>\n",
       "      <td>-0.364554</td>\n",
       "      <td>1.320417</td>\n",
       "      <td>-2.804159</td>\n",
       "      <td>-0.560566</td>\n",
       "      <td>-0.308718</td>\n",
       "      <td>0.595375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DOSVAL004_HT29_24H:BRD-K50266413:20</th>\n",
       "      <td>-0.884172</td>\n",
       "      <td>0.803241</td>\n",
       "      <td>0.803791</td>\n",
       "      <td>0.287481</td>\n",
       "      <td>1.597376</td>\n",
       "      <td>-1.322457</td>\n",
       "      <td>-0.825672</td>\n",
       "      <td>-0.100797</td>\n",
       "      <td>0.305771</td>\n",
       "      <td>-0.241062</td>\n",
       "      <td>...</td>\n",
       "      <td>0.613587</td>\n",
       "      <td>0.321272</td>\n",
       "      <td>-1.079701</td>\n",
       "      <td>0.374976</td>\n",
       "      <td>-0.712192</td>\n",
       "      <td>0.427770</td>\n",
       "      <td>-0.165815</td>\n",
       "      <td>0.702094</td>\n",
       "      <td>3.054970</td>\n",
       "      <td>0.956917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DOSVAL002_A375_24H:BRD-K10749593:20</th>\n",
       "      <td>4.415511</td>\n",
       "      <td>0.608378</td>\n",
       "      <td>1.604217</td>\n",
       "      <td>-0.911175</td>\n",
       "      <td>-2.611416</td>\n",
       "      <td>-1.742975</td>\n",
       "      <td>-2.500287</td>\n",
       "      <td>-2.503129</td>\n",
       "      <td>-3.472708</td>\n",
       "      <td>3.008501</td>\n",
       "      <td>...</td>\n",
       "      <td>0.916562</td>\n",
       "      <td>-1.403280</td>\n",
       "      <td>0.479218</td>\n",
       "      <td>4.528471</td>\n",
       "      <td>1.701896</td>\n",
       "      <td>0.141621</td>\n",
       "      <td>1.953133</td>\n",
       "      <td>-1.480089</td>\n",
       "      <td>1.549125</td>\n",
       "      <td>1.414482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DOSVAL001_HT29_24H:BRD-K11624501:9.99164</th>\n",
       "      <td>1.540175</td>\n",
       "      <td>-0.196926</td>\n",
       "      <td>-0.094410</td>\n",
       "      <td>-1.951286</td>\n",
       "      <td>-2.848082</td>\n",
       "      <td>-2.478519</td>\n",
       "      <td>-1.257487</td>\n",
       "      <td>-1.247405</td>\n",
       "      <td>-4.006328</td>\n",
       "      <td>-0.362494</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.371798</td>\n",
       "      <td>3.735393</td>\n",
       "      <td>2.011243</td>\n",
       "      <td>1.693114</td>\n",
       "      <td>2.924200</td>\n",
       "      <td>2.535851</td>\n",
       "      <td>1.861230</td>\n",
       "      <td>-3.021530</td>\n",
       "      <td>0.127304</td>\n",
       "      <td>0.980487</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2276 rows × 978 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                16        23        25  \\\n",
       "PCL001_HT29_24H:BRD-K42991516:10          0.266452 -0.250874 -0.854204   \n",
       "PCL001_HT29_24H:BRD-K50817946:10          6.074023 -0.524075 -0.635742   \n",
       "PCL001_HT29_24H:BRD-K58479490:10          4.145089 -0.881727 -1.720977   \n",
       "DOSBIO001_A375_24H:BRD-K72343629:10.1316  1.545521  1.061800  1.165320   \n",
       "PCL001_HT29_24H:BRD-A89859721:0.12       -0.063263  0.358551 -0.024186   \n",
       "...                                            ...       ...       ...   \n",
       "DOSVAL001_HT29_24H:BRD-K74587389:10       1.385211  0.296959  0.557355   \n",
       "DOSVAL001_HT29_24H:BRD-K11432829:10.0841  1.846355 -1.135175 -0.396850   \n",
       "DOSVAL004_HT29_24H:BRD-K50266413:20      -0.884172  0.803241  0.803791   \n",
       "DOSVAL002_A375_24H:BRD-K10749593:20       4.415511  0.608378  1.604217   \n",
       "DOSVAL001_HT29_24H:BRD-K11624501:9.99164  1.540175 -0.196926 -0.094410   \n",
       "\n",
       "                                                30        39        47  \\\n",
       "PCL001_HT29_24H:BRD-K42991516:10         -0.041545  0.204450  0.709800   \n",
       "PCL001_HT29_24H:BRD-K50817946:10          2.014629 -3.747274  2.109600   \n",
       "PCL001_HT29_24H:BRD-K58479490:10          1.636901  1.614980  0.092948   \n",
       "DOSBIO001_A375_24H:BRD-K72343629:10.1316 -1.052685 -3.449826  0.503872   \n",
       "PCL001_HT29_24H:BRD-A89859721:0.12        0.695202 -2.394504  0.329883   \n",
       "...                                            ...       ...       ...   \n",
       "DOSVAL001_HT29_24H:BRD-K74587389:10       1.468583 -1.779466 -1.511970   \n",
       "DOSVAL001_HT29_24H:BRD-K11432829:10.0841 -0.564071 -3.391137 -1.861201   \n",
       "DOSVAL004_HT29_24H:BRD-K50266413:20       0.287481  1.597376 -1.322457   \n",
       "DOSVAL002_A375_24H:BRD-K10749593:20      -0.911175 -2.611416 -1.742975   \n",
       "DOSVAL001_HT29_24H:BRD-K11624501:9.99164 -1.951286 -2.848082 -2.478519   \n",
       "\n",
       "                                               102       128       142  \\\n",
       "PCL001_HT29_24H:BRD-K42991516:10         -0.328601 -0.498116 -1.454481   \n",
       "PCL001_HT29_24H:BRD-K50817946:10          0.847576 -2.732549 -5.729352   \n",
       "PCL001_HT29_24H:BRD-K58479490:10          0.711952 -0.088671 -1.531390   \n",
       "DOSBIO001_A375_24H:BRD-K72343629:10.1316  1.850187 -0.426328  0.004190   \n",
       "PCL001_HT29_24H:BRD-A89859721:0.12       -0.117662 -0.779904  0.439334   \n",
       "...                                            ...       ...       ...   \n",
       "DOSVAL001_HT29_24H:BRD-K74587389:10       3.965060 -0.564973 -3.802075   \n",
       "DOSVAL001_HT29_24H:BRD-K11432829:10.0841 -0.696921 -0.365471 -2.643652   \n",
       "DOSVAL004_HT29_24H:BRD-K50266413:20      -0.825672 -0.100797  0.305771   \n",
       "DOSVAL002_A375_24H:BRD-K10749593:20      -2.500287 -2.503129 -3.472708   \n",
       "DOSVAL001_HT29_24H:BRD-K11624501:9.99164 -1.257487 -1.247405 -4.006328   \n",
       "\n",
       "                                               154  ...     94239    116832  \\\n",
       "PCL001_HT29_24H:BRD-K42991516:10          0.506321  ...  0.536235  0.024452   \n",
       "PCL001_HT29_24H:BRD-K50817946:10          2.164091  ...  0.447939  1.543649   \n",
       "PCL001_HT29_24H:BRD-K58479490:10         -0.591393  ... -0.312573  0.095138   \n",
       "DOSBIO001_A375_24H:BRD-K72343629:10.1316  1.948446  ... -1.748134 -0.636907   \n",
       "PCL001_HT29_24H:BRD-A89859721:0.12        3.228897  ... -0.474038 -0.143447   \n",
       "...                                            ...  ...       ...       ...   \n",
       "DOSVAL001_HT29_24H:BRD-K74587389:10       1.090815  ... -0.506828 -0.669734   \n",
       "DOSVAL001_HT29_24H:BRD-K11432829:10.0841 -0.320416  ... -0.298227 -0.076363   \n",
       "DOSVAL004_HT29_24H:BRD-K50266413:20      -0.241062  ...  0.613587  0.321272   \n",
       "DOSVAL002_A375_24H:BRD-K10749593:20       3.008501  ...  0.916562 -1.403280   \n",
       "DOSVAL001_HT29_24H:BRD-K11624501:9.99164 -0.362494  ... -0.371798  3.735393   \n",
       "\n",
       "                                            124583    147179    148022  \\\n",
       "PCL001_HT29_24H:BRD-K42991516:10          0.928558 -0.453246 -0.140290   \n",
       "PCL001_HT29_24H:BRD-K50817946:10         -3.775020  1.827991 -0.088051   \n",
       "PCL001_HT29_24H:BRD-K58479490:10          2.229333  0.250220  1.523056   \n",
       "DOSBIO001_A375_24H:BRD-K72343629:10.1316  1.142301  1.178548  5.263110   \n",
       "PCL001_HT29_24H:BRD-A89859721:0.12        1.764741  1.436673  0.602154   \n",
       "...                                            ...       ...       ...   \n",
       "DOSVAL001_HT29_24H:BRD-K74587389:10       3.007058  0.830324  3.094987   \n",
       "DOSVAL001_HT29_24H:BRD-K11432829:10.0841 -1.810653  0.400317 -0.364554   \n",
       "DOSVAL004_HT29_24H:BRD-K50266413:20      -1.079701  0.374976 -0.712192   \n",
       "DOSVAL002_A375_24H:BRD-K10749593:20       0.479218  4.528471  1.701896   \n",
       "DOSVAL001_HT29_24H:BRD-K11624501:9.99164  2.011243  1.693114  2.924200   \n",
       "\n",
       "                                            200081    200734    256364  \\\n",
       "PCL001_HT29_24H:BRD-K42991516:10          0.205065  1.148706 -1.933820   \n",
       "PCL001_HT29_24H:BRD-K50817946:10          0.382848  1.400255 -3.087269   \n",
       "PCL001_HT29_24H:BRD-K58479490:10         -0.394704 -0.167089  0.833252   \n",
       "DOSBIO001_A375_24H:BRD-K72343629:10.1316 -0.141872 -1.490323  0.526244   \n",
       "PCL001_HT29_24H:BRD-A89859721:0.12        1.120865 -0.255665  0.316766   \n",
       "...                                            ...       ...       ...   \n",
       "DOSVAL001_HT29_24H:BRD-K74587389:10       1.875227  1.484006 -2.704566   \n",
       "DOSVAL001_HT29_24H:BRD-K11432829:10.0841  1.320417 -2.804159 -0.560566   \n",
       "DOSVAL004_HT29_24H:BRD-K50266413:20       0.427770 -0.165815  0.702094   \n",
       "DOSVAL002_A375_24H:BRD-K10749593:20       0.141621  1.953133 -1.480089   \n",
       "DOSVAL001_HT29_24H:BRD-K11624501:9.99164  2.535851  1.861230 -3.021530   \n",
       "\n",
       "                                            375346    388650  \n",
       "PCL001_HT29_24H:BRD-K42991516:10          1.966937 -0.159919  \n",
       "PCL001_HT29_24H:BRD-K50817946:10          1.392148  1.027263  \n",
       "PCL001_HT29_24H:BRD-K58479490:10          0.325481 -0.652675  \n",
       "DOSBIO001_A375_24H:BRD-K72343629:10.1316  1.637315 -0.829246  \n",
       "PCL001_HT29_24H:BRD-A89859721:0.12        0.717193 -0.772269  \n",
       "...                                            ...       ...  \n",
       "DOSVAL001_HT29_24H:BRD-K74587389:10      -0.136536  0.028342  \n",
       "DOSVAL001_HT29_24H:BRD-K11432829:10.0841 -0.308718  0.595375  \n",
       "DOSVAL004_HT29_24H:BRD-K50266413:20       3.054970  0.956917  \n",
       "DOSVAL002_A375_24H:BRD-K10749593:20       1.549125  1.414482  \n",
       "DOSVAL001_HT29_24H:BRD-K11624501:9.99164  0.127304  0.980487  \n",
       "\n",
       "[2276 rows x 978 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Gex data \n",
    "#cmap = pd.read_csv('../preprocessing/preprocessed_data/cmap_HA1E_PC3.csv',index_col=0)\n",
    "cmap = pd.read_csv('../preprocessing/preprocessed_data/cmap_landmarks_HT29_A375.csv',index_col=0)\n",
    "gene_size = len(cmap.columns)\n",
    "X = cmap.values\n",
    "display(cmap)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75d5ecf9",
   "metadata": {},
   "source": [
    "# Train one trasnlation model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "01186e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_params = {'encoder_1_hiddens':[640,384],\n",
    "                'encoder_2_hiddens':[640,384],\n",
    "                'latent_dim': 292,\n",
    "                'decoder_1_hiddens':[384,640],\n",
    "                'decoder_2_hiddens':[384,640],\n",
    "                'dropout_decoder':0.2,\n",
    "                'dropout_encoder':0.1,\n",
    "                'encoder_activation':torch.nn.ELU(),\n",
    "                'decoder_activation':torch.nn.ELU(),\n",
    "                'V_dropout':0.25,\n",
    "                'state_class_hidden':[256,128,64],\n",
    "                'state_class_drop_in':0.5,\n",
    "                'state_class_drop':0.25,\n",
    "                'no_states':2,\n",
    "                'adv_class_hidden':[256,128,64],\n",
    "                'adv_class_drop_in':0.3,\n",
    "                'adv_class_drop':0.1,\n",
    "                'no_adv_class':2,\n",
    "                'encoding_lr':0.001,\n",
    "                'adv_lr':0.001,\n",
    "                'schedule_step_adv':200,\n",
    "                'gamma_adv':0.5,\n",
    "                'schedule_step_enc':200,\n",
    "                'gamma_enc':0.8,\n",
    "                'batch_size_1':178,\n",
    "                'batch_size_2':154,\n",
    "                'batch_size_paired':90,\n",
    "                'epochs':1000,\n",
    "                'prior_beta':1.0,\n",
    "                'no_folds':10,\n",
    "                'v_reg':1e-04,\n",
    "                'state_class_reg':1e-02,\n",
    "                'enc_l2_reg':0.01,\n",
    "                'dec_l2_reg':0.01,\n",
    "                'lambda_mi_loss':100,\n",
    "                'effsize_reg': 100,\n",
    "                'cosine_loss': 10,\n",
    "                'adv_penalnty':100,\n",
    "                'reg_adv':1000,\n",
    "                'reg_classifier': 1000,\n",
    "                'similarity_reg' : 10,\n",
    "                'adversary_steps':4,\n",
    "                'autoencoder_wd': 0.,\n",
    "                'adversary_wd': 0.}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "92abbd27",
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dimensions = [32,16,8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9a9327fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_criterion = torch.nn.CrossEntropyLoss()\n",
    "bs_1 = model_params['batch_size_1']\n",
    "bs_2 = model_params['batch_size_2']\n",
    "bs_paired = model_params['batch_size_paired']\n",
    "NUM_EPOCHS=model_params['epochs']\n",
    "num_genes = cmap.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f303b296",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Start model with latent dimension = 32\n",
      "Split 1: Epoch=1/1000, r2_1=-5.1841, pearson_1=0.3583, MSE_1=4.1566, r2_2=-3.0661, pearson_2=0.3537, MSE_2=2.7395, MI Loss=2.2475, Prior Loss=1.3300, Entropy Loss=0.7254, F1=0.2143, Accuracy=0.4762, loss=6998.1797\n",
      "Split 1: Epoch=251/1000, r2_1=0.3915, pearson_1=0.7473, MSE_1=1.2088, r2_2=0.4697, pearson_2=0.7747, MSE_2=1.1271, MI Loss=-0.6053, Prior Loss=0.0049, Entropy Loss=0.7137, F1=0.2963, Accuracy=0.5476, loss=2433.3401\n",
      "Split 1: Epoch=501/1000, r2_1=0.5274, pearson_1=0.8209, MSE_1=1.0998, r2_2=0.5521, pearson_2=0.7709, MSE_2=0.9292, MI Loss=-0.6174, Prior Loss=0.0000, Entropy Loss=0.7371, F1=0.1600, Accuracy=0.5000, loss=2123.3826\n",
      "Split 1: Epoch=751/1000, r2_1=0.4795, pearson_1=0.8165, MSE_1=1.2026, r2_2=0.4974, pearson_2=0.8451, MSE_2=1.1478, MI Loss=-0.6256, Prior Loss=0.0000, Entropy Loss=0.7192, F1=0.2143, Accuracy=0.4762, loss=2431.1436\n",
      "Split 1: Epoch=1000/1000, r2_1=0.4975, pearson_1=0.7737, MSE_1=1.1377, r2_2=0.5866, pearson_2=0.8274, MSE_2=0.9756, MI Loss=-0.6297, Prior Loss=0.0000, Entropy Loss=0.7051, F1=0.1600, Accuracy=0.5000, loss=2195.5610\n",
      "Classification accuracy: 0.5434782608695652\n",
      "Classification F1 score: 0.23357664233576642\n",
      "Pearson correlation 1: 0.7988317012786865\n",
      "Pearson correlation 2: 0.8212836384773254\n",
      "Pearson correlation 1 to 2: 0.8212836384773254\n",
      "Pearson correlation 2 to 1: 0.6240023374557495\n",
      "Split 2: Epoch=1/1000, r2_1=-3.9951, pearson_1=0.2776, MSE_1=3.0029, r2_2=-2.2664, pearson_2=0.3296, MSE_2=2.2272, MI Loss=2.7732, Prior Loss=1.3209, Entropy Loss=0.7627, F1=0.3243, Accuracy=0.4048, loss=5387.4707\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-8b02a5db5729>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    112\u001b[0m                 X_1 = torch.tensor(np.concatenate((cmap.loc[df_pairs['sig_id.x']].values,\n\u001b[0;32m    113\u001b[0m                                                      cmap.loc[df_1.sig_id].values))).float().to(device)\n\u001b[1;32m--> 114\u001b[1;33m                 X_2 = torch.tensor(np.concatenate((cmap.loc[df_pairs['sig_id.y']].values,\n\u001b[0m\u001b[0;32m    115\u001b[0m                                                      cmap.loc[df_2.sig_id].values))).float().to(device)\n\u001b[0;32m    116\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mconcatenate\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "df_result_all = pd.DataFrame({})\n",
    "for latent_dim in latent_dimensions:\n",
    "    print2log('Start model with latent dimension = %s'%latent_dim)\n",
    "    latent_dim = int(latent_dim)\n",
    "    Path('../results/LatentDimAnalysis/'+str(latent_dim)+'/models').mkdir(parents=True, exist_ok=True)\n",
    "    valPear = []\n",
    "    valPear_1 = []\n",
    "    valPear_2 = []\n",
    "    valF1 = []\n",
    "    valClassAcc =[]\n",
    "    #if latent_dim <32:\n",
    "    #    model_params['state_class_hidden'] = [16,8,4]\n",
    "    #elif latent_dim <=64:\n",
    "    #    model_params['state_class_hidden'] = [32,16,8]\n",
    "    #elif latent_dim <=128:\n",
    "    #    model_params['state_class_hidden'] = [128,64,32]\n",
    "    \n",
    "    for i in range(model_params['no_folds']):\n",
    "        trainInfo_paired = pd.read_csv('../preprocessing/preprocessed_data/10fold_validation_spit/train_paired_%s.csv'%i,index_col=0)\n",
    "        trainInfo_1 = pd.read_csv('../preprocessing/preprocessed_data/10fold_validation_spit/train_a375_%s.csv'%i,index_col=0)\n",
    "        trainInfo_2 = pd.read_csv('../preprocessing/preprocessed_data/10fold_validation_spit/train_ht29_%s.csv'%i,index_col=0)\n",
    "        valInfo_paired = pd.read_csv('../preprocessing/preprocessed_data/10fold_validation_spit/val_paired_%s.csv'%i,index_col=0)\n",
    "        valInfo_1 = pd.read_csv('../preprocessing/preprocessed_data/10fold_validation_spit/val_a375_%s.csv'%i,index_col=0)\n",
    "        valInfo_2 = pd.read_csv('../preprocessing/preprocessed_data/10fold_validation_spit/val_ht29_%s.csv'%i,index_col=0)\n",
    "        \n",
    "        N_paired = len(trainInfo_paired)\n",
    "        N_1 = len(trainInfo_1)\n",
    "        N_2 = len(trainInfo_2)\n",
    "        N = N_1\n",
    "        if N_2>N:\n",
    "            N=N_2\n",
    "        # Network\n",
    "        decoder_1 = Decoder(latent_dim,model_params['decoder_1_hiddens'],num_genes,\n",
    "                                dropRate=model_params['dropout_decoder'], \n",
    "                                activation=model_params['decoder_activation']).to(device)\n",
    "        decoder_2 = Decoder(latent_dim,model_params['decoder_2_hiddens'],num_genes,\n",
    "                                dropRate=model_params['dropout_decoder'], \n",
    "                                activation=model_params['decoder_activation']).to(device)\n",
    "        encoder_1 = SimpleEncoder(num_genes,model_params['encoder_1_hiddens'],latent_dim,\n",
    "                                      dropRate=model_params['dropout_encoder'], \n",
    "                                      activation=model_params['encoder_activation'],\n",
    "                                     normalizeOutput=False).to(device)\n",
    "        encoder_2 = SimpleEncoder(num_genes,model_params['encoder_2_hiddens'],latent_dim,\n",
    "                                          dropRate=model_params['dropout_encoder'], \n",
    "                                          activation=model_params['encoder_activation'],\n",
    "                                     normalizeOutput=False).to(device)\n",
    "        classifier = Classifier(in_channel=latent_dim,\n",
    "                            hidden_layers=model_params['state_class_hidden'],\n",
    "                            num_classes=model_params['no_states'],\n",
    "                            drop_in=model_params['state_class_drop_in'],\n",
    "                            drop=model_params['state_class_drop']).to(device)\n",
    "        prior_d = PriorDiscriminator(latent_dim).to(device)\n",
    "        local_d = LocalDiscriminator(latent_dim,latent_dim).to(device)\n",
    "\n",
    "        allParams = list(decoder_1.parameters()) + list(encoder_1.parameters())\n",
    "        allParams = allParams + list(decoder_2.parameters()) + list(encoder_2.parameters())\n",
    "        allParams = allParams  + list(local_d.parameters())\n",
    "        allParams = allParams + list(prior_d.parameters())\n",
    "        optimizer = torch.optim.Adam(allParams, lr=model_params['encoding_lr'])\n",
    "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n",
    "                                                        step_size=model_params['schedule_step_enc'],\n",
    "                                                        gamma=model_params['gamma_enc'])\n",
    "        \n",
    "        trainLoss = []\n",
    "        trainLossSTD = []\n",
    "        for e in range(NUM_EPOCHS):\n",
    "            trainloader_1 = getSamples(N_1, bs_1)\n",
    "            len_1 = len(trainloader_1)\n",
    "            trainloader_2 = getSamples(N_2, bs_2)\n",
    "            len_2 = len(trainloader_2)\n",
    "            trainloader_paired = getSamples(N_paired, bs_paired)\n",
    "            len_paired = len(trainloader_paired)\n",
    "\n",
    "            lens = [len_1,len_2,len_paired]\n",
    "            maxLen = np.max(lens)\n",
    "\n",
    "            if maxLen>lens[0]:\n",
    "                trainloader_suppl = getSamples(N_1, bs_1)\n",
    "                for jj in range(maxLen-lens[0]):\n",
    "                    trainloader_1.insert(jj,trainloader_suppl[jj])\n",
    "\n",
    "            if maxLen>lens[1]:\n",
    "                trainloader_suppl = getSamples(N_2, bs_2)\n",
    "                for jj in range(maxLen-lens[1]):\n",
    "                    trainloader_2.insert(jj,trainloader_suppl[jj])\n",
    "\n",
    "            if maxLen>lens[2]:\n",
    "                trainloader_suppl = getSamples(N_paired, bs_paired)\n",
    "                for jj in range(maxLen-lens[2]):\n",
    "                    trainloader_paired.insert(jj,trainloader_suppl[jj])\n",
    "            encoder_1.train()\n",
    "            decoder_1.train()\n",
    "            encoder_2.train()\n",
    "            decoder_2.train()\n",
    "            prior_d.train()\n",
    "            local_d.train()\n",
    "            classifier.train()\n",
    "\n",
    "            trainLoss_ALL = []\n",
    "            \n",
    "            for j in range(maxLen):\n",
    "                dataIndex_1 = trainloader_1[j]\n",
    "                dataIndex_2 = trainloader_2[j]\n",
    "                dataIndex_paired = trainloader_paired[j]\n",
    "\n",
    "                df_pairs = trainInfo_paired.iloc[dataIndex_paired,:]\n",
    "                df_1 = trainInfo_1.iloc[dataIndex_1,:]\n",
    "                df_2 = trainInfo_2.iloc[dataIndex_2,:]\n",
    "                paired_inds = len(df_pairs)\n",
    "\n",
    "\n",
    "                X_1 = torch.tensor(np.concatenate((cmap.loc[df_pairs['sig_id.x']].values,\n",
    "                                                     cmap.loc[df_1.sig_id].values))).float().to(device)\n",
    "                X_2 = torch.tensor(np.concatenate((cmap.loc[df_pairs['sig_id.y']].values,\n",
    "                                                     cmap.loc[df_2.sig_id].values))).float().to(device)\n",
    "\n",
    "\n",
    "                conditions = np.concatenate((df_pairs.conditionId.values,\n",
    "                                             df_1.conditionId.values,\n",
    "                                             df_pairs.conditionId.values,\n",
    "                                             df_2.conditionId.values))\n",
    "                size = conditions.size\n",
    "                conditions = conditions.reshape(size,1)\n",
    "                conditions = conditions == conditions.transpose()\n",
    "                conditions = conditions*1\n",
    "                mask = torch.tensor(conditions).to(device).detach()\n",
    "                pos_mask = mask\n",
    "                neg_mask = 1 - mask\n",
    "                log_2 = math.log(2.)\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                z_1 = encoder_1(X_1)\n",
    "                z_2 = encoder_2(X_2)\n",
    "\n",
    "                z_un = local_d(torch.cat((z_1, z_2), 0))\n",
    "                res_un = torch.matmul(z_un, z_un.t())\n",
    "\n",
    "                y_pred_1 = decoder_1(z_1)\n",
    "                fitLoss_1 = torch.mean(torch.sum((y_pred_1 - X_1)**2,dim=1))\n",
    "                L2Loss_1 = decoder_1.L2Regularization(0.01) + encoder_1.L2Regularization(0.01)\n",
    "                loss_1 = fitLoss_1 + L2Loss_1\n",
    "\n",
    "                y_pred_2 = decoder_2(z_2)\n",
    "                fitLoss_2 = torch.mean(torch.sum((y_pred_2 - X_2)**2,dim=1))\n",
    "                L2Loss_2 = decoder_2.L2Regularization(0.01) + encoder_2.L2Regularization(0.01)\n",
    "                loss_2 = fitLoss_2 + L2Loss_2\n",
    "\n",
    "                silimalityLoss = torch.mean(torch.sum((z_1[0:paired_inds,:] - z_2[0:paired_inds,:])**2,dim=-1))\n",
    "\n",
    "                p_samples = res_un * pos_mask.float()\n",
    "                q_samples = res_un * neg_mask.float()\n",
    "\n",
    "                Ep = log_2 - F.softplus(- p_samples)\n",
    "                Eq = F.softplus(-q_samples) + q_samples - log_2\n",
    "\n",
    "                Ep = (Ep * pos_mask.float()).sum() / pos_mask.float().sum()\n",
    "                Eq = (Eq * neg_mask.float()).sum() / neg_mask.float().sum()\n",
    "                mi_loss = Eq - Ep\n",
    "\n",
    "                prior = torch.rand_like(torch.cat((z_1, z_2), 0))\n",
    "\n",
    "                term_a = torch.log(prior_d(prior)).mean()\n",
    "                term_b = torch.log(1.0 - prior_d(torch.cat((z_1, z_2), 0))).mean()\n",
    "                prior_loss = -(term_a + term_b) * model_params['prior_beta']\n",
    "\n",
    "                # Classification loss\n",
    "                labels = classifier(torch.cat((z_1, z_2), 0))\n",
    "                true_labels = torch.cat((torch.ones(z_1.shape[0]),\n",
    "                                         torch.zeros(z_2.shape[0])),0).long().to(device)\n",
    "                entropy = class_criterion(labels,true_labels)\n",
    "                _, predicted = torch.max(labels, 1)\n",
    "                predicted = predicted.cpu().numpy()\n",
    "                cf_matrix = confusion_matrix(true_labels.cpu().numpy(),predicted)\n",
    "                tn, fp, fn, tp = cf_matrix.ravel()\n",
    "                class_acc = (tp+tn)/predicted.size\n",
    "                f1 = 2*tp/(2*tp+fp+fn)\n",
    "\n",
    "                loss = loss_1 + loss_2 + mi_loss + prior_loss + silimalityLoss + 100*entropy +classifier.L2Regularization(1e-2)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                pearson_1 = pearson_r(y_pred_1.detach().flatten(), X_1.detach().flatten())\n",
    "                r2_1 = r_square(y_pred_1.detach().flatten(), X_1.detach().flatten())\n",
    "                mse_1 = torch.mean(torch.mean((y_pred_1.detach() - X_1.detach())**2,dim=1))\n",
    "\n",
    "                pearson_2 = pearson_r(y_pred_2.detach().flatten(), X_2.detach().flatten())\n",
    "                r2_2 = r_square(y_pred_2.detach().flatten(), X_2.detach().flatten())\n",
    "                mse_2 = torch.mean(torch.mean((y_pred_2.detach() - X_2.detach())**2,dim=1))\n",
    "\n",
    "\n",
    "            scheduler.step()\n",
    "            outString = 'Split {:.0f}: Epoch={:.0f}/{:.0f}'.format(i+1,e+1,NUM_EPOCHS)\n",
    "            outString += ', r2_1={:.4f}'.format(r2_1.item())\n",
    "            outString += ', pearson_1={:.4f}'.format(pearson_1.item())\n",
    "            outString += ', MSE_1={:.4f}'.format(mse_1.item())\n",
    "            outString += ', r2_2={:.4f}'.format(r2_2.item())\n",
    "            outString += ', pearson_2={:.4f}'.format(pearson_2.item())\n",
    "            outString += ', MSE_2={:.4f}'.format(mse_2.item())\n",
    "            outString += ', MI Loss={:.4f}'.format(mi_loss.item())\n",
    "            outString += ', Prior Loss={:.4f}'.format(prior_loss.item())\n",
    "            outString += ', Entropy Loss={:.4f}'.format(entropy.item())\n",
    "            outString += ', F1={:.4f}'.format(f1)\n",
    "            outString += ', Accuracy={:.4f}'.format(class_acc)\n",
    "            outString += ', loss={:.4f}'.format(loss.item())\n",
    "            if (e%250==0):\n",
    "                print2log(outString)\n",
    "        print2log(outString)\n",
    "        #trainLoss.append(splitLoss)\n",
    "        decoder_1.eval()\n",
    "        decoder_2.eval()\n",
    "        encoder_1.eval()\n",
    "        encoder_2.eval()\n",
    "        prior_d.eval()\n",
    "        local_d.eval()\n",
    "        classifier.eval()\n",
    "        #model.eval()\n",
    "        #master_encoder.eval()\n",
    "\n",
    "        paired_val_inds = len(valInfo_paired)\n",
    "        x_1 = torch.tensor(np.concatenate((cmap.loc[valInfo_paired['sig_id.x']].values,\n",
    "                                              cmap.loc[valInfo_1.sig_id].values))).float().to(device)\n",
    "        x_2 = torch.tensor(np.concatenate((cmap.loc[valInfo_paired['sig_id.y']].values,\n",
    "                                              cmap.loc[valInfo_2.sig_id].values))).float().to(device)\n",
    "\n",
    "        z_latent_1 = encoder_1(x_1)\n",
    "        z_latent_2 = encoder_2(x_2)\n",
    "\n",
    "        labels = classifier(torch.cat((z_latent_1, z_latent_2), 0))\n",
    "        true_labels = torch.cat((torch.ones(z_latent_1.shape[0]).view(z_latent_1.shape[0],1),\n",
    "                                 torch.zeros(z_latent_2.shape[0]).view(z_latent_2.shape[0],1)),0).long()\n",
    "        _, predicted = torch.max(labels, 1)\n",
    "        predicted = predicted.cpu().numpy()\n",
    "        cf_matrix = confusion_matrix(true_labels.numpy(),predicted)\n",
    "        tn, fp, fn, tp = cf_matrix.ravel()\n",
    "        class_acc = (tp+tn)/predicted.size\n",
    "        f1 = 2*tp/(2*tp+fp+fn)\n",
    "\n",
    "        valF1.append(f1)\n",
    "        valClassAcc.append(class_acc)\n",
    "\n",
    "        print2log('Classification accuracy: %s'%class_acc)\n",
    "        print2log('Classification F1 score: %s'%f1)\n",
    "\n",
    "        xhat_1 = decoder_1(z_latent_1)\n",
    "        xhat_2 = decoder_2(z_latent_2)\n",
    "\n",
    "\n",
    "        pearson_1 = pearson_r(xhat_1.detach().flatten(), x_1.detach().flatten())\n",
    "        pearson_2 = pearson_r(xhat_2.detach().flatten(), x_2.detach().flatten())\n",
    "        valPear_1.append(pearson_1.item())\n",
    "        valPear_2.append(pearson_2.item())\n",
    "        print2log('Pearson correlation 1: %s'%pearson_1.item())\n",
    "        print2log('Pearson correlation 2: %s'%pearson_2.item())\n",
    "\n",
    "        x_1_equivalent = x_1[0:paired_val_inds,:]\n",
    "        x_2_equivalent = x_2[0:paired_val_inds,:]\n",
    "\n",
    "        z_latent_1_equivalent  = encoder_1(x_1_equivalent)\n",
    "        x_hat_2_equivalent = decoder_2(z_latent_1_equivalent).detach()\n",
    "        print2log('Pearson correlation 1 to 2: %s'%pearson_2.item())\n",
    "        \n",
    "        z_latent_2_equivalent  = encoder_2(x_2_equivalent)\n",
    "        x_hat_1_equivalent = decoder_1(z_latent_2_equivalent).detach()\n",
    "        pearson_1 = pearson_r(x_hat_1_equivalent.detach().flatten(), x_1_equivalent.detach().flatten())\n",
    "        print2log('Pearson correlation 2 to 1: %s'%pearson_1.item())\n",
    "        valPear.append([pearson_2.item(),pearson_1.item()])\n",
    "            \n",
    "        torch.save(decoder_1,'../results/LatentDimAnalysis/'+str(latent_dim)+'/models/decoder_1_%s.pt'%i)\n",
    "        torch.save(decoder_2,'../results/LatentDimAnalysis/'+str(latent_dim)+'/models/decoder_2_%s.pt'%i)\n",
    "        torch.save(prior_d,'../results/LatentDimAnalysis/'+str(latent_dim)+'/models/priorDiscr_%s.pt'%i)\n",
    "        torch.save(local_d,'../results/LatentDimAnalysis/'+str(latent_dim)+'/models/localDiscr_%s.pt'%i)\n",
    "        torch.save(encoder_1,'../results/LatentDimAnalysis/'+str(latent_dim)+'/models/encoder_1_%s.pt'%i)\n",
    "        torch.save(encoder_2,'../results/LatentDimAnalysis/'+str(latent_dim)+'/models/encoder_2_%s.pt'%i)\n",
    "        torch.save(classifier,'../results/LatentDimAnalysis/'+str(latent_dim)+'/models/classifier_%s.pt'%i)\n",
    "    \n",
    "    valPear = np.array(valPear)\n",
    "    df_result = pd.DataFrame({'model_pearson1to2':valPear[:,0],'model_pearson2to1':valPear[:,1],\n",
    "                              'recon_pear_2':valPear_2 ,'recon_pear_1':valPear_1,\n",
    "                              'ClassF1':valF1,'ClassAcc':valClassAcc})\n",
    "    df_result['latent_dim'] = latent_dim\n",
    "    df_result_all = df_result_all.append(df_result)\n",
    "    df_result_all.to_csv('../results/LatentDimAnalysis/validation_results.csv')\n",
    "    print2log('Finished model with latent dimension = %s'%latent_dim)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
