{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "913b1b9d",
   "metadata": {},
   "source": [
    "# Load dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18c35c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import silhouette_score,confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import tree\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "import scipy\n",
    "from scipy.signal import savgol_filter\n",
    "from scipy.spatial.distance import pdist,squareform\n",
    "from scipy.stats import mannwhitneyu\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from matplotlib import pyplot as plt\n",
    "import umap\n",
    "from models import SimpleEncoder,Decoder,PriorDiscriminator,LocalDiscriminator,Classifier\n",
    "from evaluationUtils import r_square,get_cindex,pearson_r,pseudoAccuracy\n",
    "from pathlib import Path\n",
    "from IPython.display import clear_output\n",
    "import seaborn as sns\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a8500c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(message)s')\n",
    "logger = logging.getLogger()\n",
    "print2log = logger.info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a4d73ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5ec26083",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a train generators\n",
    "def getSamples(N, batchSize):\n",
    "    order = np.random.permutation(N)\n",
    "    outList = []\n",
    "    while len(order)>0:\n",
    "        outList.append(order[0:batchSize])\n",
    "        order = order[batchSize:]\n",
    "    return outList"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f97e64",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8d9ca653",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>16</th>\n",
       "      <th>23</th>\n",
       "      <th>25</th>\n",
       "      <th>30</th>\n",
       "      <th>39</th>\n",
       "      <th>47</th>\n",
       "      <th>102</th>\n",
       "      <th>128</th>\n",
       "      <th>142</th>\n",
       "      <th>154</th>\n",
       "      <th>...</th>\n",
       "      <th>94239</th>\n",
       "      <th>116832</th>\n",
       "      <th>124583</th>\n",
       "      <th>147179</th>\n",
       "      <th>148022</th>\n",
       "      <th>200081</th>\n",
       "      <th>200734</th>\n",
       "      <th>256364</th>\n",
       "      <th>375346</th>\n",
       "      <th>388650</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>CPD001_PC3_24H:BRD-K47407372-001-11-4:10</th>\n",
       "      <td>0.523987</td>\n",
       "      <td>0.205788</td>\n",
       "      <td>-0.658799</td>\n",
       "      <td>0.280822</td>\n",
       "      <td>1.804530</td>\n",
       "      <td>-1.580919</td>\n",
       "      <td>-1.236128</td>\n",
       "      <td>-0.183599</td>\n",
       "      <td>-1.939110</td>\n",
       "      <td>-0.938237</td>\n",
       "      <td>...</td>\n",
       "      <td>3.538500</td>\n",
       "      <td>-1.178640</td>\n",
       "      <td>-0.516988</td>\n",
       "      <td>-0.432544</td>\n",
       "      <td>-1.556926</td>\n",
       "      <td>-2.074699</td>\n",
       "      <td>-0.782234</td>\n",
       "      <td>-2.072898</td>\n",
       "      <td>-0.933860</td>\n",
       "      <td>2.501641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DOSBIO001_PC3_24H:BRD-K60391168:10.1841</th>\n",
       "      <td>2.633331</td>\n",
       "      <td>-0.885202</td>\n",
       "      <td>-1.031261</td>\n",
       "      <td>0.103718</td>\n",
       "      <td>1.138970</td>\n",
       "      <td>0.100005</td>\n",
       "      <td>-0.469939</td>\n",
       "      <td>-0.916813</td>\n",
       "      <td>-0.949264</td>\n",
       "      <td>-1.501841</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.037178</td>\n",
       "      <td>0.877230</td>\n",
       "      <td>-0.712881</td>\n",
       "      <td>-0.158028</td>\n",
       "      <td>1.189959</td>\n",
       "      <td>0.910724</td>\n",
       "      <td>-0.287494</td>\n",
       "      <td>-0.606833</td>\n",
       "      <td>0.531542</td>\n",
       "      <td>-0.205312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DOSBIO001_PC3_24H:BRD-K08317416:10.1576</th>\n",
       "      <td>1.521831</td>\n",
       "      <td>-0.888528</td>\n",
       "      <td>-1.346109</td>\n",
       "      <td>1.097922</td>\n",
       "      <td>-0.497609</td>\n",
       "      <td>0.406287</td>\n",
       "      <td>-0.376153</td>\n",
       "      <td>0.434083</td>\n",
       "      <td>-0.632720</td>\n",
       "      <td>3.684922</td>\n",
       "      <td>...</td>\n",
       "      <td>0.144411</td>\n",
       "      <td>1.843784</td>\n",
       "      <td>1.050648</td>\n",
       "      <td>-0.098999</td>\n",
       "      <td>-2.003519</td>\n",
       "      <td>0.637561</td>\n",
       "      <td>0.476710</td>\n",
       "      <td>-0.180074</td>\n",
       "      <td>-0.896202</td>\n",
       "      <td>-0.368347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CPD001_PC3_24H:BRD-A41020680-003-24-9:10</th>\n",
       "      <td>5.537284</td>\n",
       "      <td>-1.457163</td>\n",
       "      <td>-0.384932</td>\n",
       "      <td>-0.713108</td>\n",
       "      <td>-3.885885</td>\n",
       "      <td>-0.524857</td>\n",
       "      <td>-0.505401</td>\n",
       "      <td>-1.250013</td>\n",
       "      <td>-0.484814</td>\n",
       "      <td>-0.325739</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.499735</td>\n",
       "      <td>0.080087</td>\n",
       "      <td>0.895180</td>\n",
       "      <td>-0.337255</td>\n",
       "      <td>0.238709</td>\n",
       "      <td>1.505617</td>\n",
       "      <td>-0.971875</td>\n",
       "      <td>0.447626</td>\n",
       "      <td>-1.748496</td>\n",
       "      <td>-0.758977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CPD001_PC3_24H:BRD-K13078532-001-21-9:10</th>\n",
       "      <td>-2.036632</td>\n",
       "      <td>0.350618</td>\n",
       "      <td>6.560456</td>\n",
       "      <td>2.893327</td>\n",
       "      <td>-1.481848</td>\n",
       "      <td>1.216153</td>\n",
       "      <td>1.043610</td>\n",
       "      <td>-0.028861</td>\n",
       "      <td>2.053373</td>\n",
       "      <td>-1.992963</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.133354</td>\n",
       "      <td>-0.658455</td>\n",
       "      <td>-3.022378</td>\n",
       "      <td>0.901453</td>\n",
       "      <td>0.777467</td>\n",
       "      <td>-0.902310</td>\n",
       "      <td>-1.123269</td>\n",
       "      <td>1.637385</td>\n",
       "      <td>1.427197</td>\n",
       "      <td>0.725433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DOSVAL003_HA1E_24H:BRD-K99481965:20</th>\n",
       "      <td>1.228643</td>\n",
       "      <td>2.201454</td>\n",
       "      <td>1.268494</td>\n",
       "      <td>-2.925642</td>\n",
       "      <td>-5.702180</td>\n",
       "      <td>-1.048154</td>\n",
       "      <td>-0.511689</td>\n",
       "      <td>-1.073713</td>\n",
       "      <td>-0.987551</td>\n",
       "      <td>0.088006</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.521899</td>\n",
       "      <td>-1.388441</td>\n",
       "      <td>0.738075</td>\n",
       "      <td>-0.532254</td>\n",
       "      <td>-0.637348</td>\n",
       "      <td>-0.950856</td>\n",
       "      <td>1.951221</td>\n",
       "      <td>1.633684</td>\n",
       "      <td>0.036308</td>\n",
       "      <td>1.814479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DOSVAL001_HA1E_24H:BRD-K25712227:10.0073</th>\n",
       "      <td>1.240146</td>\n",
       "      <td>-0.854687</td>\n",
       "      <td>-0.066887</td>\n",
       "      <td>-0.824974</td>\n",
       "      <td>-3.184588</td>\n",
       "      <td>-1.167342</td>\n",
       "      <td>-1.893655</td>\n",
       "      <td>-0.301650</td>\n",
       "      <td>-2.429506</td>\n",
       "      <td>2.668036</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.902034</td>\n",
       "      <td>-0.527167</td>\n",
       "      <td>1.294609</td>\n",
       "      <td>-0.512041</td>\n",
       "      <td>1.322372</td>\n",
       "      <td>1.399906</td>\n",
       "      <td>1.182881</td>\n",
       "      <td>0.106317</td>\n",
       "      <td>-0.632841</td>\n",
       "      <td>-1.876167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DOSVAL001_PC3_24H:BRD-K60763357:10.036</th>\n",
       "      <td>5.411014</td>\n",
       "      <td>-0.472251</td>\n",
       "      <td>-0.512716</td>\n",
       "      <td>1.804495</td>\n",
       "      <td>0.990145</td>\n",
       "      <td>0.694528</td>\n",
       "      <td>0.157606</td>\n",
       "      <td>-1.507224</td>\n",
       "      <td>-6.045609</td>\n",
       "      <td>-3.202184</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.563177</td>\n",
       "      <td>-2.870153</td>\n",
       "      <td>1.764212</td>\n",
       "      <td>-0.991561</td>\n",
       "      <td>3.929906</td>\n",
       "      <td>1.254488</td>\n",
       "      <td>-0.011530</td>\n",
       "      <td>-1.532616</td>\n",
       "      <td>-0.706827</td>\n",
       "      <td>0.888209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DOSVAL002_PC3_24H:BRD-K58415436:20</th>\n",
       "      <td>4.105516</td>\n",
       "      <td>0.305956</td>\n",
       "      <td>2.209438</td>\n",
       "      <td>-0.029295</td>\n",
       "      <td>-1.663379</td>\n",
       "      <td>-0.174490</td>\n",
       "      <td>2.631207</td>\n",
       "      <td>-1.057758</td>\n",
       "      <td>-1.062944</td>\n",
       "      <td>1.510586</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.394032</td>\n",
       "      <td>-0.496681</td>\n",
       "      <td>1.682302</td>\n",
       "      <td>-0.722316</td>\n",
       "      <td>0.301925</td>\n",
       "      <td>0.757042</td>\n",
       "      <td>1.770458</td>\n",
       "      <td>-0.691024</td>\n",
       "      <td>-0.490237</td>\n",
       "      <td>0.569134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DOSVAL005_PC3_24H:BRD-K86354270:20</th>\n",
       "      <td>0.711225</td>\n",
       "      <td>-1.162790</td>\n",
       "      <td>-1.024860</td>\n",
       "      <td>-0.046859</td>\n",
       "      <td>1.238543</td>\n",
       "      <td>1.394547</td>\n",
       "      <td>-0.307746</td>\n",
       "      <td>-2.547696</td>\n",
       "      <td>-1.129214</td>\n",
       "      <td>1.428087</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.043247</td>\n",
       "      <td>0.861824</td>\n",
       "      <td>0.920193</td>\n",
       "      <td>-0.000206</td>\n",
       "      <td>1.322194</td>\n",
       "      <td>0.275835</td>\n",
       "      <td>-3.393023</td>\n",
       "      <td>-1.152656</td>\n",
       "      <td>-1.138677</td>\n",
       "      <td>0.496622</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2489 rows Ã— 978 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                16        23        25  \\\n",
       "CPD001_PC3_24H:BRD-K47407372-001-11-4:10  0.523987  0.205788 -0.658799   \n",
       "DOSBIO001_PC3_24H:BRD-K60391168:10.1841   2.633331 -0.885202 -1.031261   \n",
       "DOSBIO001_PC3_24H:BRD-K08317416:10.1576   1.521831 -0.888528 -1.346109   \n",
       "CPD001_PC3_24H:BRD-A41020680-003-24-9:10  5.537284 -1.457163 -0.384932   \n",
       "CPD001_PC3_24H:BRD-K13078532-001-21-9:10 -2.036632  0.350618  6.560456   \n",
       "...                                            ...       ...       ...   \n",
       "DOSVAL003_HA1E_24H:BRD-K99481965:20       1.228643  2.201454  1.268494   \n",
       "DOSVAL001_HA1E_24H:BRD-K25712227:10.0073  1.240146 -0.854687 -0.066887   \n",
       "DOSVAL001_PC3_24H:BRD-K60763357:10.036    5.411014 -0.472251 -0.512716   \n",
       "DOSVAL002_PC3_24H:BRD-K58415436:20        4.105516  0.305956  2.209438   \n",
       "DOSVAL005_PC3_24H:BRD-K86354270:20        0.711225 -1.162790 -1.024860   \n",
       "\n",
       "                                                30        39        47  \\\n",
       "CPD001_PC3_24H:BRD-K47407372-001-11-4:10  0.280822  1.804530 -1.580919   \n",
       "DOSBIO001_PC3_24H:BRD-K60391168:10.1841   0.103718  1.138970  0.100005   \n",
       "DOSBIO001_PC3_24H:BRD-K08317416:10.1576   1.097922 -0.497609  0.406287   \n",
       "CPD001_PC3_24H:BRD-A41020680-003-24-9:10 -0.713108 -3.885885 -0.524857   \n",
       "CPD001_PC3_24H:BRD-K13078532-001-21-9:10  2.893327 -1.481848  1.216153   \n",
       "...                                            ...       ...       ...   \n",
       "DOSVAL003_HA1E_24H:BRD-K99481965:20      -2.925642 -5.702180 -1.048154   \n",
       "DOSVAL001_HA1E_24H:BRD-K25712227:10.0073 -0.824974 -3.184588 -1.167342   \n",
       "DOSVAL001_PC3_24H:BRD-K60763357:10.036    1.804495  0.990145  0.694528   \n",
       "DOSVAL002_PC3_24H:BRD-K58415436:20       -0.029295 -1.663379 -0.174490   \n",
       "DOSVAL005_PC3_24H:BRD-K86354270:20       -0.046859  1.238543  1.394547   \n",
       "\n",
       "                                               102       128       142  \\\n",
       "CPD001_PC3_24H:BRD-K47407372-001-11-4:10 -1.236128 -0.183599 -1.939110   \n",
       "DOSBIO001_PC3_24H:BRD-K60391168:10.1841  -0.469939 -0.916813 -0.949264   \n",
       "DOSBIO001_PC3_24H:BRD-K08317416:10.1576  -0.376153  0.434083 -0.632720   \n",
       "CPD001_PC3_24H:BRD-A41020680-003-24-9:10 -0.505401 -1.250013 -0.484814   \n",
       "CPD001_PC3_24H:BRD-K13078532-001-21-9:10  1.043610 -0.028861  2.053373   \n",
       "...                                            ...       ...       ...   \n",
       "DOSVAL003_HA1E_24H:BRD-K99481965:20      -0.511689 -1.073713 -0.987551   \n",
       "DOSVAL001_HA1E_24H:BRD-K25712227:10.0073 -1.893655 -0.301650 -2.429506   \n",
       "DOSVAL001_PC3_24H:BRD-K60763357:10.036    0.157606 -1.507224 -6.045609   \n",
       "DOSVAL002_PC3_24H:BRD-K58415436:20        2.631207 -1.057758 -1.062944   \n",
       "DOSVAL005_PC3_24H:BRD-K86354270:20       -0.307746 -2.547696 -1.129214   \n",
       "\n",
       "                                               154  ...     94239    116832  \\\n",
       "CPD001_PC3_24H:BRD-K47407372-001-11-4:10 -0.938237  ...  3.538500 -1.178640   \n",
       "DOSBIO001_PC3_24H:BRD-K60391168:10.1841  -1.501841  ... -1.037178  0.877230   \n",
       "DOSBIO001_PC3_24H:BRD-K08317416:10.1576   3.684922  ...  0.144411  1.843784   \n",
       "CPD001_PC3_24H:BRD-A41020680-003-24-9:10 -0.325739  ... -1.499735  0.080087   \n",
       "CPD001_PC3_24H:BRD-K13078532-001-21-9:10 -1.992963  ... -0.133354 -0.658455   \n",
       "...                                            ...  ...       ...       ...   \n",
       "DOSVAL003_HA1E_24H:BRD-K99481965:20       0.088006  ... -1.521899 -1.388441   \n",
       "DOSVAL001_HA1E_24H:BRD-K25712227:10.0073  2.668036  ... -1.902034 -0.527167   \n",
       "DOSVAL001_PC3_24H:BRD-K60763357:10.036   -3.202184  ... -0.563177 -2.870153   \n",
       "DOSVAL002_PC3_24H:BRD-K58415436:20        1.510586  ... -0.394032 -0.496681   \n",
       "DOSVAL005_PC3_24H:BRD-K86354270:20        1.428087  ... -0.043247  0.861824   \n",
       "\n",
       "                                            124583    147179    148022  \\\n",
       "CPD001_PC3_24H:BRD-K47407372-001-11-4:10 -0.516988 -0.432544 -1.556926   \n",
       "DOSBIO001_PC3_24H:BRD-K60391168:10.1841  -0.712881 -0.158028  1.189959   \n",
       "DOSBIO001_PC3_24H:BRD-K08317416:10.1576   1.050648 -0.098999 -2.003519   \n",
       "CPD001_PC3_24H:BRD-A41020680-003-24-9:10  0.895180 -0.337255  0.238709   \n",
       "CPD001_PC3_24H:BRD-K13078532-001-21-9:10 -3.022378  0.901453  0.777467   \n",
       "...                                            ...       ...       ...   \n",
       "DOSVAL003_HA1E_24H:BRD-K99481965:20       0.738075 -0.532254 -0.637348   \n",
       "DOSVAL001_HA1E_24H:BRD-K25712227:10.0073  1.294609 -0.512041  1.322372   \n",
       "DOSVAL001_PC3_24H:BRD-K60763357:10.036    1.764212 -0.991561  3.929906   \n",
       "DOSVAL002_PC3_24H:BRD-K58415436:20        1.682302 -0.722316  0.301925   \n",
       "DOSVAL005_PC3_24H:BRD-K86354270:20        0.920193 -0.000206  1.322194   \n",
       "\n",
       "                                            200081    200734    256364  \\\n",
       "CPD001_PC3_24H:BRD-K47407372-001-11-4:10 -2.074699 -0.782234 -2.072898   \n",
       "DOSBIO001_PC3_24H:BRD-K60391168:10.1841   0.910724 -0.287494 -0.606833   \n",
       "DOSBIO001_PC3_24H:BRD-K08317416:10.1576   0.637561  0.476710 -0.180074   \n",
       "CPD001_PC3_24H:BRD-A41020680-003-24-9:10  1.505617 -0.971875  0.447626   \n",
       "CPD001_PC3_24H:BRD-K13078532-001-21-9:10 -0.902310 -1.123269  1.637385   \n",
       "...                                            ...       ...       ...   \n",
       "DOSVAL003_HA1E_24H:BRD-K99481965:20      -0.950856  1.951221  1.633684   \n",
       "DOSVAL001_HA1E_24H:BRD-K25712227:10.0073  1.399906  1.182881  0.106317   \n",
       "DOSVAL001_PC3_24H:BRD-K60763357:10.036    1.254488 -0.011530 -1.532616   \n",
       "DOSVAL002_PC3_24H:BRD-K58415436:20        0.757042  1.770458 -0.691024   \n",
       "DOSVAL005_PC3_24H:BRD-K86354270:20        0.275835 -3.393023 -1.152656   \n",
       "\n",
       "                                            375346    388650  \n",
       "CPD001_PC3_24H:BRD-K47407372-001-11-4:10 -0.933860  2.501641  \n",
       "DOSBIO001_PC3_24H:BRD-K60391168:10.1841   0.531542 -0.205312  \n",
       "DOSBIO001_PC3_24H:BRD-K08317416:10.1576  -0.896202 -0.368347  \n",
       "CPD001_PC3_24H:BRD-A41020680-003-24-9:10 -1.748496 -0.758977  \n",
       "CPD001_PC3_24H:BRD-K13078532-001-21-9:10  1.427197  0.725433  \n",
       "...                                            ...       ...  \n",
       "DOSVAL003_HA1E_24H:BRD-K99481965:20       0.036308  1.814479  \n",
       "DOSVAL001_HA1E_24H:BRD-K25712227:10.0073 -0.632841 -1.876167  \n",
       "DOSVAL001_PC3_24H:BRD-K60763357:10.036   -0.706827  0.888209  \n",
       "DOSVAL002_PC3_24H:BRD-K58415436:20       -0.490237  0.569134  \n",
       "DOSVAL005_PC3_24H:BRD-K86354270:20       -1.138677  0.496622  \n",
       "\n",
       "[2489 rows x 978 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Gex data \n",
    "#cmap = pd.read_csv('../preprocessing/preprocessed_data/cmap_HA1E_PC3.csv',index_col=0)\n",
    "cmap = pd.read_csv('../preprocessing/preprocessed_data/cmap_landmarks_HA1E_PC3.csv',index_col=0)\n",
    "gene_size = len(cmap.columns)\n",
    "X = cmap.values\n",
    "display(cmap)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75d5ecf9",
   "metadata": {},
   "source": [
    "# Train one trasnlation model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "01186e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_params = {'encoder_1_hiddens':[640,384],\n",
    "                'encoder_2_hiddens':[640,384],\n",
    "                'latent_dim': 292,\n",
    "                'decoder_1_hiddens':[384,640],\n",
    "                'decoder_2_hiddens':[384,640],\n",
    "                'dropout_decoder':0.2,\n",
    "                'dropout_encoder':0.1,\n",
    "                'encoder_activation':torch.nn.ELU(),\n",
    "                'decoder_activation':torch.nn.ELU(),\n",
    "                'V_dropout':0.25,\n",
    "                'state_class_hidden':[256,128,64],\n",
    "                'state_class_drop_in':0.5,\n",
    "                'state_class_drop':0.25,\n",
    "                'no_states':2,\n",
    "                'adv_class_hidden':[256,128,64],\n",
    "                'adv_class_drop_in':0.3,\n",
    "                'adv_class_drop':0.1,\n",
    "                'no_adv_class':2,\n",
    "                'encoding_lr':0.001,\n",
    "                'adv_lr':0.001,\n",
    "                'schedule_step_adv':200,\n",
    "                'gamma_adv':0.5,\n",
    "                'schedule_step_enc':200,\n",
    "                'gamma_enc':0.8,\n",
    "                'batch_size_1':178,\n",
    "                'batch_size_2':154,\n",
    "                'batch_size_paired':90,\n",
    "                'epochs':1000,\n",
    "                'prior_beta':1.0,\n",
    "                'no_folds':10,\n",
    "                'v_reg':1e-04,\n",
    "                'state_class_reg':1e-02,\n",
    "                'enc_l2_reg':0.01,\n",
    "                'dec_l2_reg':0.01,\n",
    "                'lambda_mi_loss':100,\n",
    "                'effsize_reg': 100,\n",
    "                'cosine_loss': 10,\n",
    "                'adv_penalnty':100,\n",
    "                'reg_adv':1000,\n",
    "                'reg_classifier': 1000,\n",
    "                'similarity_reg' : 10,\n",
    "                'adversary_steps':4,\n",
    "                'autoencoder_wd': 0.,\n",
    "                'adversary_wd': 0.}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "92abbd27",
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dimensions = [256,192,128,64,32,16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9a9327fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_criterion = torch.nn.CrossEntropyLoss()\n",
    "bs_1 = model_params['batch_size_1']\n",
    "bs_2 = model_params['batch_size_2']\n",
    "bs_paired = model_params['batch_size_paired']\n",
    "NUM_EPOCHS=model_params['epochs']\n",
    "num_genes = cmap.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f303b296",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Start model with latent dimension = 16\n",
      "Split 1: Epoch=1/1000, r2_1=-2.3609, pearson_1=0.3668, MSE_1=2.1398, r2_2=-2.5686, pearson_2=0.3424, MSE_2=2.3895, MI Loss=3.3690, Prior Loss=1.3434, Entropy Loss=0.6549, F1=0.7634, Accuracy=0.6303, loss=4656.9922\n",
      "Split 1: Epoch=251/1000, r2_1=0.4400, pearson_1=0.7911, MSE_1=0.9870, r2_2=0.4120, pearson_2=0.7581, MSE_2=1.0911, MI Loss=-0.5058, Prior Loss=0.0028, Entropy Loss=0.6125, F1=0.8021, Accuracy=0.6891, loss=2146.3528\n",
      "Split 1: Epoch=501/1000, r2_1=0.4507, pearson_1=0.8192, MSE_1=0.9582, r2_2=0.5236, pearson_2=0.8265, MSE_2=0.9791, MI Loss=-0.5318, Prior Loss=0.0045, Entropy Loss=0.5888, F1=0.8229, Accuracy=0.7269, loss=2000.6498\n",
      "Split 1: Epoch=751/1000, r2_1=0.5301, pearson_1=0.8117, MSE_1=0.8470, r2_2=0.5051, pearson_2=0.8267, MSE_2=0.9493, MI Loss=-0.5355, Prior Loss=0.0009, Entropy Loss=0.5979, F1=0.8216, Accuracy=0.7227, loss=1860.4875\n",
      "Split 1: Epoch=1000/1000, r2_1=0.5524, pearson_1=0.8392, MSE_1=0.8823, r2_2=0.5372, pearson_2=0.8317, MSE_2=0.9388, MI Loss=-0.5421, Prior Loss=0.0002, Entropy Loss=0.5994, F1=0.8111, Accuracy=0.7143, loss=1881.5151\n",
      "Classification accuracy: 0.7936507936507936\n",
      "Classification F1 score: 0.8488372093023255\n",
      "Pearson correlation 1: 0.7600960731506348\n",
      "Pearson correlation 2: 0.7459774613380432\n",
      "Pearson correlation 1 to 2: 0.7459774613380432\n",
      "Pearson correlation 2 to 1: 0.48242607712745667\n",
      "Split 2: Epoch=1/1000, r2_1=-2.5310, pearson_1=0.4094, MSE_1=2.3258, r2_2=-2.9589, pearson_2=0.3276, MSE_2=2.3261, MI Loss=2.4121, Prior Loss=1.3957, Entropy Loss=0.6732, F1=0.7289, Accuracy=0.6092, loss=4791.8926\n",
      "Split 2: Epoch=251/1000, r2_1=0.4357, pearson_1=0.7771, MSE_1=0.9301, r2_2=0.4830, pearson_2=0.8226, MSE_2=1.0348, MI Loss=-0.5120, Prior Loss=0.0061, Entropy Loss=0.6277, F1=0.7874, Accuracy=0.6891, loss=2034.9841\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-ecd5bdbc99b7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    170\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    171\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss_1\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mloss_2\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mmi_loss\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mprior_loss\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msilimalityLoss\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mentropy\u001b[0m \u001b[1;33m+\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mL2Regularization\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1e-2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 172\u001b[1;33m                 \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    173\u001b[0m                 \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    174\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    394\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    395\u001b[0m                 inputs=inputs)\n\u001b[1;32m--> 396\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    397\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    398\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    171\u001b[0m     \u001b[1;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    172\u001b[0m     \u001b[1;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 173\u001b[1;33m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[0;32m    174\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    175\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "df_result_all = pd.DataFrame({})\n",
    "for latent_dim in latent_dimensions:\n",
    "    print2log('Start model with latent dimension = %s'%latent_dim)\n",
    "    latent_dim = int(latent_dim)\n",
    "    Path('../results/LatentDimAnalysis/'+str(latent_dim)+'/models').mkdir(parents=True, exist_ok=True)\n",
    "    valPear = []\n",
    "    valPear_1 = []\n",
    "    valPear_2 = []\n",
    "    valF1 = []\n",
    "    valClassAcc =[]\n",
    "    for i in range(model_params['no_folds']):\n",
    "        trainInfo_paired = pd.read_csv('../preprocessing/preprocessed_data/10fold_validation_spit/train_paired_pc3_ha1e_%s.csv'%i,index_col=0)\n",
    "        trainInfo_1 = pd.read_csv('../preprocessing/preprocessed_data/10fold_validation_spit/train_pc3_%s.csv'%i,index_col=0)\n",
    "        trainInfo_2 = pd.read_csv('../preprocessing/preprocessed_data/10fold_validation_spit/train_ha1e_%s.csv'%i,index_col=0)\n",
    "        valInfo_paired = pd.read_csv('../preprocessing/preprocessed_data/10fold_validation_spit/val_paired_pc3_ha1e_%s.csv'%i,index_col=0)\n",
    "        valInfo_1 = pd.read_csv('../preprocessing/preprocessed_data/10fold_validation_spit/val_pc3_%s.csv'%i,index_col=0)\n",
    "        valInfo_2 = pd.read_csv('../preprocessing/preprocessed_data/10fold_validation_spit/val_ha1e_%s.csv'%i,index_col=0)\n",
    "        \n",
    "        N_paired = len(trainInfo_paired)\n",
    "        N_1 = len(trainInfo_1)\n",
    "        N_2 = len(trainInfo_2)\n",
    "        N = N_1\n",
    "        if N_2>N:\n",
    "            N=N_2\n",
    "        # Network\n",
    "        decoder_1 = Decoder(latent_dim,model_params['decoder_1_hiddens'],num_genes,\n",
    "                                dropRate=model_params['dropout_decoder'], \n",
    "                                activation=model_params['decoder_activation']).to(device)\n",
    "        decoder_2 = Decoder(latent_dim,model_params['decoder_2_hiddens'],num_genes,\n",
    "                                dropRate=model_params['dropout_decoder'], \n",
    "                                activation=model_params['decoder_activation']).to(device)\n",
    "        encoder_1 = SimpleEncoder(num_genes,model_params['encoder_1_hiddens'],latent_dim,\n",
    "                                      dropRate=model_params['dropout_encoder'], \n",
    "                                      activation=model_params['encoder_activation'],\n",
    "                                     normalizeOutput=False).to(device)\n",
    "        encoder_2 = SimpleEncoder(num_genes,model_params['encoder_2_hiddens'],latent_dim,\n",
    "                                          dropRate=model_params['dropout_encoder'], \n",
    "                                          activation=model_params['encoder_activation'],\n",
    "                                     normalizeOutput=False).to(device)\n",
    "        classifier = Classifier(in_channel=latent_dim,\n",
    "                            hidden_layers=model_params['state_class_hidden'],\n",
    "                            num_classes=model_params['no_states'],\n",
    "                            drop_in=model_params['state_class_drop_in'],\n",
    "                            drop=model_params['state_class_drop']).to(device)\n",
    "        prior_d = PriorDiscriminator(latent_dim).to(device)\n",
    "        local_d = LocalDiscriminator(latent_dim,latent_dim).to(device)\n",
    "\n",
    "        allParams = list(decoder_1.parameters()) + list(encoder_1.parameters())\n",
    "        allParams = allParams + list(decoder_2.parameters()) + list(encoder_2.parameters())\n",
    "        allParams = allParams  + list(local_d.parameters())\n",
    "        allParams = allParams + list(prior_d.parameters())\n",
    "        optimizer = torch.optim.Adam(allParams, lr=model_params['encoding_lr'])\n",
    "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n",
    "                                                        step_size=model_params['schedule_step_enc'],\n",
    "                                                        gamma=model_params['gamma_enc'])\n",
    "        \n",
    "        trainLoss = []\n",
    "        trainLossSTD = []\n",
    "        for e in range(NUM_EPOCHS):\n",
    "            trainloader_1 = getSamples(N_1, bs_1)\n",
    "            len_1 = len(trainloader_1)\n",
    "            trainloader_2 = getSamples(N_2, bs_2)\n",
    "            len_2 = len(trainloader_2)\n",
    "            trainloader_paired = getSamples(N_paired, bs_paired)\n",
    "            len_paired = len(trainloader_paired)\n",
    "\n",
    "            lens = [len_1,len_2,len_paired]\n",
    "            maxLen = np.max(lens)\n",
    "\n",
    "            if maxLen>lens[0]:\n",
    "                trainloader_suppl = getSamples(N_1, bs_1)\n",
    "                for jj in range(maxLen-lens[0]):\n",
    "                    trainloader_1.insert(jj,trainloader_suppl[jj])\n",
    "\n",
    "            if maxLen>lens[1]:\n",
    "                trainloader_suppl = getSamples(N_2, bs_2)\n",
    "                for jj in range(maxLen-lens[1]):\n",
    "                    trainloader_2.insert(jj,trainloader_suppl[jj])\n",
    "\n",
    "            if maxLen>lens[2]:\n",
    "                trainloader_suppl = getSamples(N_paired, bs_paired)\n",
    "                for jj in range(maxLen-lens[2]):\n",
    "                    trainloader_paired.insert(jj,trainloader_suppl[jj])\n",
    "            encoder_1.train()\n",
    "            decoder_1.train()\n",
    "            encoder_2.train()\n",
    "            decoder_2.train()\n",
    "            prior_d.train()\n",
    "            local_d.train()\n",
    "            classifier.train()\n",
    "\n",
    "            trainLoss_ALL = []\n",
    "            \n",
    "            for j in range(maxLen):\n",
    "                dataIndex_1 = trainloader_1[j]\n",
    "                dataIndex_2 = trainloader_2[j]\n",
    "                dataIndex_paired = trainloader_paired[j]\n",
    "\n",
    "                df_pairs = trainInfo_paired.iloc[dataIndex_paired,:]\n",
    "                df_1 = trainInfo_1.iloc[dataIndex_1,:]\n",
    "                df_2 = trainInfo_2.iloc[dataIndex_2,:]\n",
    "                paired_inds = len(df_pairs)\n",
    "\n",
    "\n",
    "                X_1 = torch.tensor(np.concatenate((cmap.loc[df_pairs['sig_id.x']].values,\n",
    "                                                     cmap.loc[df_1.sig_id].values))).float().to(device)\n",
    "                X_2 = torch.tensor(np.concatenate((cmap.loc[df_pairs['sig_id.y']].values,\n",
    "                                                     cmap.loc[df_2.sig_id].values))).float().to(device)\n",
    "\n",
    "\n",
    "                conditions = np.concatenate((df_pairs.conditionId.values,\n",
    "                                             df_1.conditionId.values,\n",
    "                                             df_pairs.conditionId.values,\n",
    "                                             df_2.conditionId.values))\n",
    "                size = conditions.size\n",
    "                conditions = conditions.reshape(size,1)\n",
    "                conditions = conditions == conditions.transpose()\n",
    "                conditions = conditions*1\n",
    "                mask = torch.tensor(conditions).to(device).detach()\n",
    "                pos_mask = mask\n",
    "                neg_mask = 1 - mask\n",
    "                log_2 = math.log(2.)\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                z_1 = encoder_1(X_1)\n",
    "                z_2 = encoder_2(X_2)\n",
    "\n",
    "                z_un = local_d(torch.cat((z_1, z_2), 0))\n",
    "                res_un = torch.matmul(z_un, z_un.t())\n",
    "\n",
    "                y_pred_1 = decoder_1(z_1)\n",
    "                fitLoss_1 = torch.mean(torch.sum((y_pred_1 - X_1)**2,dim=1))\n",
    "                L2Loss_1 = decoder_1.L2Regularization(0.01) + encoder_1.L2Regularization(0.01)\n",
    "                loss_1 = fitLoss_1 + L2Loss_1\n",
    "\n",
    "                y_pred_2 = decoder_2(z_2)\n",
    "                fitLoss_2 = torch.mean(torch.sum((y_pred_2 - X_2)**2,dim=1))\n",
    "                L2Loss_2 = decoder_2.L2Regularization(0.01) + encoder_2.L2Regularization(0.01)\n",
    "                loss_2 = fitLoss_2 + L2Loss_2\n",
    "\n",
    "                silimalityLoss = torch.mean(torch.sum((z_1[0:paired_inds,:] - z_2[0:paired_inds,:])**2,dim=-1))\n",
    "\n",
    "                p_samples = res_un * pos_mask.float()\n",
    "                q_samples = res_un * neg_mask.float()\n",
    "\n",
    "                Ep = log_2 - F.softplus(- p_samples)\n",
    "                Eq = F.softplus(-q_samples) + q_samples - log_2\n",
    "\n",
    "                Ep = (Ep * pos_mask.float()).sum() / pos_mask.float().sum()\n",
    "                Eq = (Eq * neg_mask.float()).sum() / neg_mask.float().sum()\n",
    "                mi_loss = Eq - Ep\n",
    "\n",
    "                prior = torch.rand_like(torch.cat((z_1, z_2), 0))\n",
    "\n",
    "                term_a = torch.log(prior_d(prior)).mean()\n",
    "                term_b = torch.log(1.0 - prior_d(torch.cat((z_1, z_2), 0))).mean()\n",
    "                prior_loss = -(term_a + term_b) * model_params['prior_beta']\n",
    "\n",
    "                # Classification loss\n",
    "                labels = classifier(torch.cat((z_1, z_2), 0))\n",
    "                true_labels = torch.cat((torch.ones(z_1.shape[0]),\n",
    "                                         torch.zeros(z_2.shape[0])),0).long().to(device)\n",
    "                entropy = class_criterion(labels,true_labels)\n",
    "                _, predicted = torch.max(labels, 1)\n",
    "                predicted = predicted.cpu().numpy()\n",
    "                cf_matrix = confusion_matrix(true_labels.cpu().numpy(),predicted)\n",
    "                tn, fp, fn, tp = cf_matrix.ravel()\n",
    "                class_acc = (tp+tn)/predicted.size\n",
    "                f1 = 2*tp/(2*tp+fp+fn)\n",
    "\n",
    "                loss = loss_1 + loss_2 + mi_loss + prior_loss + silimalityLoss + 100*entropy +classifier.L2Regularization(1e-2)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                pearson_1 = pearson_r(y_pred_1.detach().flatten(), X_1.detach().flatten())\n",
    "                r2_1 = r_square(y_pred_1.detach().flatten(), X_1.detach().flatten())\n",
    "                mse_1 = torch.mean(torch.mean((y_pred_1.detach() - X_1.detach())**2,dim=1))\n",
    "\n",
    "                pearson_2 = pearson_r(y_pred_2.detach().flatten(), X_2.detach().flatten())\n",
    "                r2_2 = r_square(y_pred_2.detach().flatten(), X_2.detach().flatten())\n",
    "                mse_2 = torch.mean(torch.mean((y_pred_2.detach() - X_2.detach())**2,dim=1))\n",
    "\n",
    "\n",
    "            scheduler.step()\n",
    "            outString = 'Split {:.0f}: Epoch={:.0f}/{:.0f}'.format(i+1,e+1,NUM_EPOCHS)\n",
    "            outString += ', r2_1={:.4f}'.format(r2_1.item())\n",
    "            outString += ', pearson_1={:.4f}'.format(pearson_1.item())\n",
    "            outString += ', MSE_1={:.4f}'.format(mse_1.item())\n",
    "            outString += ', r2_2={:.4f}'.format(r2_2.item())\n",
    "            outString += ', pearson_2={:.4f}'.format(pearson_2.item())\n",
    "            outString += ', MSE_2={:.4f}'.format(mse_2.item())\n",
    "            outString += ', MI Loss={:.4f}'.format(mi_loss.item())\n",
    "            outString += ', Prior Loss={:.4f}'.format(prior_loss.item())\n",
    "            outString += ', Entropy Loss={:.4f}'.format(entropy.item())\n",
    "            outString += ', F1={:.4f}'.format(f1)\n",
    "            outString += ', Accuracy={:.4f}'.format(class_acc)\n",
    "            outString += ', loss={:.4f}'.format(loss.item())\n",
    "            if (e%250==0):\n",
    "                print2log(outString)\n",
    "        print2log(outString)\n",
    "        #trainLoss.append(splitLoss)\n",
    "        decoder_1.eval()\n",
    "        decoder_2.eval()\n",
    "        encoder_1.eval()\n",
    "        encoder_2.eval()\n",
    "        prior_d.eval()\n",
    "        local_d.eval()\n",
    "        classifier.eval()\n",
    "        #model.eval()\n",
    "        #master_encoder.eval()\n",
    "\n",
    "        paired_val_inds = len(valInfo_paired)\n",
    "        x_1 = torch.tensor(np.concatenate((cmap.loc[valInfo_paired['sig_id.x']].values,\n",
    "                                              cmap.loc[valInfo_1.sig_id].values))).float().to(device)\n",
    "        x_2 = torch.tensor(np.concatenate((cmap.loc[valInfo_paired['sig_id.y']].values,\n",
    "                                              cmap.loc[valInfo_2.sig_id].values))).float().to(device)\n",
    "\n",
    "        z_latent_1 = encoder_1(x_1)\n",
    "        z_latent_2 = encoder_2(x_2)\n",
    "\n",
    "        labels = classifier(torch.cat((z_latent_1, z_latent_2), 0))\n",
    "        true_labels = torch.cat((torch.ones(z_latent_1.shape[0]).view(z_latent_1.shape[0],1),\n",
    "                                 torch.zeros(z_latent_2.shape[0]).view(z_latent_2.shape[0],1)),0).long()\n",
    "        _, predicted = torch.max(labels, 1)\n",
    "        predicted = predicted.cpu().numpy()\n",
    "        cf_matrix = confusion_matrix(true_labels.numpy(),predicted)\n",
    "        tn, fp, fn, tp = cf_matrix.ravel()\n",
    "        class_acc = (tp+tn)/predicted.size\n",
    "        f1 = 2*tp/(2*tp+fp+fn)\n",
    "\n",
    "        valF1.append(f1)\n",
    "        valClassAcc.append(class_acc)\n",
    "\n",
    "        print2log('Classification accuracy: %s'%class_acc)\n",
    "        print2log('Classification F1 score: %s'%f1)\n",
    "\n",
    "        xhat_1 = decoder_1(z_latent_1)\n",
    "        xhat_2 = decoder_2(z_latent_2)\n",
    "\n",
    "\n",
    "        pearson_1 = pearson_r(xhat_1.detach().flatten(), x_1.detach().flatten())\n",
    "        pearson_2 = pearson_r(xhat_2.detach().flatten(), x_2.detach().flatten())\n",
    "        valPear_1.append(pearson_1.item())\n",
    "        valPear_2.append(pearson_2.item())\n",
    "        print2log('Pearson correlation 1: %s'%pearson_1.item())\n",
    "        print2log('Pearson correlation 2: %s'%pearson_2.item())\n",
    "\n",
    "        x_1_equivalent = x_1[0:paired_val_inds,:]\n",
    "        x_2_equivalent = x_2[0:paired_val_inds,:]\n",
    "\n",
    "        z_latent_1_equivalent  = encoder_1(x_1_equivalent)\n",
    "        x_hat_2_equivalent = decoder_2(z_latent_1_equivalent).detach()\n",
    "        print2log('Pearson correlation 1 to 2: %s'%pearson_2.item())\n",
    "        \n",
    "        z_latent_2_equivalent  = encoder_2(x_2_equivalent)\n",
    "        x_hat_1_equivalent = decoder_1(z_latent_2_equivalent).detach()\n",
    "        pearson_1 = pearson_r(x_hat_1_equivalent.detach().flatten(), x_1_equivalent.detach().flatten())\n",
    "        print2log('Pearson correlation 2 to 1: %s'%pearson_1.item())\n",
    "        valPear.append([pearson_2.item(),pearson_1.item()])\n",
    "            \n",
    "        torch.save(decoder_1,'../results/LatentDimAnalysis/'+str(latent_dim)+'/models/decoder_1_%s.pt'%i)\n",
    "        torch.save(decoder_2,'../results/LatentDimAnalysis/'+str(latent_dim)+'/models/decoder_2_%s.pt'%i)\n",
    "        torch.save(prior_d,'../results/LatentDimAnalysis/'+str(latent_dim)+'/models/priorDiscr_%s.pt'%i)\n",
    "        torch.save(local_d,'../results/LatentDimAnalysis/'+str(latent_dim)+'/models/localDiscr_%s.pt'%i)\n",
    "        torch.save(encoder_1,'../results/LatentDimAnalysis/'+str(latent_dim)+'/models/encoder_1_%s.pt'%i)\n",
    "        torch.save(encoder_2,'../results/LatentDimAnalysis/'+str(latent_dim)+'/models/encoder_2_%s.pt'%i)\n",
    "        torch.save(classifier,'../results/LatentDimAnalysis/'+str(latent_dim)+'/models/classifier_%s.pt'%i)\n",
    "    \n",
    "    valPear = np.array(valPear)\n",
    "    valPearDirect = np.array(valPearDirect)\n",
    "    crossCorrelation = np.array(crossCorrelation)\n",
    "    valSpear = np.array(valSpear)\n",
    "    valAccuracy= np.array(valAccuracy)\n",
    "    valSpearDirect= np.array(valSpearDirect)\n",
    "    valAccDirect= np.array(valAccDirect)\n",
    "    df_result = pd.DataFrame({'model_pearson1to2':valPear[:,0],'model_pearson2to1':valPear[:,1],\n",
    "                              'recon_pear_2':valPear_2 ,'recon_pear_1':valPear_1,\n",
    "                              'ClassF1':valF1,'ClassAcc':valClassAcc})\n",
    "    df_result['latent_dim'] = latent_dim\n",
    "    df_result_all = df_result_all.append(df_result)\n",
    "    df_result_all.to_csv('../results/LatentDimAnalysis/validation_results.csv')\n",
    "    print2log('Finished model with latent dimension = %s'%latent_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f337d356",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
