{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3dac22ff",
   "metadata": {},
   "source": [
    "# Load dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "473664fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import silhouette_score,confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import tree\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "import scipy\n",
    "from scipy.signal import savgol_filter\n",
    "from scipy.spatial.distance import pdist,squareform\n",
    "from scipy.stats import mannwhitneyu\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from matplotlib import pyplot as plt\n",
    "import umap\n",
    "\n",
    "from models import SimpleEncoder,Decoder,PriorDiscriminator,LocalDiscriminator\n",
    "from evaluationUtils import r_square,get_cindex,pearson_r,pseudoAccuracy\n",
    "\n",
    "from IPython.display import clear_output\n",
    "import seaborn as sns\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64039296",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e83c8f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a train generators\n",
    "def getSamples(N, batchSize):\n",
    "    order = np.random.permutation(N)\n",
    "    outList = []\n",
    "    while len(order)>0:\n",
    "        outList.append(order[0:batchSize])\n",
    "        order = order[batchSize:]\n",
    "    return outList"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e71dbd12",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c2886cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>16</th>\n",
       "      <th>23</th>\n",
       "      <th>25</th>\n",
       "      <th>30</th>\n",
       "      <th>39</th>\n",
       "      <th>47</th>\n",
       "      <th>102</th>\n",
       "      <th>128</th>\n",
       "      <th>142</th>\n",
       "      <th>154</th>\n",
       "      <th>...</th>\n",
       "      <th>94239</th>\n",
       "      <th>116832</th>\n",
       "      <th>124583</th>\n",
       "      <th>147179</th>\n",
       "      <th>148022</th>\n",
       "      <th>200081</th>\n",
       "      <th>200734</th>\n",
       "      <th>256364</th>\n",
       "      <th>375346</th>\n",
       "      <th>388650</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>OFL001_A549_96H:G15</th>\n",
       "      <td>1.854175</td>\n",
       "      <td>1.868439</td>\n",
       "      <td>-0.140405</td>\n",
       "      <td>-0.278911</td>\n",
       "      <td>0.396597</td>\n",
       "      <td>0.334116</td>\n",
       "      <td>0.473704</td>\n",
       "      <td>-0.565553</td>\n",
       "      <td>1.372410</td>\n",
       "      <td>1.181299</td>\n",
       "      <td>...</td>\n",
       "      <td>1.252141</td>\n",
       "      <td>-0.291923</td>\n",
       "      <td>1.193942</td>\n",
       "      <td>0.978987</td>\n",
       "      <td>2.381282</td>\n",
       "      <td>-1.065447</td>\n",
       "      <td>1.174847</td>\n",
       "      <td>-0.885704</td>\n",
       "      <td>0.879203</td>\n",
       "      <td>0.216700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OFL001_MCF7_96H:J10</th>\n",
       "      <td>0.081511</td>\n",
       "      <td>0.651525</td>\n",
       "      <td>-0.205014</td>\n",
       "      <td>0.054704</td>\n",
       "      <td>0.726742</td>\n",
       "      <td>-0.126017</td>\n",
       "      <td>0.200712</td>\n",
       "      <td>0.915557</td>\n",
       "      <td>0.780285</td>\n",
       "      <td>0.007211</td>\n",
       "      <td>...</td>\n",
       "      <td>0.341261</td>\n",
       "      <td>0.405606</td>\n",
       "      <td>-0.054713</td>\n",
       "      <td>0.264261</td>\n",
       "      <td>-0.096964</td>\n",
       "      <td>0.752965</td>\n",
       "      <td>-0.249324</td>\n",
       "      <td>-1.176310</td>\n",
       "      <td>0.282062</td>\n",
       "      <td>-0.212717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ABY001_NCIH1975_XH:CMAP-000:-666:3</th>\n",
       "      <td>0.543459</td>\n",
       "      <td>1.647965</td>\n",
       "      <td>-1.731661</td>\n",
       "      <td>0.319534</td>\n",
       "      <td>1.078192</td>\n",
       "      <td>0.602553</td>\n",
       "      <td>0.323291</td>\n",
       "      <td>0.787790</td>\n",
       "      <td>0.888264</td>\n",
       "      <td>1.532468</td>\n",
       "      <td>...</td>\n",
       "      <td>0.704732</td>\n",
       "      <td>-1.326966</td>\n",
       "      <td>1.433667</td>\n",
       "      <td>-0.037051</td>\n",
       "      <td>1.016276</td>\n",
       "      <td>-0.481035</td>\n",
       "      <td>1.061352</td>\n",
       "      <td>1.616178</td>\n",
       "      <td>1.540468</td>\n",
       "      <td>-0.958139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ZTO.XPR001_THP1_408H:CMAP-000:-666</th>\n",
       "      <td>-0.054865</td>\n",
       "      <td>-0.085794</td>\n",
       "      <td>-0.319447</td>\n",
       "      <td>0.180520</td>\n",
       "      <td>0.124284</td>\n",
       "      <td>-0.117936</td>\n",
       "      <td>-0.267994</td>\n",
       "      <td>0.429114</td>\n",
       "      <td>-0.144781</td>\n",
       "      <td>0.190815</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.114969</td>\n",
       "      <td>0.308555</td>\n",
       "      <td>0.055869</td>\n",
       "      <td>-0.450732</td>\n",
       "      <td>-0.394338</td>\n",
       "      <td>0.029793</td>\n",
       "      <td>0.046924</td>\n",
       "      <td>-0.231632</td>\n",
       "      <td>-0.186150</td>\n",
       "      <td>-0.309360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MOA001_A549_24H:N01</th>\n",
       "      <td>0.401776</td>\n",
       "      <td>1.197786</td>\n",
       "      <td>0.946556</td>\n",
       "      <td>0.794930</td>\n",
       "      <td>0.662958</td>\n",
       "      <td>0.473484</td>\n",
       "      <td>1.335021</td>\n",
       "      <td>0.338371</td>\n",
       "      <td>0.300303</td>\n",
       "      <td>0.690938</td>\n",
       "      <td>...</td>\n",
       "      <td>0.020668</td>\n",
       "      <td>0.171860</td>\n",
       "      <td>0.862337</td>\n",
       "      <td>0.525409</td>\n",
       "      <td>-0.029795</td>\n",
       "      <td>-0.263026</td>\n",
       "      <td>0.271724</td>\n",
       "      <td>0.934595</td>\n",
       "      <td>0.552001</td>\n",
       "      <td>-0.711617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DOSVAL001_HT29_24H:CMAP-000:-666</th>\n",
       "      <td>0.038320</td>\n",
       "      <td>-0.426547</td>\n",
       "      <td>0.183131</td>\n",
       "      <td>0.450992</td>\n",
       "      <td>-0.414180</td>\n",
       "      <td>-0.619587</td>\n",
       "      <td>-0.318295</td>\n",
       "      <td>0.066966</td>\n",
       "      <td>-0.618409</td>\n",
       "      <td>0.539847</td>\n",
       "      <td>...</td>\n",
       "      <td>0.085557</td>\n",
       "      <td>0.018541</td>\n",
       "      <td>0.174058</td>\n",
       "      <td>-0.101450</td>\n",
       "      <td>-0.279539</td>\n",
       "      <td>-0.303862</td>\n",
       "      <td>0.019368</td>\n",
       "      <td>1.111968</td>\n",
       "      <td>0.387193</td>\n",
       "      <td>-0.770082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DOSVAL001_HA1E_24H:CMAP-000:-666</th>\n",
       "      <td>0.319681</td>\n",
       "      <td>-0.182241</td>\n",
       "      <td>0.689418</td>\n",
       "      <td>0.542491</td>\n",
       "      <td>-0.124395</td>\n",
       "      <td>0.252069</td>\n",
       "      <td>-0.348502</td>\n",
       "      <td>0.145006</td>\n",
       "      <td>-0.018389</td>\n",
       "      <td>0.190280</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.048443</td>\n",
       "      <td>0.188158</td>\n",
       "      <td>0.422073</td>\n",
       "      <td>0.123565</td>\n",
       "      <td>0.097611</td>\n",
       "      <td>-0.003442</td>\n",
       "      <td>0.924158</td>\n",
       "      <td>-0.212382</td>\n",
       "      <td>0.166562</td>\n",
       "      <td>0.142994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DOSVAL001_A375_24H:CMAP-000:-666</th>\n",
       "      <td>0.091151</td>\n",
       "      <td>-0.007194</td>\n",
       "      <td>0.360459</td>\n",
       "      <td>0.430177</td>\n",
       "      <td>-0.443078</td>\n",
       "      <td>-0.370296</td>\n",
       "      <td>-0.450974</td>\n",
       "      <td>0.616529</td>\n",
       "      <td>0.258591</td>\n",
       "      <td>0.111886</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.571711</td>\n",
       "      <td>0.084373</td>\n",
       "      <td>0.240619</td>\n",
       "      <td>-0.372428</td>\n",
       "      <td>-0.168089</td>\n",
       "      <td>-0.137313</td>\n",
       "      <td>0.157594</td>\n",
       "      <td>0.256362</td>\n",
       "      <td>0.080780</td>\n",
       "      <td>-0.065995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DOSVAL001_HEPG2_24H:CMAP-000:-666</th>\n",
       "      <td>-0.276361</td>\n",
       "      <td>-0.321295</td>\n",
       "      <td>0.412983</td>\n",
       "      <td>0.040179</td>\n",
       "      <td>-0.144093</td>\n",
       "      <td>-0.374313</td>\n",
       "      <td>-0.488024</td>\n",
       "      <td>0.273988</td>\n",
       "      <td>-0.278131</td>\n",
       "      <td>-0.075510</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.440074</td>\n",
       "      <td>0.220422</td>\n",
       "      <td>-0.144075</td>\n",
       "      <td>-0.162023</td>\n",
       "      <td>-0.328652</td>\n",
       "      <td>-0.300582</td>\n",
       "      <td>0.469960</td>\n",
       "      <td>-0.533808</td>\n",
       "      <td>0.158130</td>\n",
       "      <td>-0.492051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DOSVAL001_MCF7_24H:CMAP-000:-666</th>\n",
       "      <td>0.562066</td>\n",
       "      <td>0.520038</td>\n",
       "      <td>0.751154</td>\n",
       "      <td>0.046425</td>\n",
       "      <td>0.820798</td>\n",
       "      <td>0.413035</td>\n",
       "      <td>0.023128</td>\n",
       "      <td>0.366703</td>\n",
       "      <td>-0.251062</td>\n",
       "      <td>0.249479</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.369176</td>\n",
       "      <td>-0.102268</td>\n",
       "      <td>0.466466</td>\n",
       "      <td>-0.158036</td>\n",
       "      <td>0.181730</td>\n",
       "      <td>-0.741085</td>\n",
       "      <td>0.759574</td>\n",
       "      <td>0.078421</td>\n",
       "      <td>-0.240611</td>\n",
       "      <td>0.000913</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3214 rows × 978 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          16        23        25        30  \\\n",
       "OFL001_A549_96H:G15                 1.854175  1.868439 -0.140405 -0.278911   \n",
       "OFL001_MCF7_96H:J10                 0.081511  0.651525 -0.205014  0.054704   \n",
       "ABY001_NCIH1975_XH:CMAP-000:-666:3  0.543459  1.647965 -1.731661  0.319534   \n",
       "ZTO.XPR001_THP1_408H:CMAP-000:-666 -0.054865 -0.085794 -0.319447  0.180520   \n",
       "MOA001_A549_24H:N01                 0.401776  1.197786  0.946556  0.794930   \n",
       "...                                      ...       ...       ...       ...   \n",
       "DOSVAL001_HT29_24H:CMAP-000:-666    0.038320 -0.426547  0.183131  0.450992   \n",
       "DOSVAL001_HA1E_24H:CMAP-000:-666    0.319681 -0.182241  0.689418  0.542491   \n",
       "DOSVAL001_A375_24H:CMAP-000:-666    0.091151 -0.007194  0.360459  0.430177   \n",
       "DOSVAL001_HEPG2_24H:CMAP-000:-666  -0.276361 -0.321295  0.412983  0.040179   \n",
       "DOSVAL001_MCF7_24H:CMAP-000:-666    0.562066  0.520038  0.751154  0.046425   \n",
       "\n",
       "                                          39        47       102       128  \\\n",
       "OFL001_A549_96H:G15                 0.396597  0.334116  0.473704 -0.565553   \n",
       "OFL001_MCF7_96H:J10                 0.726742 -0.126017  0.200712  0.915557   \n",
       "ABY001_NCIH1975_XH:CMAP-000:-666:3  1.078192  0.602553  0.323291  0.787790   \n",
       "ZTO.XPR001_THP1_408H:CMAP-000:-666  0.124284 -0.117936 -0.267994  0.429114   \n",
       "MOA001_A549_24H:N01                 0.662958  0.473484  1.335021  0.338371   \n",
       "...                                      ...       ...       ...       ...   \n",
       "DOSVAL001_HT29_24H:CMAP-000:-666   -0.414180 -0.619587 -0.318295  0.066966   \n",
       "DOSVAL001_HA1E_24H:CMAP-000:-666   -0.124395  0.252069 -0.348502  0.145006   \n",
       "DOSVAL001_A375_24H:CMAP-000:-666   -0.443078 -0.370296 -0.450974  0.616529   \n",
       "DOSVAL001_HEPG2_24H:CMAP-000:-666  -0.144093 -0.374313 -0.488024  0.273988   \n",
       "DOSVAL001_MCF7_24H:CMAP-000:-666    0.820798  0.413035  0.023128  0.366703   \n",
       "\n",
       "                                         142       154  ...     94239  \\\n",
       "OFL001_A549_96H:G15                 1.372410  1.181299  ...  1.252141   \n",
       "OFL001_MCF7_96H:J10                 0.780285  0.007211  ...  0.341261   \n",
       "ABY001_NCIH1975_XH:CMAP-000:-666:3  0.888264  1.532468  ...  0.704732   \n",
       "ZTO.XPR001_THP1_408H:CMAP-000:-666 -0.144781  0.190815  ... -0.114969   \n",
       "MOA001_A549_24H:N01                 0.300303  0.690938  ...  0.020668   \n",
       "...                                      ...       ...  ...       ...   \n",
       "DOSVAL001_HT29_24H:CMAP-000:-666   -0.618409  0.539847  ...  0.085557   \n",
       "DOSVAL001_HA1E_24H:CMAP-000:-666   -0.018389  0.190280  ... -0.048443   \n",
       "DOSVAL001_A375_24H:CMAP-000:-666    0.258591  0.111886  ... -0.571711   \n",
       "DOSVAL001_HEPG2_24H:CMAP-000:-666  -0.278131 -0.075510  ... -0.440074   \n",
       "DOSVAL001_MCF7_24H:CMAP-000:-666   -0.251062  0.249479  ... -0.369176   \n",
       "\n",
       "                                      116832    124583    147179    148022  \\\n",
       "OFL001_A549_96H:G15                -0.291923  1.193942  0.978987  2.381282   \n",
       "OFL001_MCF7_96H:J10                 0.405606 -0.054713  0.264261 -0.096964   \n",
       "ABY001_NCIH1975_XH:CMAP-000:-666:3 -1.326966  1.433667 -0.037051  1.016276   \n",
       "ZTO.XPR001_THP1_408H:CMAP-000:-666  0.308555  0.055869 -0.450732 -0.394338   \n",
       "MOA001_A549_24H:N01                 0.171860  0.862337  0.525409 -0.029795   \n",
       "...                                      ...       ...       ...       ...   \n",
       "DOSVAL001_HT29_24H:CMAP-000:-666    0.018541  0.174058 -0.101450 -0.279539   \n",
       "DOSVAL001_HA1E_24H:CMAP-000:-666    0.188158  0.422073  0.123565  0.097611   \n",
       "DOSVAL001_A375_24H:CMAP-000:-666    0.084373  0.240619 -0.372428 -0.168089   \n",
       "DOSVAL001_HEPG2_24H:CMAP-000:-666   0.220422 -0.144075 -0.162023 -0.328652   \n",
       "DOSVAL001_MCF7_24H:CMAP-000:-666   -0.102268  0.466466 -0.158036  0.181730   \n",
       "\n",
       "                                      200081    200734    256364    375346  \\\n",
       "OFL001_A549_96H:G15                -1.065447  1.174847 -0.885704  0.879203   \n",
       "OFL001_MCF7_96H:J10                 0.752965 -0.249324 -1.176310  0.282062   \n",
       "ABY001_NCIH1975_XH:CMAP-000:-666:3 -0.481035  1.061352  1.616178  1.540468   \n",
       "ZTO.XPR001_THP1_408H:CMAP-000:-666  0.029793  0.046924 -0.231632 -0.186150   \n",
       "MOA001_A549_24H:N01                -0.263026  0.271724  0.934595  0.552001   \n",
       "...                                      ...       ...       ...       ...   \n",
       "DOSVAL001_HT29_24H:CMAP-000:-666   -0.303862  0.019368  1.111968  0.387193   \n",
       "DOSVAL001_HA1E_24H:CMAP-000:-666   -0.003442  0.924158 -0.212382  0.166562   \n",
       "DOSVAL001_A375_24H:CMAP-000:-666   -0.137313  0.157594  0.256362  0.080780   \n",
       "DOSVAL001_HEPG2_24H:CMAP-000:-666  -0.300582  0.469960 -0.533808  0.158130   \n",
       "DOSVAL001_MCF7_24H:CMAP-000:-666   -0.741085  0.759574  0.078421 -0.240611   \n",
       "\n",
       "                                      388650  \n",
       "OFL001_A549_96H:G15                 0.216700  \n",
       "OFL001_MCF7_96H:J10                -0.212717  \n",
       "ABY001_NCIH1975_XH:CMAP-000:-666:3 -0.958139  \n",
       "ZTO.XPR001_THP1_408H:CMAP-000:-666 -0.309360  \n",
       "MOA001_A549_24H:N01                -0.711617  \n",
       "...                                      ...  \n",
       "DOSVAL001_HT29_24H:CMAP-000:-666   -0.770082  \n",
       "DOSVAL001_HA1E_24H:CMAP-000:-666    0.142994  \n",
       "DOSVAL001_A375_24H:CMAP-000:-666   -0.065995  \n",
       "DOSVAL001_HEPG2_24H:CMAP-000:-666  -0.492051  \n",
       "DOSVAL001_MCF7_24H:CMAP-000:-666    0.000913  \n",
       "\n",
       "[3214 rows x 978 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Gex data \n",
    "cmap = pd.read_csv('../preprocessing/preprocessed_data/baselineCell/cmap_all_baselines_q1.csv',index_col=0)\n",
    "gene_size = len(cmap.columns)\n",
    "X = cmap.values\n",
    "display(cmap)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce1240d",
   "metadata": {},
   "source": [
    "# Train autoencoder with all q1 controls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf77e663",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_params = {'encoder_hiddens':[640,384],\n",
    "                'latent_dim': 292,\n",
    "                'decoder_hiddens':[384,640],\n",
    "                'dropout_decoder':0.2,\n",
    "                'dropout_encoder':0.1,\n",
    "                'encoder_activation':torch.nn.ELU(),\n",
    "                'decoder_activation':torch.nn.ELU(),\n",
    "                'lr':0.001,\n",
    "                'schedule_step':200,\n",
    "                'gamma':0.5,\n",
    "                'batch_size':128,\n",
    "                'epochs':500,\n",
    "                'no_folds':10,\n",
    "                'enc_l2_reg':0.01,\n",
    "                'dec_l2_reg':0.01,\n",
    "                'autoencoder_wd': 0.}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "62434fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "bs=model_params['batch_size']\n",
    "k_folds=model_params['no_folds']\n",
    "NUM_EPOCHS=model_params['epochs']\n",
    "kfold=KFold(n_splits=k_folds,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9c317996",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = torch.utils.data.TensorDataset(torch.tensor(X).float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1b98e0e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold=0, Epoch=1/500, loss=628.4364, Pearson`s r=0.3593\n",
      "Fold=0, Epoch=101/500, loss=230.4944, Pearson`s r=0.7575\n",
      "Fold=0, Epoch=201/500, loss=217.2103, Pearson`s r=0.7772\n",
      "Fold=0, Epoch=301/500, loss=196.5769, Pearson`s r=0.8076\n",
      "Fold=0, Epoch=401/500, loss=196.0548, Pearson`s r=0.8070\n",
      "Fold=0, Epoch=500/500, loss=217.0013, Pearson`s r=0.8155\n",
      "Validation performance: Fold=0, Pearson`s r=0.8063\n",
      "Fold=1, Epoch=1/500, loss=575.0684, Pearson`s r=0.3447\n",
      "Fold=1, Epoch=101/500, loss=241.0442, Pearson`s r=0.7342\n",
      "Fold=1, Epoch=201/500, loss=219.8844, Pearson`s r=0.7930\n",
      "Fold=1, Epoch=301/500, loss=207.7966, Pearson`s r=0.7703\n",
      "Fold=1, Epoch=401/500, loss=199.3321, Pearson`s r=0.7780\n",
      "Fold=1, Epoch=500/500, loss=196.1797, Pearson`s r=0.7780\n",
      "Validation performance: Fold=1, Pearson`s r=0.8126\n",
      "Fold=2, Epoch=1/500, loss=486.4526, Pearson`s r=0.3202\n",
      "Fold=2, Epoch=101/500, loss=243.7110, Pearson`s r=0.7464\n",
      "Fold=2, Epoch=201/500, loss=222.2343, Pearson`s r=0.7854\n",
      "Fold=2, Epoch=301/500, loss=227.8018, Pearson`s r=0.7860\n",
      "Fold=2, Epoch=401/500, loss=189.2890, Pearson`s r=0.8122\n",
      "Fold=2, Epoch=500/500, loss=174.4308, Pearson`s r=0.7977\n",
      "Validation performance: Fold=2, Pearson`s r=0.8086\n",
      "Fold=3, Epoch=1/500, loss=613.6668, Pearson`s r=0.3523\n",
      "Fold=3, Epoch=101/500, loss=226.6063, Pearson`s r=0.7157\n",
      "Fold=3, Epoch=201/500, loss=237.2387, Pearson`s r=0.7834\n",
      "Fold=3, Epoch=301/500, loss=203.3533, Pearson`s r=0.7901\n",
      "Fold=3, Epoch=401/500, loss=194.3092, Pearson`s r=0.7719\n",
      "Fold=3, Epoch=500/500, loss=197.3653, Pearson`s r=0.7880\n",
      "Validation performance: Fold=3, Pearson`s r=0.8105\n",
      "Fold=4, Epoch=1/500, loss=543.4808, Pearson`s r=0.3386\n",
      "Fold=4, Epoch=101/500, loss=237.6587, Pearson`s r=0.7637\n",
      "Fold=4, Epoch=201/500, loss=203.3184, Pearson`s r=0.7656\n",
      "Fold=4, Epoch=301/500, loss=201.2360, Pearson`s r=0.7572\n",
      "Fold=4, Epoch=401/500, loss=183.8125, Pearson`s r=0.7923\n",
      "Fold=4, Epoch=500/500, loss=197.4427, Pearson`s r=0.8179\n",
      "Validation performance: Fold=4, Pearson`s r=0.8130\n",
      "Fold=5, Epoch=1/500, loss=613.3776, Pearson`s r=0.3500\n",
      "Fold=5, Epoch=101/500, loss=226.3463, Pearson`s r=0.7362\n",
      "Fold=5, Epoch=201/500, loss=227.2606, Pearson`s r=0.7823\n",
      "Fold=5, Epoch=301/500, loss=235.9522, Pearson`s r=0.8020\n",
      "Fold=5, Epoch=401/500, loss=194.0721, Pearson`s r=0.7684\n",
      "Fold=5, Epoch=500/500, loss=191.2036, Pearson`s r=0.7823\n",
      "Validation performance: Fold=5, Pearson`s r=0.8175\n",
      "Fold=6, Epoch=1/500, loss=521.7677, Pearson`s r=0.3189\n",
      "Fold=6, Epoch=101/500, loss=229.1718, Pearson`s r=0.7357\n",
      "Fold=6, Epoch=201/500, loss=217.5766, Pearson`s r=0.7550\n",
      "Fold=6, Epoch=301/500, loss=199.9509, Pearson`s r=0.7653\n",
      "Fold=6, Epoch=401/500, loss=229.4502, Pearson`s r=0.8234\n",
      "Fold=6, Epoch=500/500, loss=207.1217, Pearson`s r=0.8042\n",
      "Validation performance: Fold=6, Pearson`s r=0.8126\n",
      "Fold=7, Epoch=1/500, loss=545.7416, Pearson`s r=0.3360\n",
      "Fold=7, Epoch=101/500, loss=222.7119, Pearson`s r=0.7332\n",
      "Fold=7, Epoch=201/500, loss=207.4752, Pearson`s r=0.7701\n",
      "Fold=7, Epoch=301/500, loss=213.9847, Pearson`s r=0.7745\n",
      "Fold=7, Epoch=401/500, loss=188.6587, Pearson`s r=0.7659\n",
      "Fold=7, Epoch=500/500, loss=188.5724, Pearson`s r=0.7868\n",
      "Validation performance: Fold=7, Pearson`s r=0.8103\n",
      "Fold=8, Epoch=1/500, loss=552.8594, Pearson`s r=0.3172\n",
      "Fold=8, Epoch=101/500, loss=233.4043, Pearson`s r=0.7336\n",
      "Fold=8, Epoch=201/500, loss=212.0236, Pearson`s r=0.7596\n",
      "Fold=8, Epoch=301/500, loss=222.2335, Pearson`s r=0.8225\n",
      "Fold=8, Epoch=401/500, loss=206.8793, Pearson`s r=0.8109\n",
      "Fold=8, Epoch=500/500, loss=208.1276, Pearson`s r=0.8156\n",
      "Validation performance: Fold=8, Pearson`s r=0.8023\n",
      "Fold=9, Epoch=1/500, loss=485.3776, Pearson`s r=0.3394\n",
      "Fold=9, Epoch=101/500, loss=232.4685, Pearson`s r=0.7460\n",
      "Fold=9, Epoch=201/500, loss=227.3347, Pearson`s r=0.7874\n",
      "Fold=9, Epoch=301/500, loss=217.3155, Pearson`s r=0.7641\n",
      "Fold=9, Epoch=401/500, loss=179.3988, Pearson`s r=0.7941\n",
      "Fold=9, Epoch=500/500, loss=202.0591, Pearson`s r=0.8265\n",
      "Validation performance: Fold=9, Pearson`s r=0.8027\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "valPear = []\n",
    "trainPear = []\n",
    "for fold,(train_idx,val_idx) in enumerate(kfold.split(dataset)):\n",
    "    \n",
    "    X_train = torch.tensor(X[train_idx,:]).float().to(device)\n",
    "    X_val = torch.tensor(X[val_idx,:]).float().to(device)\n",
    "    N = X_train.shape[0]\n",
    "    \n",
    "    decoder = Decoder(model_params['latent_dim'],model_params['decoder_hiddens'],gene_size,\n",
    "                      dropRate=model_params['dropout_decoder'], \n",
    "                      activation=model_params['decoder_activation']).to(device)\n",
    "    encoder = SimpleEncoder(gene_size,model_params['encoder_hiddens'],model_params['latent_dim'],\n",
    "                            dropRate=model_params['dropout_encoder'], \n",
    "                            activation=model_params['encoder_activation']).to(device)\n",
    "    \n",
    "    allParams = list(decoder.parameters()) + list(encoder.parameters())\n",
    "    optimizer = torch.optim.Adam(allParams, lr=model_params['lr'])\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n",
    "                                                step_size=model_params['schedule_step'],\n",
    "                                                gamma=model_params['gamma'])\n",
    "    trainLoss = []\n",
    "    trainLossSTD = []\n",
    "    for e in range(NUM_EPOCHS):\n",
    "        encoder.train()\n",
    "        decoder.train()\n",
    "        \n",
    "        trainloader = getSamples(N, bs)\n",
    "        trainLoss_ALL = []\n",
    "        for dataIndex in trainloader:\n",
    "            \n",
    "            dataIn = X_train[dataIndex,:]\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            Xhat = decoder(encoder(dataIn))\n",
    "            L2Loss = encoder.L2Regularization(model_params['enc_l2_reg']) + decoder.L2Regularization(model_params['dec_l2_reg'])\n",
    "            loss = torch.mean(torch.sum((Xhat - dataIn)**2,dim=1)) + L2Loss\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            pear = pearson_r(Xhat.detach(), dataIn.detach())\n",
    "            trainLoss_ALL.append(loss.item())\n",
    "        if e%100==0:\n",
    "            outString = 'Fold={:.0f}'.format(fold)\n",
    "            outString += ', Epoch={:.0f}/{:.0f}'.format(e+1,NUM_EPOCHS)\n",
    "            outString += ', loss={:.4f}'.format(loss.item())\n",
    "            outString += ', Pearson`s r={:.4f}'.format(pear.item())\n",
    "            print(outString)\n",
    "        scheduler.step()\n",
    "        trainLoss.append(np.mean(trainLoss_ALL))\n",
    "        trainLossSTD.append(np.std(trainLoss_ALL))\n",
    "    outString = 'Fold={:.0f}'.format(fold)\n",
    "    outString += ', Epoch={:.0f}/{:.0f}'.format(e+1,NUM_EPOCHS)\n",
    "    outString += ', loss={:.4f}'.format(loss.item())\n",
    "    outString += ', Pearson`s r={:.4f}'.format(pear.item())\n",
    "    print(outString)\n",
    "    \n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "    \n",
    "    Xhat = decoder(encoder(X_val))\n",
    "    pear = pearson_r(Xhat.detach(), X_val.detach())\n",
    "    \n",
    "    outString = 'Validation performance: Fold={:.0f}'.format(fold)\n",
    "    outString += ', Pearson`s r={:.4f}'.format(pear.item())\n",
    "    print(outString)\n",
    "    valPear.append(pear.item())\n",
    "    \n",
    "    Xhat = decoder(encoder(X_train))\n",
    "    pear = pearson_r(Xhat.detach(), X_train.detach())\n",
    "    trainPear.append(pear.item())\n",
    "    \n",
    "    plt.plot(range(1,model_params['epochs']+1),np.array(trainLoss))\n",
    "    curColor = plt.gca().lines[-1].get_color()\n",
    "    plt.fill_between(range(1,model_params['epochs']+1), \n",
    "                    np.array(trainLoss) - np.array(trainLossSTD), \n",
    "                    np.array(trainLoss) + np.array(trainLossSTD),\n",
    "                    color=curColor, alpha=0.2)\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f1ac327d",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame({'r':trainPear,\n",
    "                        'set':'train'})\n",
    "results = results.append(pd.DataFrame({'r':valPear,\n",
    "                        'set':'validation'}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "98cc62ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "results\n",
    "results.to_csv('../results/BaselineCellsAnalysis/ae_results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0be0ebd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MannwhitneyuResult(statistic=100.0, pvalue=0.00018267179110955002)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "stats_results = mannwhitneyu(np.array(trainPear),np.array(valPear))\n",
    "print(stats_results)\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "sns.boxplot(data=results,x=\"set\",y = \"r\")\n",
    "plt.axhline(y=0.7, color='black', linestyle='--',linewidth=3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e309734d",
   "metadata": {},
   "source": [
    "# Train trasnlation model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e73867a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_params = {'encoder_1_hiddens':[384,256],\n",
    "                'encoder_2_hiddens':[384,256],\n",
    "                'latent_dim': 128,\n",
    "                'decoder_1_hiddens':[256,384],\n",
    "                'decoder_2_hiddens':[256,384],\n",
    "                'dropout_decoder':0.2,\n",
    "                'dropout_encoder':0.1,\n",
    "                'encoder_activation':torch.nn.ELU(),\n",
    "                'decoder_activation':torch.nn.ELU(),\n",
    "                'state_class_hidden':[256,128,64],\n",
    "                'state_class_drop_in':0.5,\n",
    "                'state_class_drop':0.25,\n",
    "                'no_states':2,\n",
    "                'encoding_lr':0.001,\n",
    "                'schedule_step_enc':200,\n",
    "                'gamma_enc':0.8,\n",
    "                'batch_size':512,\n",
    "                'epochs':1000,\n",
    "                'prior_beta':1.0,\n",
    "                'no_folds':2,\n",
    "                'state_class_reg':1e-02,\n",
    "                'enc_l2_reg':0.001,\n",
    "                'dec_l2_reg':0.001,\n",
    "                'lambda_mi_loss':100,\n",
    "                'cosine_loss': 10,\n",
    "                'reg_classifier': 10,\n",
    "                'similarity_reg' : 10.,\n",
    "                'autoencoder_wd': 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7122a72f",
   "metadata": {},
   "outputs": [],
   "source": [
    "bs =  model_params['batch_size']\n",
    "# k_folds=model_params['no_folds']\n",
    "NUM_EPOCHS=model_params['epochs']\n",
    "# kfold=KFold(n_splits=k_folds,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d423145e",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_genes = int(0.5*gene_size)\n",
    "random_iterations = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e075aa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cell-line : A549, rand_iter 1/2, Split 1: Epoch=1/1000, pearson_1=0.0081, MSE_1=1.2559, pearson_2=0.0012, MSE_2=1.2675, MI Loss=0.0682, Prior Loss=1.3885, Cosine =0.0171, silimalityLoss =1.3982, loss=1258.6855\n",
      "Cell-line : A549, rand_iter 1/2, Split 1: Epoch=101/1000, pearson_1=0.6092, MSE_1=0.3047, pearson_2=0.6092, MSE_2=0.3037, MI Loss=-1.3808, Prior Loss=0.0058, Cosine =0.2244, silimalityLoss =1.2281, loss=172.3745\n",
      "Cell-line : A549, rand_iter 1/2, Split 1: Epoch=201/1000, pearson_1=0.7214, MSE_1=0.2248, pearson_2=0.7121, MSE_2=0.2317, MI Loss=-1.3810, Prior Loss=0.0007, Cosine =0.3909, silimalityLoss =1.0808, loss=94.9587\n",
      "Cell-line : A549, rand_iter 1/2, Split 1: Epoch=301/1000, pearson_1=0.7587, MSE_1=0.1983, pearson_2=0.7615, MSE_2=0.1961, MI Loss=-1.3762, Prior Loss=0.0002, Cosine =0.5336, silimalityLoss =0.9357, loss=62.2209\n",
      "Cell-line : A549, rand_iter 1/2, Split 1: Epoch=401/1000, pearson_1=0.7936, MSE_1=0.1728, pearson_2=0.7958, MSE_2=0.1713, MI Loss=-1.3842, Prior Loss=0.0001, Cosine =0.6474, silimalityLoss =0.8041, loss=34.3769\n",
      "Cell-line : A549, rand_iter 1/2, Split 1: Epoch=501/1000, pearson_1=0.8079, MSE_1=0.1621, pearson_2=0.8105, MSE_2=0.1602, MI Loss=-1.3833, Prior Loss=0.0001, Cosine =0.7096, silimalityLoss =0.7276, loss=22.4111\n",
      "Cell-line : A549, rand_iter 1/2, Split 1: Epoch=601/1000, pearson_1=0.8214, MSE_1=0.1523, pearson_2=0.8205, MSE_2=0.1524, MI Loss=-1.3854, Prior Loss=0.0000, Cosine =0.7593, silimalityLoss =0.6623, loss=12.4730\n",
      "Cell-line : A549, rand_iter 1/2, Split 1: Epoch=701/1000, pearson_1=0.8272, MSE_1=0.1474, pearson_2=0.8239, MSE_2=0.1504, MI Loss=-1.3854, Prior Loss=0.0000, Cosine =0.7877, silimalityLoss =0.6197, loss=8.3838\n",
      "Cell-line : A549, rand_iter 1/2, Split 1: Epoch=801/1000, pearson_1=0.8342, MSE_1=0.1426, pearson_2=0.8325, MSE_2=0.1437, MI Loss=-1.3392, Prior Loss=0.0000, Cosine =0.8075, silimalityLoss =0.5895, loss=6.8930\n",
      "Cell-line : A549, rand_iter 1/2, Split 1: Epoch=901/1000, pearson_1=0.8373, MSE_1=0.1399, pearson_2=0.8385, MSE_2=0.1390, MI Loss=-1.3828, Prior Loss=0.0000, Cosine =0.8321, silimalityLoss =0.5548, loss=-1.6890\n",
      "Cell-line : A549, rand_iter 1/2, Split 1: Epoch=1000/1000, pearson_1=0.8393, MSE_1=0.1381, pearson_2=0.8369, MSE_2=0.1404, MI Loss=-1.3847, Prior Loss=0.0000, Cosine =0.8438, silimalityLoss =0.5329, loss=-2.4116\n",
      "Validation performance for cell A549 for try 0 for split 0\n",
      "Pearson correlation 1: 0.9199733734130859\n",
      "Pearson correlation 2: 0.9199752807617188\n",
      "Pearson correlation 1 to 2: 0.10080847144126892\n",
      "Pearson correlation 2 to 1: 0.11381104588508606\n",
      "Cell-line : A549, rand_iter 1/2, Split 2: Epoch=1/1000, pearson_1=-0.0032, MSE_1=1.2577, pearson_2=0.0018, MSE_2=1.2599, MI Loss=0.0650, Prior Loss=1.3462, Cosine =0.0158, silimalityLoss =1.3992, loss=1255.5457\n",
      "Cell-line : A549, rand_iter 1/2, Split 2: Epoch=101/1000, pearson_1=0.5895, MSE_1=0.3053, pearson_2=0.5956, MSE_2=0.3015, MI Loss=-1.3705, Prior Loss=0.0045, Cosine =0.1624, silimalityLoss =1.2761, loss=173.6949\n",
      "Cell-line : A549, rand_iter 1/2, Split 2: Epoch=201/1000, pearson_1=0.7099, MSE_1=0.2242, pearson_2=0.7067, MSE_2=0.2262, MI Loss=-1.3615, Prior Loss=0.0007, Cosine =0.2780, silimalityLoss =1.1799, loss=96.0326\n",
      "Cell-line : A549, rand_iter 1/2, Split 2: Epoch=301/1000, pearson_1=0.7526, MSE_1=0.1942, pearson_2=0.7525, MSE_2=0.1947, MI Loss=-1.3739, Prior Loss=0.0003, Cosine =0.4168, silimalityLoss =1.0535, loss=62.1217\n",
      "Cell-line : A549, rand_iter 1/2, Split 2: Epoch=401/1000, pearson_1=0.7790, MSE_1=0.1760, pearson_2=0.7815, MSE_2=0.1748, MI Loss=-1.3616, Prior Loss=0.0001, Cosine =0.5421, silimalityLoss =0.9280, loss=42.2166\n",
      "Cell-line : A549, rand_iter 1/2, Split 2: Epoch=501/1000, pearson_1=0.7944, MSE_1=0.1657, pearson_2=0.7998, MSE_2=0.1613, MI Loss=-1.3749, Prior Loss=0.0001, Cosine =0.6179, silimalityLoss =0.8381, loss=27.6188\n",
      "Cell-line : A549, rand_iter 1/2, Split 2: Epoch=601/1000, pearson_1=0.8037, MSE_1=0.1590, pearson_2=0.8104, MSE_2=0.1540, MI Loss=-1.3687, Prior Loss=0.0000, Cosine =0.6783, silimalityLoss =0.7684, loss=20.1303\n",
      "Cell-line : A549, rand_iter 1/2, Split 2: Epoch=701/1000, pearson_1=0.8193, MSE_1=0.1476, pearson_2=0.8177, MSE_2=0.1484, MI Loss=-1.3731, Prior Loss=0.0000, Cosine =0.7283, silimalityLoss =0.7058, loss=10.2611\n",
      "Cell-line : A549, rand_iter 1/2, Split 2: Epoch=801/1000, pearson_1=0.8209, MSE_1=0.1464, pearson_2=0.8239, MSE_2=0.1442, MI Loss=-1.3753, Prior Loss=0.0000, Cosine =0.7521, silimalityLoss =0.6702, loss=6.7800\n",
      "Cell-line : A549, rand_iter 1/2, Split 2: Epoch=901/1000, pearson_1=0.8299, MSE_1=0.1400, pearson_2=0.8302, MSE_2=0.1396, MI Loss=-1.3808, Prior Loss=0.0000, Cosine =0.7862, silimalityLoss =0.6268, loss=0.0977\n",
      "Cell-line : A549, rand_iter 1/2, Split 2: Epoch=1000/1000, pearson_1=0.8311, MSE_1=0.1387, pearson_2=0.8339, MSE_2=0.1371, MI Loss=-1.3664, Prior Loss=0.0000, Cosine =0.7996, silimalityLoss =0.6058, loss=-0.6711\n",
      "Validation performance for cell A549 for try 0 for split 1\n",
      "Pearson correlation 1: 0.9113413691520691\n",
      "Pearson correlation 2: 0.9106324911117554\n",
      "Pearson correlation 1 to 2: 0.07235973328351974\n",
      "Pearson correlation 2 to 1: 0.08956247568130493\n",
      "Cell-line : A549, rand_iter 2/2, Split 1: Epoch=1/1000, pearson_1=0.0042, MSE_1=1.2890, pearson_2=0.0030, MSE_2=1.2787, MI Loss=0.0890, Prior Loss=1.3673, Cosine =0.0162, silimalityLoss =1.3989, loss=1282.4882\n",
      "Cell-line : A549, rand_iter 2/2, Split 1: Epoch=101/1000, pearson_1=0.5997, MSE_1=0.3148, pearson_2=0.6089, MSE_2=0.3089, MI Loss=-1.3710, Prior Loss=0.0058, Cosine =0.2041, silimalityLoss =1.2434, loss=181.1889\n",
      "Cell-line : A549, rand_iter 2/2, Split 1: Epoch=201/1000, pearson_1=0.7076, MSE_1=0.2376, pearson_2=0.7209, MSE_2=0.2277, MI Loss=-1.3755, Prior Loss=0.0006, Cosine =0.3669, silimalityLoss =1.1002, loss=100.2586\n",
      "Cell-line : A549, rand_iter 2/2, Split 1: Epoch=301/1000, pearson_1=0.7590, MSE_1=0.2003, pearson_2=0.7599, MSE_2=0.1995, MI Loss=-1.3846, Prior Loss=0.0002, Cosine =0.5033, silimalityLoss =0.9661, loss=64.6468\n",
      "Cell-line : A549, rand_iter 2/2, Split 1: Epoch=401/1000, pearson_1=0.7894, MSE_1=0.1779, pearson_2=0.7891, MSE_2=0.1775, MI Loss=-1.3856, Prior Loss=0.0001, Cosine =0.6103, silimalityLoss =0.8476, loss=40.5957\n",
      "Cell-line : A549, rand_iter 2/2, Split 1: Epoch=501/1000, pearson_1=0.8051, MSE_1=0.1660, pearson_2=0.8048, MSE_2=0.1662, MI Loss=-1.3834, Prior Loss=0.0001, Cosine =0.6819, silimalityLoss =0.7604, loss=27.8922\n",
      "Cell-line : A549, rand_iter 2/2, Split 1: Epoch=601/1000, pearson_1=0.8159, MSE_1=0.1577, pearson_2=0.8140, MSE_2=0.1593, MI Loss=-1.3861, Prior Loss=0.0000, Cosine =0.7353, silimalityLoss =0.6949, loss=19.0139\n",
      "Cell-line : A549, rand_iter 2/2, Split 1: Epoch=701/1000, pearson_1=0.8257, MSE_1=0.1506, pearson_2=0.8276, MSE_2=0.1487, MI Loss=-1.3842, Prior Loss=0.0000, Cosine =0.7647, silimalityLoss =0.6539, loss=9.8466\n",
      "Cell-line : A549, rand_iter 2/2, Split 1: Epoch=801/1000, pearson_1=0.8267, MSE_1=0.1493, pearson_2=0.8289, MSE_2=0.1480, MI Loss=-1.3859, Prior Loss=0.0000, Cosine =0.7987, silimalityLoss =0.6085, loss=7.9057\n",
      "Cell-line : A549, rand_iter 2/2, Split 1: Epoch=901/1000, pearson_1=0.8315, MSE_1=0.1457, pearson_2=0.8359, MSE_2=0.1425, MI Loss=-1.3853, Prior Loss=0.0000, Cosine =0.8172, silimalityLoss =0.5798, loss=3.0125\n",
      "Cell-line : A549, rand_iter 2/2, Split 1: Epoch=1000/1000, pearson_1=0.8357, MSE_1=0.1428, pearson_2=0.8386, MSE_2=0.1404, MI Loss=-1.3859, Prior Loss=0.0000, Cosine =0.8354, silimalityLoss =0.5500, loss=0.0312\n",
      "Validation performance for cell A549 for try 1 for split 0\n",
      "Pearson correlation 1: 0.9189900755882263\n",
      "Pearson correlation 2: 0.920504629611969\n",
      "Pearson correlation 1 to 2: 0.19902831315994263\n",
      "Pearson correlation 2 to 1: 0.16701321303844452\n",
      "Cell-line : A549, rand_iter 2/2, Split 2: Epoch=1/1000, pearson_1=0.0046, MSE_1=1.2479, pearson_2=-0.0063, MSE_2=1.2662, MI Loss=0.0805, Prior Loss=1.4061, Cosine =0.0179, silimalityLoss =1.3978, loss=1255.4280\n",
      "Cell-line : A549, rand_iter 2/2, Split 2: Epoch=101/1000, pearson_1=0.5864, MSE_1=0.3099, pearson_2=0.5829, MSE_2=0.3122, MI Loss=-1.3216, Prior Loss=0.0045, Cosine =0.1978, silimalityLoss =1.2469, loss=185.4604\n",
      "Cell-line : A549, rand_iter 2/2, Split 2: Epoch=201/1000, pearson_1=0.7026, MSE_1=0.2314, pearson_2=0.7094, MSE_2=0.2260, MI Loss=-1.3315, Prior Loss=0.0004, Cosine =0.3661, silimalityLoss =1.1020, loss=100.7881\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cell-line : A549, rand_iter 2/2, Split 2: Epoch=301/1000, pearson_1=0.7514, MSE_1=0.1975, pearson_2=0.7498, MSE_2=0.1986, MI Loss=-1.2933, Prior Loss=0.0001, Cosine =0.4955, silimalityLoss =0.9680, loss=72.0968\n",
      "Cell-line : A549, rand_iter 2/2, Split 2: Epoch=401/1000, pearson_1=0.7809, MSE_1=0.1770, pearson_2=0.7825, MSE_2=0.1755, MI Loss=-1.3749, Prior Loss=0.0000, Cosine =0.6030, silimalityLoss =0.8571, loss=40.4177\n"
     ]
    }
   ],
   "source": [
    "for cell in [\"A549\",\"MCF7\",\"A375\",\"PC3\",\"HT29\",\"VCAP\",\"AGS\",\"ES2\",\"YAPC\",\"BICR6\",\"U251MG\"]:\n",
    "    for n in range(random_iterations):\n",
    "        genes_1 = np.random.choice(cmap.columns.values, size=num_genes, replace=False)\n",
    "        genes_2 = np.setdiff1d(cmap.columns.values,genes_1)\n",
    "        valPear = []\n",
    "        valPear_1 = []\n",
    "        valPear_2 = []\n",
    "        for i in range(model_params['no_folds']):\n",
    "            trainInfo = pd.read_csv('../preprocessing/preprocessed_data/baselineCell/'+cell+'/train_'+str(i)+'.csv',index_col=0)\n",
    "            valInfo = pd.read_csv('../preprocessing/preprocessed_data/baselineCell/'+cell+'/train_'+str(i)+'.csv',index_col=0)\n",
    "\n",
    "            cmap_train = cmap.loc[trainInfo.sig_id,:]\n",
    "            cmap_val = cmap.loc[valInfo.sig_id,:]\n",
    "            N = len(cmap_train)\n",
    "\n",
    "            # Network\n",
    "            decoder_1 = Decoder(model_params['latent_dim'],model_params['decoder_1_hiddens'],num_genes,\n",
    "                                dropRate=model_params['dropout_decoder'], \n",
    "                                activation=model_params['decoder_activation']).to(device)\n",
    "            decoder_2 = Decoder(model_params['latent_dim'],model_params['decoder_2_hiddens'],num_genes,\n",
    "                                dropRate=model_params['dropout_decoder'], \n",
    "                                activation=model_params['decoder_activation']).to(device)\n",
    "            encoder_1 = SimpleEncoder(num_genes,model_params['encoder_1_hiddens'],model_params['latent_dim'],\n",
    "                                      dropRate=model_params['dropout_encoder'], \n",
    "                                      activation=model_params['encoder_activation'],\n",
    "                                     normalizeOutput=True).to(device)\n",
    "            encoder_2 = SimpleEncoder(num_genes,model_params['encoder_2_hiddens'],model_params['latent_dim'],\n",
    "                                          dropRate=model_params['dropout_encoder'], \n",
    "                                          activation=model_params['encoder_activation'],\n",
    "                                     normalizeOutput=True).to(device)\n",
    "            prior_d = PriorDiscriminator(model_params['latent_dim']).to(device)\n",
    "            local_d = LocalDiscriminator(model_params['latent_dim'],model_params['latent_dim']).to(device)\n",
    "\n",
    "            allParams = list(decoder_1.parameters()) + list(encoder_1.parameters())\n",
    "            allParams = allParams + list(decoder_2.parameters()) + list(encoder_2.parameters())\n",
    "            allParams = allParams  + list(local_d.parameters())\n",
    "            allParams = allParams + list(prior_d.parameters())\n",
    "            optimizer = torch.optim.Adam(allParams, lr=model_params['encoding_lr'])\n",
    "            scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n",
    "                                                        step_size=model_params['schedule_step_enc'],\n",
    "                                                        gamma=model_params['gamma_enc'])\n",
    "            trainLoss = []\n",
    "            trainLossSTD = []\n",
    "            for e in range(NUM_EPOCHS):\n",
    "                encoder_1.train()\n",
    "                decoder_1.train()\n",
    "                encoder_2.train()\n",
    "                decoder_2.train()\n",
    "                prior_d.train()\n",
    "                local_d.train()\n",
    "\n",
    "                trainloader = getSamples(N, bs)\n",
    "                trainLoss_ALL = []\n",
    "                for dataIndex in trainloader:\n",
    "\n",
    "                    data_1 = torch.tensor(cmap_train.loc[:,genes_1].values).float()\n",
    "                    data_1 = data_1[dataIndex,:].to(device)\n",
    "                    data_2 = torch.tensor(cmap_train.loc[:,genes_1].values).float()\n",
    "                    data_2 = data_2[dataIndex,:].to(device)\n",
    "                    \n",
    "                    conditions = trainInfo.conditionId.values[dataIndex]\n",
    "                    conditions = np.concatenate((conditions,conditions))\n",
    "                    size = conditions.size\n",
    "                    conditions = conditions.reshape(size,1)\n",
    "                    conditions = conditions == conditions.transpose()\n",
    "                    conditions = conditions*1\n",
    "                    mask = torch.tensor(conditions).to(device).detach()\n",
    "                    pos_mask = mask\n",
    "                    neg_mask = 1 - mask\n",
    "                    log_2 = math.log(2.)\n",
    "                \n",
    "                    optimizer.zero_grad()\n",
    "                    \n",
    "                    z_1 = encoder_1(data_1)\n",
    "                    z_2 = encoder_2(data_2)\n",
    "                    \n",
    "                    #print('Epoch %s'%e)\n",
    "                    #print(z_1)\n",
    "                    \n",
    "                    latent_vectors = torch.cat((z_1, z_2), 0)\n",
    "                    z_un = local_d(latent_vectors)\n",
    "                    #z_un_1 = local_d(z_1)\n",
    "                    #z_un_2 = local_d(z_2)\n",
    "                    #res_un = torch.matmul(z_un_1, z_un_2.t())\n",
    "                    res_un = torch.matmul(z_un, z_un.t())\n",
    "                    \n",
    "                    #print(res_un)\n",
    "                    \n",
    "                    Xhat_1 = decoder_1(z_1)\n",
    "                    Xhat_2 = decoder_2(z_2)\n",
    "                    loss_1 = torch.mean(torch.sum((Xhat_1 - data_1)**2,dim=1)) + encoder_1.L2Regularization(model_params['enc_l2_reg']) + decoder_1.L2Regularization(model_params['dec_l2_reg'])\n",
    "                    loss_2 = torch.mean(torch.sum((Xhat_2 - data_2)**2,dim=1)) +encoder_2.L2Regularization(model_params['enc_l2_reg']) + decoder_2.L2Regularization(model_params['dec_l2_reg'])\n",
    "                    \n",
    "                    silimalityLoss = torch.sum(torch.cdist(latent_vectors, latent_vectors) * pos_mask.float()) / pos_mask.float().sum()\n",
    "                    #silimalityLoss = torch.mean(torch.cdist(z_1, z_2))\n",
    "                    w1 = latent_vectors.norm(p=2, dim=1, keepdim=True)\n",
    "                    w2 = latent_vectors.norm(p=2, dim=1, keepdim=True)\n",
    "                    cosineLoss = torch.mm(latent_vectors, latent_vectors.t()) / (w1 * w2.t()).clamp(min=1e-6)\n",
    "                    cosineLoss = torch.sum(cosineLoss * pos_mask.float()) / pos_mask.float().sum()\n",
    "                    #cosineLoss = torch.mean(cosineLoss)\n",
    "\n",
    "                    p_samples = res_un * pos_mask.float()\n",
    "                    q_samples = res_un * neg_mask.float()\n",
    "\n",
    "                    Ep = log_2 - F.softplus(- p_samples)\n",
    "                    Eq = F.softplus(-q_samples) + q_samples - log_2\n",
    "\n",
    "                    Ep = (Ep * pos_mask.float()).sum() / pos_mask.float().sum()\n",
    "                    Eq = (Eq * neg_mask.float()).sum() / neg_mask.float().sum()\n",
    "                    mi_loss = Eq - Ep\n",
    "                    #mi_loss = torch.nan_to_num(mi_loss, nan=1e-03)\n",
    "\n",
    "                    prior = torch.rand_like(torch.cat((z_1, z_2), 0))\n",
    "                    term_a = torch.log(prior_d(prior)).mean()\n",
    "                    term_b = torch.log(1.0 - prior_d(torch.cat((z_1, z_2), 0))).mean()\n",
    "                    prior_loss = -(term_a + term_b) * model_params['prior_beta']\n",
    "                    \n",
    "                    loss = loss_1 + loss_2 + model_params[\n",
    "                        'similarity_reg']*silimalityLoss - model_params[\n",
    "                        'cosine_loss'] * cosineLoss + prior_loss + model_params['lambda_mi_loss'] * mi_loss\n",
    "                    #loss = loss_1 + loss_2 + model_params[\n",
    "                    #    'similarity_reg'] * silimalityLoss + model_params[\n",
    "                    #    'lambda_mi_loss'] * mi_loss + prior_loss \n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    \n",
    "                    #w1 = latent_vectors.norm(p=2, dim=1, keepdim=True)\n",
    "                    #w2 = latent_vectors.norm(p=2, dim=1, keepdim=True)\n",
    "                    #cosineLoss = torch.mm(latent_vectors, latent_vectors.t()) / (w1 * w2.t()).clamp(min=1e-6)\n",
    "                    #cosineLoss = torch.sum(cosineLoss * pos_mask.float()) / pos_mask.float().sum()\n",
    "\n",
    "                    pear_1 = pearson_r(Xhat_1.detach(), data_1.detach())\n",
    "                    mse_1 = torch.mean(torch.mean((Xhat_1.detach() - data_1.detach()) ** 2, dim=1))\n",
    "                    pear_2 = pearson_r(Xhat_2.detach(), data_2.detach())\n",
    "                    mse_2 = torch.mean(torch.mean((Xhat_2.detach() - data_2.detach()) ** 2, dim=1))\n",
    "                    trainLoss_ALL.append(loss.item())\n",
    "                    \n",
    "                if e%100==0:\n",
    "                    outString = 'Cell-line : '+cell+', rand_iter {:.0f}/{:.0f}'.format(n + 1, random_iterations)\n",
    "                    outString += ', Split {:.0f}: Epoch={:.0f}/{:.0f}'.format(i + 1, e + 1, NUM_EPOCHS)\n",
    "                    outString += ', pearson_1={:.4f}'.format(pear_1.item())\n",
    "                    outString += ', MSE_1={:.4f}'.format(mse_1.item())\n",
    "                    outString += ', pearson_2={:.4f}'.format(pear_2.item())\n",
    "                    outString += ', MSE_2={:.4f}'.format(mse_2.item())\n",
    "                    outString += ', MI Loss={:.4f}'.format(mi_loss.item())\n",
    "                    outString += ', Prior Loss={:.4f}'.format(prior_loss.item())\n",
    "                    outString += ', Cosine ={:.4f}'.format(cosineLoss.item())\n",
    "                    outString += ', silimalityLoss ={:.4f}'.format(silimalityLoss.item())\n",
    "                    outString += ', loss={:.4f}'.format(loss.item())\n",
    "                    print(outString)\n",
    "                scheduler.step()\n",
    "                trainLoss.append(np.mean(trainLoss_ALL))\n",
    "                trainLossSTD.append(np.std(trainLoss_ALL))\n",
    "            outString = 'Cell-line : '+cell+', rand_iter {:.0f}/{:.0f}'.format(n + 1, random_iterations)\n",
    "            outString += ', Split {:.0f}: Epoch={:.0f}/{:.0f}'.format(i + 1, e + 1, NUM_EPOCHS)\n",
    "            outString += ', pearson_1={:.4f}'.format(pear_1.item())\n",
    "            outString += ', MSE_1={:.4f}'.format(mse_1.item())\n",
    "            outString += ', pearson_2={:.4f}'.format(pear_2.item())\n",
    "            outString += ', MSE_2={:.4f}'.format(mse_2.item())\n",
    "            outString += ', MI Loss={:.4f}'.format(mi_loss.item())\n",
    "            outString += ', Prior Loss={:.4f}'.format(prior_loss.item())\n",
    "            outString += ', Cosine ={:.4f}'.format(cosineLoss.item())\n",
    "            outString += ', silimalityLoss ={:.4f}'.format(silimalityLoss.item())\n",
    "            outString += ', loss={:.4f}'.format(loss.item())\n",
    "            print(outString)\n",
    "\n",
    "            encoder_1.eval()\n",
    "            decoder_1.eval()\n",
    "            encoder_2.eval()\n",
    "            decoder_2.eval()\n",
    "            prior_d.eval()\n",
    "            local_d.eval()\n",
    "            \n",
    "            print('Validation performance for cell %s for try %s for split %s'%(cell,n,i))\n",
    "\n",
    "\n",
    "            X_1 = torch.tensor(cmap_val.loc[:,genes_1].values).float().to(device)\n",
    "            X_2 = torch.tensor(cmap_val.loc[:,genes_1].values).float().to(device)\n",
    "                    \n",
    "            z_1 = encoder_1(X_1)\n",
    "            z_2 = encoder_2(X_2)\n",
    "            Xhat_1 = decoder_1(z_1)\n",
    "            Xhat_2 = decoder_2(z_2)\n",
    "            \n",
    "            pear_1 = pearson_r(Xhat_1.detach(), X_1.detach())\n",
    "            pear_2 = pearson_r(Xhat_2.detach(), X_2.detach())\n",
    "            valPear_1.append(pear_1.item())\n",
    "            valPear_2.append(pear_2.item())\n",
    "\n",
    "            print('Pearson correlation 1: %s'%pear_1.item())\n",
    "            print('Pearson correlation 2: %s'%pear_2.item())\n",
    "    \n",
    "    \n",
    "            x_hat_2_equivalent = decoder_2(z_1).detach()\n",
    "            pearson_2 = pearson_r(x_hat_2_equivalent.detach(), X_2.detach())\n",
    "            print('Pearson correlation 1 to 2: %s'%pearson_2.item())\n",
    "            x_hat_1_equivalent = decoder_1(z_2).detach()\n",
    "            pearson_1 = pearson_r(x_hat_1_equivalent.detach(), X_1.detach())\n",
    "            print('Pearson correlation 2 to 1: %s'%pearson_1.item())\n",
    "            \n",
    "            valPear.append([pearson_2.item(),pearson_1.item()])\n",
    "            \n",
    "            torch.save(decoder_1,'../results/BaselineCellsAnalysis/models_AutoTransOp/'+cell+'/decoder_1_fold%s_iter%s.pt'%(i,n))\n",
    "            torch.save(decoder_2,'../results/BaselineCellsAnalysis/models_AutoTransOp/'+cell+'/decoder_2_fold%s_iter%s.pt'%(i,n))\n",
    "            torch.save(prior_d,'../results/BaselineCellsAnalysis/models_AutoTransOp/'+cell+'/priorDiscr_fold%s_iter%s.pt'%(i,n))\n",
    "            torch.save(local_d,'../results/BaselineCellsAnalysis/models_AutoTransOp/'+cell+'/localDiscr_fold%s_iter%s.pt'%(i,n))\n",
    "            torch.save(encoder_1,'../results/BaselineCellsAnalysis/models_AutoTransOp/'+cell+'/encoder_1_fold%s_iter%s.pt'%(i,n))\n",
    "            torch.save(encoder_2,'../results/BaselineCellsAnalysis/models_AutoTransOp/'+cell+'/encoder_2_fold%s_iter%s.pt'%(i,n))            \n",
    "            \n",
    "        valPear = np.array(valPear)\n",
    "        df_result = pd.DataFrame({'model_pearson2to1':valPear[:,0],'model_pearson1to2':valPear[:,1],\n",
    "                          'recon_pear_2':valPear_2 ,'recon_pear_1':valPear_1})\n",
    "        df_result['cell'] = cell\n",
    "        df_result['iteration'] = n\n",
    "        df_result['fold'] = i\n",
    "        df_result.to_csv('../results/BaselineCellsAnalysis/translation_results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58479e11",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
