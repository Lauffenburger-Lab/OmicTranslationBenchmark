{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42ad5bd6",
   "metadata": {},
   "source": [
    "# Before you begin make sure you have the appropriate folder structure and correct paths for you when saving results duting training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "913b1b9d",
   "metadata": {},
   "source": [
    "# Load dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18c35c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import silhouette_score,confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import tree\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "import scipy\n",
    "from scipy.signal import savgol_filter\n",
    "from scipy.spatial.distance import pdist,squareform\n",
    "from scipy.stats import mannwhitneyu\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from matplotlib import pyplot as plt\n",
    "import umap\n",
    "\n",
    "from models import SimpleEncoder,Decoder,PriorDiscriminator,LocalDiscriminator\n",
    "from evaluationUtils import r_square,get_cindex,pearson_r,pseudoAccuracy\n",
    "\n",
    "from IPython.display import clear_output\n",
    "import seaborn as sns\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a4d73ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ec26083",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a train generators\n",
    "def getSamples(N, batchSize):\n",
    "    order = np.random.permutation(N)\n",
    "    outList = []\n",
    "    while len(order)>0:\n",
    "        outList.append(order[0:batchSize])\n",
    "        order = order[batchSize:]\n",
    "    return outList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "55e6136f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def uniformLoss(curState, dataIndex, z, targetMin = 0, targetMax = 1.0, maxConstraintFactor = 10):\n",
    "    data = curState.detach().clone()\n",
    "    data[dataIndex, :] = z\n",
    "\n",
    "    targetMean = (targetMax-targetMin)/2\n",
    "    targetVar= (targetMax-targetMin)**2/12\n",
    "\n",
    "    factor = 1\n",
    "    meanFactor = factor\n",
    "    varFactor = factor\n",
    "    minFactor = factor\n",
    "    maxFactor = factor\n",
    "    maxConstraintFactor = factor * maxConstraintFactor\n",
    "\n",
    "    nodeMean = torch.mean(data, dim=0)\n",
    "    nodeVar = torch.mean(torch.square(data-nodeMean), dim=0)\n",
    "    maxVal, _ = torch.max(data, dim=0)\n",
    "    minVal, _ = torch.min(data, dim=0)\n",
    "\n",
    "    meanLoss = meanFactor * torch.sum(torch.square(nodeMean - targetMean))\n",
    "    varLoss =  varFactor * torch.sum(torch.square(nodeVar - targetVar))\n",
    "    maxLoss = maxFactor * torch.sum(torch.square(maxVal - targetMax))\n",
    "    minloss = minFactor * torch.sum(torch.square(minVal- targetMin))\n",
    "    maxConstraint = -maxConstraintFactor * torch.sum(maxVal[maxVal.detach()<=0]) #max value should never be negative\n",
    "\n",
    "    loss = meanLoss + varLoss + minloss + maxLoss + maxConstraint\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5439b941",
   "metadata": {},
   "outputs": [],
   "source": [
    "def NormalLoss(curState, dataIndex, z, targetMin = -5.01, targetMax = 5.01, maxConstraintFactor = 10):\n",
    "    data = curState.detach().clone()\n",
    "    data[dataIndex, :] = z\n",
    "\n",
    "    targetMean = 0.\n",
    "    targetVar= 1.\n",
    "\n",
    "    factor = 1\n",
    "    meanFactor = factor\n",
    "    varFactor = factor\n",
    "    minFactor = factor\n",
    "    maxFactor = factor\n",
    "    maxConstraintFactor = factor * maxConstraintFactor\n",
    "\n",
    "    nodeMean = torch.mean(data, dim=0)\n",
    "    nodeVar = torch.mean(torch.square(data-nodeMean), dim=0)\n",
    "    maxVal, _ = torch.max(data, dim=0)\n",
    "    minVal, _ = torch.min(data, dim=0)\n",
    "\n",
    "    meanLoss = meanFactor * torch.sum(torch.square(nodeMean - targetMean))\n",
    "    varLoss =  varFactor * torch.sum(torch.square(nodeVar - targetVar))\n",
    "    maxLoss = maxFactor * torch.sum(torch.square(maxVal - targetMax))\n",
    "    minloss = minFactor * torch.sum(torch.square(minVal- targetMin))\n",
    "    maxConstraint = -maxConstraintFactor * torch.sum(maxVal[maxVal.detach()<=0]) #max value should never be negative\n",
    "\n",
    "    loss = meanLoss + varLoss + minloss + maxLoss + maxConstraint\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db8d9cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def kl_loss(s1,s2,m1,m2):\n",
    "#     l = torch.log(s2/s1 + 1e-6) + (s1**2 +(m1-m2)**2)/(2*s2**2 + 1e-6) - 0.5\n",
    "#     return l.mean()\n",
    "def kl_loss(m,s):\n",
    "    l = torch.square(m) + torch.square(s) - torch.log(s + 1e-04) - 0.5\n",
    "    return l.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f97e64",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8d9ca653",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>16</th>\n",
       "      <th>23</th>\n",
       "      <th>25</th>\n",
       "      <th>30</th>\n",
       "      <th>39</th>\n",
       "      <th>47</th>\n",
       "      <th>102</th>\n",
       "      <th>128</th>\n",
       "      <th>142</th>\n",
       "      <th>154</th>\n",
       "      <th>...</th>\n",
       "      <th>94239</th>\n",
       "      <th>116832</th>\n",
       "      <th>124583</th>\n",
       "      <th>147179</th>\n",
       "      <th>148022</th>\n",
       "      <th>200081</th>\n",
       "      <th>200734</th>\n",
       "      <th>256364</th>\n",
       "      <th>375346</th>\n",
       "      <th>388650</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>PCL001_HT29_24H:BRD-K42991516:10</th>\n",
       "      <td>0.266452</td>\n",
       "      <td>-0.250874</td>\n",
       "      <td>-0.854204</td>\n",
       "      <td>-0.041545</td>\n",
       "      <td>0.204450</td>\n",
       "      <td>0.709800</td>\n",
       "      <td>-0.328601</td>\n",
       "      <td>-0.498116</td>\n",
       "      <td>-1.454481</td>\n",
       "      <td>0.506321</td>\n",
       "      <td>...</td>\n",
       "      <td>0.536235</td>\n",
       "      <td>0.024452</td>\n",
       "      <td>0.928558</td>\n",
       "      <td>-0.453246</td>\n",
       "      <td>-0.140290</td>\n",
       "      <td>0.205065</td>\n",
       "      <td>1.148706</td>\n",
       "      <td>-1.933820</td>\n",
       "      <td>1.966937</td>\n",
       "      <td>-0.159919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PCL001_HT29_24H:BRD-K50817946:10</th>\n",
       "      <td>6.074023</td>\n",
       "      <td>-0.524075</td>\n",
       "      <td>-0.635742</td>\n",
       "      <td>2.014629</td>\n",
       "      <td>-3.747274</td>\n",
       "      <td>2.109600</td>\n",
       "      <td>0.847576</td>\n",
       "      <td>-2.732549</td>\n",
       "      <td>-5.729352</td>\n",
       "      <td>2.164091</td>\n",
       "      <td>...</td>\n",
       "      <td>0.447939</td>\n",
       "      <td>1.543649</td>\n",
       "      <td>-3.775020</td>\n",
       "      <td>1.827991</td>\n",
       "      <td>-0.088051</td>\n",
       "      <td>0.382848</td>\n",
       "      <td>1.400255</td>\n",
       "      <td>-3.087269</td>\n",
       "      <td>1.392148</td>\n",
       "      <td>1.027263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HOG002_A549_24H:BRD-K28296557-005-14-6:3.33</th>\n",
       "      <td>3.092555</td>\n",
       "      <td>1.760324</td>\n",
       "      <td>0.045857</td>\n",
       "      <td>-0.267738</td>\n",
       "      <td>-5.237659</td>\n",
       "      <td>-1.254134</td>\n",
       "      <td>-1.197927</td>\n",
       "      <td>-2.120804</td>\n",
       "      <td>-2.096229</td>\n",
       "      <td>0.799317</td>\n",
       "      <td>...</td>\n",
       "      <td>0.253642</td>\n",
       "      <td>-0.461737</td>\n",
       "      <td>-2.344703</td>\n",
       "      <td>1.581582</td>\n",
       "      <td>4.007076</td>\n",
       "      <td>-0.203330</td>\n",
       "      <td>0.715596</td>\n",
       "      <td>1.502107</td>\n",
       "      <td>1.281574</td>\n",
       "      <td>0.450898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DOSBIO001_MCF7_24H:BRD-K77888550:9.5278</th>\n",
       "      <td>-1.680236</td>\n",
       "      <td>1.174203</td>\n",
       "      <td>0.295703</td>\n",
       "      <td>0.555778</td>\n",
       "      <td>0.136969</td>\n",
       "      <td>-1.507160</td>\n",
       "      <td>-0.068983</td>\n",
       "      <td>-0.468983</td>\n",
       "      <td>-1.894113</td>\n",
       "      <td>-0.035792</td>\n",
       "      <td>...</td>\n",
       "      <td>1.204646</td>\n",
       "      <td>-0.688365</td>\n",
       "      <td>-1.042315</td>\n",
       "      <td>2.571737</td>\n",
       "      <td>-0.085614</td>\n",
       "      <td>-3.472259</td>\n",
       "      <td>1.436653</td>\n",
       "      <td>-1.054814</td>\n",
       "      <td>1.873788</td>\n",
       "      <td>1.680525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DOSBIO001_NPC_24H:BRD-K09069264:10.2084</th>\n",
       "      <td>-1.401400</td>\n",
       "      <td>0.308703</td>\n",
       "      <td>1.178614</td>\n",
       "      <td>-2.114849</td>\n",
       "      <td>-0.020324</td>\n",
       "      <td>-0.393869</td>\n",
       "      <td>-2.599080</td>\n",
       "      <td>-0.983008</td>\n",
       "      <td>-0.063675</td>\n",
       "      <td>-0.549799</td>\n",
       "      <td>...</td>\n",
       "      <td>0.349096</td>\n",
       "      <td>0.017305</td>\n",
       "      <td>0.356195</td>\n",
       "      <td>0.638253</td>\n",
       "      <td>0.862676</td>\n",
       "      <td>-0.106953</td>\n",
       "      <td>1.115011</td>\n",
       "      <td>2.205899</td>\n",
       "      <td>-0.306434</td>\n",
       "      <td>1.101611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DOSVAL001_MCF7_24H:BRD-K30867024:10</th>\n",
       "      <td>-0.228277</td>\n",
       "      <td>-0.574911</td>\n",
       "      <td>0.545074</td>\n",
       "      <td>1.320753</td>\n",
       "      <td>-0.102422</td>\n",
       "      <td>1.058099</td>\n",
       "      <td>0.366014</td>\n",
       "      <td>-1.520461</td>\n",
       "      <td>-0.004073</td>\n",
       "      <td>-0.637326</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.057771</td>\n",
       "      <td>-0.660938</td>\n",
       "      <td>-1.952505</td>\n",
       "      <td>0.395937</td>\n",
       "      <td>1.790094</td>\n",
       "      <td>1.072335</td>\n",
       "      <td>1.657834</td>\n",
       "      <td>1.445156</td>\n",
       "      <td>2.031223</td>\n",
       "      <td>0.026611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DOSVAL002_A375_24H:BRD-K10749593:20</th>\n",
       "      <td>4.415511</td>\n",
       "      <td>0.608378</td>\n",
       "      <td>1.604217</td>\n",
       "      <td>-0.911175</td>\n",
       "      <td>-2.611416</td>\n",
       "      <td>-1.742975</td>\n",
       "      <td>-2.500287</td>\n",
       "      <td>-2.503129</td>\n",
       "      <td>-3.472708</td>\n",
       "      <td>3.008501</td>\n",
       "      <td>...</td>\n",
       "      <td>0.916562</td>\n",
       "      <td>-1.403280</td>\n",
       "      <td>0.479218</td>\n",
       "      <td>4.528471</td>\n",
       "      <td>1.701896</td>\n",
       "      <td>0.141621</td>\n",
       "      <td>1.953133</td>\n",
       "      <td>-1.480089</td>\n",
       "      <td>1.549125</td>\n",
       "      <td>1.414482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DOSVAL002_MCF7_24H:BRD-K14002526:20</th>\n",
       "      <td>1.311693</td>\n",
       "      <td>1.834785</td>\n",
       "      <td>1.277888</td>\n",
       "      <td>-0.224320</td>\n",
       "      <td>-0.365258</td>\n",
       "      <td>0.209443</td>\n",
       "      <td>0.166746</td>\n",
       "      <td>-2.112468</td>\n",
       "      <td>-0.870127</td>\n",
       "      <td>-0.083894</td>\n",
       "      <td>...</td>\n",
       "      <td>0.894919</td>\n",
       "      <td>-0.707055</td>\n",
       "      <td>0.519019</td>\n",
       "      <td>0.916627</td>\n",
       "      <td>0.710227</td>\n",
       "      <td>0.126153</td>\n",
       "      <td>2.277303</td>\n",
       "      <td>-1.870382</td>\n",
       "      <td>1.021850</td>\n",
       "      <td>1.199542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DOSVAL001_HT29_24H:BRD-K11624501:9.99164</th>\n",
       "      <td>1.540175</td>\n",
       "      <td>-0.196926</td>\n",
       "      <td>-0.094410</td>\n",
       "      <td>-1.951286</td>\n",
       "      <td>-2.848082</td>\n",
       "      <td>-2.478519</td>\n",
       "      <td>-1.257487</td>\n",
       "      <td>-1.247405</td>\n",
       "      <td>-4.006328</td>\n",
       "      <td>-0.362494</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.371798</td>\n",
       "      <td>3.735393</td>\n",
       "      <td>2.011243</td>\n",
       "      <td>1.693114</td>\n",
       "      <td>2.924200</td>\n",
       "      <td>2.535851</td>\n",
       "      <td>1.861230</td>\n",
       "      <td>-3.021530</td>\n",
       "      <td>0.127304</td>\n",
       "      <td>0.980487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DOSVAL001_HCC515_24H:BRD-K48853221:10</th>\n",
       "      <td>3.427549</td>\n",
       "      <td>0.592761</td>\n",
       "      <td>-0.633423</td>\n",
       "      <td>-1.034512</td>\n",
       "      <td>1.337766</td>\n",
       "      <td>-0.138157</td>\n",
       "      <td>-1.700268</td>\n",
       "      <td>-0.425595</td>\n",
       "      <td>-1.978633</td>\n",
       "      <td>0.559278</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.677538</td>\n",
       "      <td>-0.900451</td>\n",
       "      <td>0.067572</td>\n",
       "      <td>-0.153004</td>\n",
       "      <td>5.329951</td>\n",
       "      <td>-0.593923</td>\n",
       "      <td>0.100499</td>\n",
       "      <td>0.381238</td>\n",
       "      <td>0.816859</td>\n",
       "      <td>0.400983</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13699 rows × 978 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   16        23        25  \\\n",
       "PCL001_HT29_24H:BRD-K42991516:10             0.266452 -0.250874 -0.854204   \n",
       "PCL001_HT29_24H:BRD-K50817946:10             6.074023 -0.524075 -0.635742   \n",
       "HOG002_A549_24H:BRD-K28296557-005-14-6:3.33  3.092555  1.760324  0.045857   \n",
       "DOSBIO001_MCF7_24H:BRD-K77888550:9.5278     -1.680236  1.174203  0.295703   \n",
       "DOSBIO001_NPC_24H:BRD-K09069264:10.2084     -1.401400  0.308703  1.178614   \n",
       "...                                               ...       ...       ...   \n",
       "DOSVAL001_MCF7_24H:BRD-K30867024:10         -0.228277 -0.574911  0.545074   \n",
       "DOSVAL002_A375_24H:BRD-K10749593:20          4.415511  0.608378  1.604217   \n",
       "DOSVAL002_MCF7_24H:BRD-K14002526:20          1.311693  1.834785  1.277888   \n",
       "DOSVAL001_HT29_24H:BRD-K11624501:9.99164     1.540175 -0.196926 -0.094410   \n",
       "DOSVAL001_HCC515_24H:BRD-K48853221:10        3.427549  0.592761 -0.633423   \n",
       "\n",
       "                                                   30        39        47  \\\n",
       "PCL001_HT29_24H:BRD-K42991516:10            -0.041545  0.204450  0.709800   \n",
       "PCL001_HT29_24H:BRD-K50817946:10             2.014629 -3.747274  2.109600   \n",
       "HOG002_A549_24H:BRD-K28296557-005-14-6:3.33 -0.267738 -5.237659 -1.254134   \n",
       "DOSBIO001_MCF7_24H:BRD-K77888550:9.5278      0.555778  0.136969 -1.507160   \n",
       "DOSBIO001_NPC_24H:BRD-K09069264:10.2084     -2.114849 -0.020324 -0.393869   \n",
       "...                                               ...       ...       ...   \n",
       "DOSVAL001_MCF7_24H:BRD-K30867024:10          1.320753 -0.102422  1.058099   \n",
       "DOSVAL002_A375_24H:BRD-K10749593:20         -0.911175 -2.611416 -1.742975   \n",
       "DOSVAL002_MCF7_24H:BRD-K14002526:20         -0.224320 -0.365258  0.209443   \n",
       "DOSVAL001_HT29_24H:BRD-K11624501:9.99164    -1.951286 -2.848082 -2.478519   \n",
       "DOSVAL001_HCC515_24H:BRD-K48853221:10       -1.034512  1.337766 -0.138157   \n",
       "\n",
       "                                                  102       128       142  \\\n",
       "PCL001_HT29_24H:BRD-K42991516:10            -0.328601 -0.498116 -1.454481   \n",
       "PCL001_HT29_24H:BRD-K50817946:10             0.847576 -2.732549 -5.729352   \n",
       "HOG002_A549_24H:BRD-K28296557-005-14-6:3.33 -1.197927 -2.120804 -2.096229   \n",
       "DOSBIO001_MCF7_24H:BRD-K77888550:9.5278     -0.068983 -0.468983 -1.894113   \n",
       "DOSBIO001_NPC_24H:BRD-K09069264:10.2084     -2.599080 -0.983008 -0.063675   \n",
       "...                                               ...       ...       ...   \n",
       "DOSVAL001_MCF7_24H:BRD-K30867024:10          0.366014 -1.520461 -0.004073   \n",
       "DOSVAL002_A375_24H:BRD-K10749593:20         -2.500287 -2.503129 -3.472708   \n",
       "DOSVAL002_MCF7_24H:BRD-K14002526:20          0.166746 -2.112468 -0.870127   \n",
       "DOSVAL001_HT29_24H:BRD-K11624501:9.99164    -1.257487 -1.247405 -4.006328   \n",
       "DOSVAL001_HCC515_24H:BRD-K48853221:10       -1.700268 -0.425595 -1.978633   \n",
       "\n",
       "                                                  154  ...     94239  \\\n",
       "PCL001_HT29_24H:BRD-K42991516:10             0.506321  ...  0.536235   \n",
       "PCL001_HT29_24H:BRD-K50817946:10             2.164091  ...  0.447939   \n",
       "HOG002_A549_24H:BRD-K28296557-005-14-6:3.33  0.799317  ...  0.253642   \n",
       "DOSBIO001_MCF7_24H:BRD-K77888550:9.5278     -0.035792  ...  1.204646   \n",
       "DOSBIO001_NPC_24H:BRD-K09069264:10.2084     -0.549799  ...  0.349096   \n",
       "...                                               ...  ...       ...   \n",
       "DOSVAL001_MCF7_24H:BRD-K30867024:10         -0.637326  ... -0.057771   \n",
       "DOSVAL002_A375_24H:BRD-K10749593:20          3.008501  ...  0.916562   \n",
       "DOSVAL002_MCF7_24H:BRD-K14002526:20         -0.083894  ...  0.894919   \n",
       "DOSVAL001_HT29_24H:BRD-K11624501:9.99164    -0.362494  ... -0.371798   \n",
       "DOSVAL001_HCC515_24H:BRD-K48853221:10        0.559278  ... -0.677538   \n",
       "\n",
       "                                               116832    124583    147179  \\\n",
       "PCL001_HT29_24H:BRD-K42991516:10             0.024452  0.928558 -0.453246   \n",
       "PCL001_HT29_24H:BRD-K50817946:10             1.543649 -3.775020  1.827991   \n",
       "HOG002_A549_24H:BRD-K28296557-005-14-6:3.33 -0.461737 -2.344703  1.581582   \n",
       "DOSBIO001_MCF7_24H:BRD-K77888550:9.5278     -0.688365 -1.042315  2.571737   \n",
       "DOSBIO001_NPC_24H:BRD-K09069264:10.2084      0.017305  0.356195  0.638253   \n",
       "...                                               ...       ...       ...   \n",
       "DOSVAL001_MCF7_24H:BRD-K30867024:10         -0.660938 -1.952505  0.395937   \n",
       "DOSVAL002_A375_24H:BRD-K10749593:20         -1.403280  0.479218  4.528471   \n",
       "DOSVAL002_MCF7_24H:BRD-K14002526:20         -0.707055  0.519019  0.916627   \n",
       "DOSVAL001_HT29_24H:BRD-K11624501:9.99164     3.735393  2.011243  1.693114   \n",
       "DOSVAL001_HCC515_24H:BRD-K48853221:10       -0.900451  0.067572 -0.153004   \n",
       "\n",
       "                                               148022    200081    200734  \\\n",
       "PCL001_HT29_24H:BRD-K42991516:10            -0.140290  0.205065  1.148706   \n",
       "PCL001_HT29_24H:BRD-K50817946:10            -0.088051  0.382848  1.400255   \n",
       "HOG002_A549_24H:BRD-K28296557-005-14-6:3.33  4.007076 -0.203330  0.715596   \n",
       "DOSBIO001_MCF7_24H:BRD-K77888550:9.5278     -0.085614 -3.472259  1.436653   \n",
       "DOSBIO001_NPC_24H:BRD-K09069264:10.2084      0.862676 -0.106953  1.115011   \n",
       "...                                               ...       ...       ...   \n",
       "DOSVAL001_MCF7_24H:BRD-K30867024:10          1.790094  1.072335  1.657834   \n",
       "DOSVAL002_A375_24H:BRD-K10749593:20          1.701896  0.141621  1.953133   \n",
       "DOSVAL002_MCF7_24H:BRD-K14002526:20          0.710227  0.126153  2.277303   \n",
       "DOSVAL001_HT29_24H:BRD-K11624501:9.99164     2.924200  2.535851  1.861230   \n",
       "DOSVAL001_HCC515_24H:BRD-K48853221:10        5.329951 -0.593923  0.100499   \n",
       "\n",
       "                                               256364    375346    388650  \n",
       "PCL001_HT29_24H:BRD-K42991516:10            -1.933820  1.966937 -0.159919  \n",
       "PCL001_HT29_24H:BRD-K50817946:10            -3.087269  1.392148  1.027263  \n",
       "HOG002_A549_24H:BRD-K28296557-005-14-6:3.33  1.502107  1.281574  0.450898  \n",
       "DOSBIO001_MCF7_24H:BRD-K77888550:9.5278     -1.054814  1.873788  1.680525  \n",
       "DOSBIO001_NPC_24H:BRD-K09069264:10.2084      2.205899 -0.306434  1.101611  \n",
       "...                                               ...       ...       ...  \n",
       "DOSVAL001_MCF7_24H:BRD-K30867024:10          1.445156  2.031223  0.026611  \n",
       "DOSVAL002_A375_24H:BRD-K10749593:20         -1.480089  1.549125  1.414482  \n",
       "DOSVAL002_MCF7_24H:BRD-K14002526:20         -1.870382  1.021850  1.199542  \n",
       "DOSVAL001_HT29_24H:BRD-K11624501:9.99164    -3.021530  0.127304  0.980487  \n",
       "DOSVAL001_HCC515_24H:BRD-K48853221:10        0.381238  0.816859  0.400983  \n",
       "\n",
       "[13699 rows x 978 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Gex data \n",
    "cmap = pd.read_csv('../preprocessing/preprocessed_data/all_cmap_landmarks.csv',index_col=0)\n",
    "gene_size = len(cmap.columns)\n",
    "X = cmap.values\n",
    "display(cmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "65cc95ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>16</th>\n",
       "      <th>23</th>\n",
       "      <th>25</th>\n",
       "      <th>30</th>\n",
       "      <th>39</th>\n",
       "      <th>47</th>\n",
       "      <th>102</th>\n",
       "      <th>128</th>\n",
       "      <th>142</th>\n",
       "      <th>154</th>\n",
       "      <th>...</th>\n",
       "      <th>94239</th>\n",
       "      <th>116832</th>\n",
       "      <th>124583</th>\n",
       "      <th>147179</th>\n",
       "      <th>148022</th>\n",
       "      <th>200081</th>\n",
       "      <th>200734</th>\n",
       "      <th>256364</th>\n",
       "      <th>375346</th>\n",
       "      <th>388650</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>OFL001_A549_96H:G15</th>\n",
       "      <td>1.854175</td>\n",
       "      <td>1.868439</td>\n",
       "      <td>-0.140405</td>\n",
       "      <td>-0.278911</td>\n",
       "      <td>0.396597</td>\n",
       "      <td>0.334116</td>\n",
       "      <td>0.473704</td>\n",
       "      <td>-0.565553</td>\n",
       "      <td>1.372410</td>\n",
       "      <td>1.181299</td>\n",
       "      <td>...</td>\n",
       "      <td>1.252141</td>\n",
       "      <td>-0.291923</td>\n",
       "      <td>1.193942</td>\n",
       "      <td>0.978987</td>\n",
       "      <td>2.381282</td>\n",
       "      <td>-1.065447</td>\n",
       "      <td>1.174847</td>\n",
       "      <td>-0.885704</td>\n",
       "      <td>0.879203</td>\n",
       "      <td>0.216700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OFL001_MCF7_96H:J10</th>\n",
       "      <td>0.081511</td>\n",
       "      <td>0.651525</td>\n",
       "      <td>-0.205014</td>\n",
       "      <td>0.054704</td>\n",
       "      <td>0.726742</td>\n",
       "      <td>-0.126017</td>\n",
       "      <td>0.200712</td>\n",
       "      <td>0.915557</td>\n",
       "      <td>0.780285</td>\n",
       "      <td>0.007211</td>\n",
       "      <td>...</td>\n",
       "      <td>0.341261</td>\n",
       "      <td>0.405606</td>\n",
       "      <td>-0.054713</td>\n",
       "      <td>0.264261</td>\n",
       "      <td>-0.096964</td>\n",
       "      <td>0.752965</td>\n",
       "      <td>-0.249324</td>\n",
       "      <td>-1.176310</td>\n",
       "      <td>0.282062</td>\n",
       "      <td>-0.212717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ABY001_NCIH1975_XH:CMAP-000:-666:3</th>\n",
       "      <td>0.543459</td>\n",
       "      <td>1.647965</td>\n",
       "      <td>-1.731661</td>\n",
       "      <td>0.319534</td>\n",
       "      <td>1.078192</td>\n",
       "      <td>0.602553</td>\n",
       "      <td>0.323291</td>\n",
       "      <td>0.787790</td>\n",
       "      <td>0.888264</td>\n",
       "      <td>1.532468</td>\n",
       "      <td>...</td>\n",
       "      <td>0.704732</td>\n",
       "      <td>-1.326966</td>\n",
       "      <td>1.433667</td>\n",
       "      <td>-0.037051</td>\n",
       "      <td>1.016276</td>\n",
       "      <td>-0.481035</td>\n",
       "      <td>1.061352</td>\n",
       "      <td>1.616178</td>\n",
       "      <td>1.540468</td>\n",
       "      <td>-0.958139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ZTO.XPR001_THP1_408H:CMAP-000:-666</th>\n",
       "      <td>-0.054865</td>\n",
       "      <td>-0.085794</td>\n",
       "      <td>-0.319447</td>\n",
       "      <td>0.180520</td>\n",
       "      <td>0.124284</td>\n",
       "      <td>-0.117936</td>\n",
       "      <td>-0.267994</td>\n",
       "      <td>0.429114</td>\n",
       "      <td>-0.144781</td>\n",
       "      <td>0.190815</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.114969</td>\n",
       "      <td>0.308555</td>\n",
       "      <td>0.055869</td>\n",
       "      <td>-0.450732</td>\n",
       "      <td>-0.394338</td>\n",
       "      <td>0.029793</td>\n",
       "      <td>0.046924</td>\n",
       "      <td>-0.231632</td>\n",
       "      <td>-0.186150</td>\n",
       "      <td>-0.309360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MOA001_A549_24H:N01</th>\n",
       "      <td>0.401776</td>\n",
       "      <td>1.197786</td>\n",
       "      <td>0.946556</td>\n",
       "      <td>0.794930</td>\n",
       "      <td>0.662958</td>\n",
       "      <td>0.473484</td>\n",
       "      <td>1.335021</td>\n",
       "      <td>0.338371</td>\n",
       "      <td>0.300303</td>\n",
       "      <td>0.690938</td>\n",
       "      <td>...</td>\n",
       "      <td>0.020668</td>\n",
       "      <td>0.171860</td>\n",
       "      <td>0.862337</td>\n",
       "      <td>0.525409</td>\n",
       "      <td>-0.029795</td>\n",
       "      <td>-0.263026</td>\n",
       "      <td>0.271724</td>\n",
       "      <td>0.934595</td>\n",
       "      <td>0.552001</td>\n",
       "      <td>-0.711617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DOSVAL001_HT29_24H:CMAP-000:-666</th>\n",
       "      <td>0.038320</td>\n",
       "      <td>-0.426547</td>\n",
       "      <td>0.183131</td>\n",
       "      <td>0.450992</td>\n",
       "      <td>-0.414180</td>\n",
       "      <td>-0.619587</td>\n",
       "      <td>-0.318295</td>\n",
       "      <td>0.066966</td>\n",
       "      <td>-0.618409</td>\n",
       "      <td>0.539847</td>\n",
       "      <td>...</td>\n",
       "      <td>0.085557</td>\n",
       "      <td>0.018541</td>\n",
       "      <td>0.174058</td>\n",
       "      <td>-0.101450</td>\n",
       "      <td>-0.279539</td>\n",
       "      <td>-0.303862</td>\n",
       "      <td>0.019368</td>\n",
       "      <td>1.111968</td>\n",
       "      <td>0.387193</td>\n",
       "      <td>-0.770082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DOSVAL001_HA1E_24H:CMAP-000:-666</th>\n",
       "      <td>0.319681</td>\n",
       "      <td>-0.182241</td>\n",
       "      <td>0.689418</td>\n",
       "      <td>0.542491</td>\n",
       "      <td>-0.124395</td>\n",
       "      <td>0.252069</td>\n",
       "      <td>-0.348502</td>\n",
       "      <td>0.145006</td>\n",
       "      <td>-0.018389</td>\n",
       "      <td>0.190280</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.048443</td>\n",
       "      <td>0.188158</td>\n",
       "      <td>0.422073</td>\n",
       "      <td>0.123565</td>\n",
       "      <td>0.097611</td>\n",
       "      <td>-0.003442</td>\n",
       "      <td>0.924158</td>\n",
       "      <td>-0.212382</td>\n",
       "      <td>0.166562</td>\n",
       "      <td>0.142994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DOSVAL001_A375_24H:CMAP-000:-666</th>\n",
       "      <td>0.091151</td>\n",
       "      <td>-0.007194</td>\n",
       "      <td>0.360459</td>\n",
       "      <td>0.430177</td>\n",
       "      <td>-0.443078</td>\n",
       "      <td>-0.370296</td>\n",
       "      <td>-0.450974</td>\n",
       "      <td>0.616529</td>\n",
       "      <td>0.258591</td>\n",
       "      <td>0.111886</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.571711</td>\n",
       "      <td>0.084373</td>\n",
       "      <td>0.240619</td>\n",
       "      <td>-0.372428</td>\n",
       "      <td>-0.168089</td>\n",
       "      <td>-0.137313</td>\n",
       "      <td>0.157594</td>\n",
       "      <td>0.256362</td>\n",
       "      <td>0.080780</td>\n",
       "      <td>-0.065995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DOSVAL001_HEPG2_24H:CMAP-000:-666</th>\n",
       "      <td>-0.276361</td>\n",
       "      <td>-0.321295</td>\n",
       "      <td>0.412983</td>\n",
       "      <td>0.040179</td>\n",
       "      <td>-0.144093</td>\n",
       "      <td>-0.374313</td>\n",
       "      <td>-0.488024</td>\n",
       "      <td>0.273988</td>\n",
       "      <td>-0.278131</td>\n",
       "      <td>-0.075510</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.440074</td>\n",
       "      <td>0.220422</td>\n",
       "      <td>-0.144075</td>\n",
       "      <td>-0.162023</td>\n",
       "      <td>-0.328652</td>\n",
       "      <td>-0.300582</td>\n",
       "      <td>0.469960</td>\n",
       "      <td>-0.533808</td>\n",
       "      <td>0.158130</td>\n",
       "      <td>-0.492051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DOSVAL001_MCF7_24H:CMAP-000:-666</th>\n",
       "      <td>0.562066</td>\n",
       "      <td>0.520038</td>\n",
       "      <td>0.751154</td>\n",
       "      <td>0.046425</td>\n",
       "      <td>0.820798</td>\n",
       "      <td>0.413035</td>\n",
       "      <td>0.023128</td>\n",
       "      <td>0.366703</td>\n",
       "      <td>-0.251062</td>\n",
       "      <td>0.249479</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.369176</td>\n",
       "      <td>-0.102268</td>\n",
       "      <td>0.466466</td>\n",
       "      <td>-0.158036</td>\n",
       "      <td>0.181730</td>\n",
       "      <td>-0.741085</td>\n",
       "      <td>0.759574</td>\n",
       "      <td>0.078421</td>\n",
       "      <td>-0.240611</td>\n",
       "      <td>0.000913</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3214 rows × 978 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          16        23        25        30  \\\n",
       "OFL001_A549_96H:G15                 1.854175  1.868439 -0.140405 -0.278911   \n",
       "OFL001_MCF7_96H:J10                 0.081511  0.651525 -0.205014  0.054704   \n",
       "ABY001_NCIH1975_XH:CMAP-000:-666:3  0.543459  1.647965 -1.731661  0.319534   \n",
       "ZTO.XPR001_THP1_408H:CMAP-000:-666 -0.054865 -0.085794 -0.319447  0.180520   \n",
       "MOA001_A549_24H:N01                 0.401776  1.197786  0.946556  0.794930   \n",
       "...                                      ...       ...       ...       ...   \n",
       "DOSVAL001_HT29_24H:CMAP-000:-666    0.038320 -0.426547  0.183131  0.450992   \n",
       "DOSVAL001_HA1E_24H:CMAP-000:-666    0.319681 -0.182241  0.689418  0.542491   \n",
       "DOSVAL001_A375_24H:CMAP-000:-666    0.091151 -0.007194  0.360459  0.430177   \n",
       "DOSVAL001_HEPG2_24H:CMAP-000:-666  -0.276361 -0.321295  0.412983  0.040179   \n",
       "DOSVAL001_MCF7_24H:CMAP-000:-666    0.562066  0.520038  0.751154  0.046425   \n",
       "\n",
       "                                          39        47       102       128  \\\n",
       "OFL001_A549_96H:G15                 0.396597  0.334116  0.473704 -0.565553   \n",
       "OFL001_MCF7_96H:J10                 0.726742 -0.126017  0.200712  0.915557   \n",
       "ABY001_NCIH1975_XH:CMAP-000:-666:3  1.078192  0.602553  0.323291  0.787790   \n",
       "ZTO.XPR001_THP1_408H:CMAP-000:-666  0.124284 -0.117936 -0.267994  0.429114   \n",
       "MOA001_A549_24H:N01                 0.662958  0.473484  1.335021  0.338371   \n",
       "...                                      ...       ...       ...       ...   \n",
       "DOSVAL001_HT29_24H:CMAP-000:-666   -0.414180 -0.619587 -0.318295  0.066966   \n",
       "DOSVAL001_HA1E_24H:CMAP-000:-666   -0.124395  0.252069 -0.348502  0.145006   \n",
       "DOSVAL001_A375_24H:CMAP-000:-666   -0.443078 -0.370296 -0.450974  0.616529   \n",
       "DOSVAL001_HEPG2_24H:CMAP-000:-666  -0.144093 -0.374313 -0.488024  0.273988   \n",
       "DOSVAL001_MCF7_24H:CMAP-000:-666    0.820798  0.413035  0.023128  0.366703   \n",
       "\n",
       "                                         142       154  ...     94239  \\\n",
       "OFL001_A549_96H:G15                 1.372410  1.181299  ...  1.252141   \n",
       "OFL001_MCF7_96H:J10                 0.780285  0.007211  ...  0.341261   \n",
       "ABY001_NCIH1975_XH:CMAP-000:-666:3  0.888264  1.532468  ...  0.704732   \n",
       "ZTO.XPR001_THP1_408H:CMAP-000:-666 -0.144781  0.190815  ... -0.114969   \n",
       "MOA001_A549_24H:N01                 0.300303  0.690938  ...  0.020668   \n",
       "...                                      ...       ...  ...       ...   \n",
       "DOSVAL001_HT29_24H:CMAP-000:-666   -0.618409  0.539847  ...  0.085557   \n",
       "DOSVAL001_HA1E_24H:CMAP-000:-666   -0.018389  0.190280  ... -0.048443   \n",
       "DOSVAL001_A375_24H:CMAP-000:-666    0.258591  0.111886  ... -0.571711   \n",
       "DOSVAL001_HEPG2_24H:CMAP-000:-666  -0.278131 -0.075510  ... -0.440074   \n",
       "DOSVAL001_MCF7_24H:CMAP-000:-666   -0.251062  0.249479  ... -0.369176   \n",
       "\n",
       "                                      116832    124583    147179    148022  \\\n",
       "OFL001_A549_96H:G15                -0.291923  1.193942  0.978987  2.381282   \n",
       "OFL001_MCF7_96H:J10                 0.405606 -0.054713  0.264261 -0.096964   \n",
       "ABY001_NCIH1975_XH:CMAP-000:-666:3 -1.326966  1.433667 -0.037051  1.016276   \n",
       "ZTO.XPR001_THP1_408H:CMAP-000:-666  0.308555  0.055869 -0.450732 -0.394338   \n",
       "MOA001_A549_24H:N01                 0.171860  0.862337  0.525409 -0.029795   \n",
       "...                                      ...       ...       ...       ...   \n",
       "DOSVAL001_HT29_24H:CMAP-000:-666    0.018541  0.174058 -0.101450 -0.279539   \n",
       "DOSVAL001_HA1E_24H:CMAP-000:-666    0.188158  0.422073  0.123565  0.097611   \n",
       "DOSVAL001_A375_24H:CMAP-000:-666    0.084373  0.240619 -0.372428 -0.168089   \n",
       "DOSVAL001_HEPG2_24H:CMAP-000:-666   0.220422 -0.144075 -0.162023 -0.328652   \n",
       "DOSVAL001_MCF7_24H:CMAP-000:-666   -0.102268  0.466466 -0.158036  0.181730   \n",
       "\n",
       "                                      200081    200734    256364    375346  \\\n",
       "OFL001_A549_96H:G15                -1.065447  1.174847 -0.885704  0.879203   \n",
       "OFL001_MCF7_96H:J10                 0.752965 -0.249324 -1.176310  0.282062   \n",
       "ABY001_NCIH1975_XH:CMAP-000:-666:3 -0.481035  1.061352  1.616178  1.540468   \n",
       "ZTO.XPR001_THP1_408H:CMAP-000:-666  0.029793  0.046924 -0.231632 -0.186150   \n",
       "MOA001_A549_24H:N01                -0.263026  0.271724  0.934595  0.552001   \n",
       "...                                      ...       ...       ...       ...   \n",
       "DOSVAL001_HT29_24H:CMAP-000:-666   -0.303862  0.019368  1.111968  0.387193   \n",
       "DOSVAL001_HA1E_24H:CMAP-000:-666   -0.003442  0.924158 -0.212382  0.166562   \n",
       "DOSVAL001_A375_24H:CMAP-000:-666   -0.137313  0.157594  0.256362  0.080780   \n",
       "DOSVAL001_HEPG2_24H:CMAP-000:-666  -0.300582  0.469960 -0.533808  0.158130   \n",
       "DOSVAL001_MCF7_24H:CMAP-000:-666   -0.741085  0.759574  0.078421 -0.240611   \n",
       "\n",
       "                                      388650  \n",
       "OFL001_A549_96H:G15                 0.216700  \n",
       "OFL001_MCF7_96H:J10                -0.212717  \n",
       "ABY001_NCIH1975_XH:CMAP-000:-666:3 -0.958139  \n",
       "ZTO.XPR001_THP1_408H:CMAP-000:-666 -0.309360  \n",
       "MOA001_A549_24H:N01                -0.711617  \n",
       "...                                      ...  \n",
       "DOSVAL001_HT29_24H:CMAP-000:-666   -0.770082  \n",
       "DOSVAL001_HA1E_24H:CMAP-000:-666    0.142994  \n",
       "DOSVAL001_A375_24H:CMAP-000:-666   -0.065995  \n",
       "DOSVAL001_HEPG2_24H:CMAP-000:-666  -0.492051  \n",
       "DOSVAL001_MCF7_24H:CMAP-000:-666    0.000913  \n",
       "\n",
       "[3214 rows x 978 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Gex data for controls\n",
    "cmap_controls = pd.read_csv('../preprocessing/preprocessed_data/baselineCell/cmap_all_baselines_q1.csv',index_col=0)\n",
    "display(cmap_controls)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75d5ecf9",
   "metadata": {},
   "source": [
    "# Train one trasnlation model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "01186e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_params = {'encoder_1_hiddens':[384,256],\n",
    "                'encoder_2_hiddens':[384,256],\n",
    "                'latent_dim': 128,\n",
    "                'decoder_1_hiddens':[256,384],\n",
    "                'decoder_2_hiddens':[256,384],\n",
    "                'dropout_decoder':0.2,\n",
    "                'dropout_encoder':0.1,\n",
    "                'encoder_activation':torch.nn.ELU(),\n",
    "                'decoder_activation':torch.nn.ELU(),\n",
    "                'V_dropout':0.25,\n",
    "                'state_class_hidden':[128,64,32],\n",
    "                'state_class_drop_in':0.5,\n",
    "                'state_class_drop':0.25,\n",
    "                'no_states':2,\n",
    "                'adv_class_hidden':[128,64,32],\n",
    "                'adv_class_drop_in':0.3,\n",
    "                'adv_class_drop':0.1,\n",
    "                'no_adv_class':2,\n",
    "                'encoding_lr':0.001,\n",
    "                'adv_lr':0.001,\n",
    "                'schedule_step_adv':200,\n",
    "                'gamma_adv':0.5,\n",
    "                'schedule_step_enc':200,\n",
    "                'gamma_enc':0.8,\n",
    "                'batch_size':512,\n",
    "                'epochs':1000,\n",
    "                'no_folds':5,\n",
    "                'v_reg':1e-04,\n",
    "                'state_class_reg':1e-02,\n",
    "                'enc_l2_reg':0.01,\n",
    "                'dec_l2_reg':0.01,\n",
    "                'lambda_mi_loss':100,\n",
    "                'effsize_reg': 100,\n",
    "                'cosine_loss': 10,\n",
    "                'adv_penalnty':100,\n",
    "                'reg_adv':1000,\n",
    "                'reg_classifier': 1000,\n",
    "                'similarity_reg' : 10,\n",
    "                'adversary_steps':4,\n",
    "                'autoencoder_wd': 0.,\n",
    "                'adversary_wd': 0.}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "92abbd27",
   "metadata": {},
   "outputs": [],
   "source": [
    "prior_betas = [1e-04,1e-03,1e-02,1e-01,1e0,1e1,1e2,1e3,1e4] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9a9327fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "bs =  model_params['batch_size']\n",
    "# k_folds=model_params['no_folds']\n",
    "NUM_EPOCHS=model_params['epochs']\n",
    "# kfold=KFold(n_splits=k_folds,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ff272630",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell = \"U2OS\"\n",
    "# df_result_all =  pd.read_csv('../results/PriorLossAnalysis/translation_results_testytest_'+cell+'.csv',index_col=0)\n",
    "# df_result_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f303b296",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_result_all = pd.DataFrame({})\n",
    "cell = \"U2OS\"\n",
    "j = 3\n",
    "for prior_beta in prior_betas:\n",
    "    genes_1 = np.load('../results/SameCellimputationModel/genes_subsets/genes_1'+cell+'_iter'+str(j)+'.npy',allow_pickle=True)\n",
    "    genes_2 = np.setdiff1d(cmap.columns.values,genes_1)\n",
    "    num_genes = genes_1.shape[0]\n",
    "    valPear = []\n",
    "    valPear_1 = []\n",
    "    valPear_2 = []\n",
    "    for i in range(model_params['no_folds']):\n",
    "        trainInfo = pd.read_csv('../preprocessing/preprocessed_data/SameCellimputationModel/'+cell+'/train_'+str(i)+'.csv',index_col=0)\n",
    "        valInfo = pd.read_csv('../preprocessing/preprocessed_data/SameCellimputationModel/'+cell+'/val_'+str(i)+'.csv',index_col=0)\n",
    "            \n",
    "        if len(trainInfo)<950:\n",
    "            bs = 256\n",
    "        else:\n",
    "            bs = model_params['batch_size']\n",
    "            \n",
    "        cmap_train = cmap.loc[trainInfo.sig_id,:]\n",
    "        cols = cmap_train.columns.values\n",
    "        cmap_train_shuffled = cmap_train.sample(frac=1, axis=1)\n",
    "        cmap_train_shuffled.columns = cols\n",
    "        cmap_val = cmap.loc[valInfo.sig_id,:]\n",
    "        N = len(cmap_train)\n",
    "        \n",
    "        curState_1 = torch.rand((N, model_params['latent_dim']), dtype=torch.float, requires_grad=False).to(device)\n",
    "        curState_2 = torch.rand((N, model_params['latent_dim']), dtype=torch.float, requires_grad=False).to(device)\n",
    "        # Network\n",
    "        decoder_1 = Decoder(model_params['latent_dim'],model_params['decoder_1_hiddens'],num_genes,\n",
    "                                dropRate=model_params['dropout_decoder'], \n",
    "                                activation=model_params['decoder_activation']).to(device)\n",
    "        decoder_2 = Decoder(model_params['latent_dim'],model_params['decoder_2_hiddens'],num_genes,\n",
    "                                dropRate=model_params['dropout_decoder'], \n",
    "                                activation=model_params['decoder_activation']).to(device)\n",
    "        encoder_1 = SimpleEncoder(num_genes,model_params['encoder_1_hiddens'],model_params['latent_dim'],\n",
    "                                      dropRate=model_params['dropout_encoder'], \n",
    "                                      activation=model_params['encoder_activation'],\n",
    "                                     normalizeOutput=False).to(device)\n",
    "        encoder_2 = SimpleEncoder(num_genes,model_params['encoder_2_hiddens'],model_params['latent_dim'],\n",
    "                                          dropRate=model_params['dropout_encoder'], \n",
    "                                          activation=model_params['encoder_activation'],\n",
    "                                     normalizeOutput=False).to(device)\n",
    "        prior_d = PriorDiscriminator(model_params['latent_dim']).to(device)\n",
    "        local_d = LocalDiscriminator(model_params['latent_dim'],model_params['latent_dim']).to(device)\n",
    "\n",
    "        allParams = list(decoder_1.parameters()) + list(encoder_1.parameters())\n",
    "        allParams = allParams + list(decoder_2.parameters()) + list(encoder_2.parameters())\n",
    "        allParams = allParams  + list(local_d.parameters())\n",
    "        allParams = allParams + list(prior_d.parameters())\n",
    "        optimizer = torch.optim.Adam(allParams, lr=model_params['encoding_lr'])\n",
    "        #optimizeD = torch.optim.Adam(prior_d.parameters(), lr=model_params['encoding_lr'])\n",
    "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n",
    "                                                        step_size=model_params['schedule_step_enc'],\n",
    "                                                        gamma=model_params['gamma_enc'])\n",
    "        trainLoss = []\n",
    "        trainLossSTD = []\n",
    "        for e in range(NUM_EPOCHS):\n",
    "            encoder_1.train()\n",
    "            decoder_1.train()\n",
    "            encoder_2.train()\n",
    "            decoder_2.train()\n",
    "            prior_d.train()\n",
    "            local_d.train()\n",
    "\n",
    "            trainloader = getSamples(N, bs)\n",
    "            trainLoss_ALL = []\n",
    "            for dataIndex in trainloader:\n",
    "\n",
    "                data_1 = torch.tensor(cmap_train.loc[:,genes_1].values).float()\n",
    "                data_1 = data_1[dataIndex,:].to(device)\n",
    "                data_2 = torch.tensor(cmap_train.loc[:,genes_2].values).float()\n",
    "                data_2 = data_2[dataIndex,:].to(device)\n",
    "                    \n",
    "                conditions = trainInfo.conditionId.values[dataIndex]\n",
    "                conditions = np.concatenate((conditions,conditions))\n",
    "                size = conditions.size\n",
    "                conditions = conditions.reshape(size,1)\n",
    "                conditions = conditions == conditions.transpose()\n",
    "                conditions = conditions*1\n",
    "                mask = torch.tensor(conditions).to(device).detach()\n",
    "                pos_mask = mask\n",
    "                neg_mask = 1 - mask\n",
    "                log_2 = math.log(2.)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                #optimizeD.zero_grad()\n",
    "                \n",
    "                z_1 = encoder_1(data_1)\n",
    "                z_2 = encoder_2(data_2)\n",
    "                    \n",
    "                #print('Epoch %s'%e)\n",
    "                #print(z_1)\n",
    "                    \n",
    "                latent_vectors = torch.cat((z_1, z_2), 0)\n",
    "                z_un = local_d(latent_vectors)\n",
    "                #z_un_1 = local_d(z_1)\n",
    "                #z_un_2 = local_d(z_2)\n",
    "                #res_un = torch.matmul(z_un_1, z_un_2.t())\n",
    "                res_un = torch.matmul(z_un, z_un.t())\n",
    "                    \n",
    "                #print(res_un)\n",
    "                    \n",
    "                Xhat_1 = decoder_1(z_1)\n",
    "                Xhat_2 = decoder_2(z_2)\n",
    "                loss_1 = torch.mean(torch.sum((Xhat_1 - data_1)**2,dim=1)) + encoder_1.L2Regularization(model_params['enc_l2_reg']) + decoder_1.L2Regularization(model_params['dec_l2_reg'])\n",
    "                loss_2 = torch.mean(torch.sum((Xhat_2 - data_2)**2,dim=1)) +encoder_2.L2Regularization(model_params['enc_l2_reg']) + decoder_2.L2Regularization(model_params['dec_l2_reg'])\n",
    "                    \n",
    "                silimalityLoss = torch.sum(torch.cdist(latent_vectors, latent_vectors) * pos_mask.float()) / pos_mask.float().sum()\n",
    "                #silimalityLoss = torch.mean(torch.cdist(z_1, z_2))\n",
    "                w1 = latent_vectors.norm(p=2, dim=1, keepdim=True)\n",
    "                w2 = latent_vectors.norm(p=2, dim=1, keepdim=True)\n",
    "                cosineLoss = torch.mm(latent_vectors, latent_vectors.t()) / (w1 * w2.t()).clamp(min=1e-6)\n",
    "                cosineLoss = torch.sum(cosineLoss * pos_mask.float()) / pos_mask.float().sum()\n",
    "                #cosineLoss = torch.mean(cosineLoss)\n",
    "\n",
    "                p_samples = res_un * pos_mask.float()\n",
    "                q_samples = res_un * neg_mask.float()\n",
    "\n",
    "                Ep = log_2 - F.softplus(- p_samples)\n",
    "                Eq = F.softplus(-q_samples) + q_samples - log_2\n",
    "\n",
    "                Ep = (Ep * pos_mask.float()).sum() / pos_mask.float().sum()\n",
    "                Eq = (Eq * neg_mask.float()).sum() / neg_mask.float().sum()\n",
    "                mi_loss = Eq - Ep\n",
    "                #mi_loss = torch.nan_to_num(mi_loss, nan=1e-03)\n",
    "\n",
    "                prior = torch.rand_like(torch.cat((z_1, z_2), 0))\n",
    "                #prior = torch.randn(torch.cat((z_1, z_2), 0).shape).to(device)\n",
    "                term_a = torch.log(prior_d(prior)).mean()\n",
    "                #term_b = torch.log(1.0 - prior_d(torch.cat((z_1, z_2), 0))).mean()\n",
    "                term_b = torch.log(prior_d(torch.cat((z_1, z_2), 0))).mean()\n",
    "                prior_loss = -(term_a + term_b)* prior_beta\n",
    "                ### MSE-LIKE PRIOR\n",
    "                #prior_loss1 = prior_beta * uniformLoss(curState_1, dataIndex, z_1,maxConstraintFactor=1)\n",
    "                #prior_loss2 = prior_beta * uniformLoss(curState_2, dataIndex, z_2,maxConstraintFactor=1)\n",
    "                #prior_loss_binned = torch.mean(torch.sum((torch.cat((z_1, z_2), 0) - prior)**2,dim=1))\n",
    "                #prior_loss = 0.5*(prior_loss1+prior_loss2) + prior_loss_binned\n",
    "                ### KL-divergence prior\n",
    "                #mu1 = torch.mean(torch.cat((z_1, z_2), 0),0)\n",
    "                ##mu2 = torch.mean(prior,0)\n",
    "                #sd1 = torch.std(torch.cat((z_1, z_2), 0),0)\n",
    "                ##sd2 = torch.std(prior,0)\n",
    "                #prior_loss = prior_beta * kl_loss(mu1,sd1)\n",
    "                    \n",
    "                loss = loss_1 + loss_2 + model_params[\n",
    "                        'similarity_reg']*silimalityLoss - model_params[\n",
    "                        'cosine_loss'] * cosineLoss + prior_loss + model_params['lambda_mi_loss'] * mi_loss\n",
    "                loss.backward()\n",
    "                #prior_loss.backward(retain_graph=True)\n",
    "                optimizer.step()\n",
    "                #optimizeD.step()\n",
    "\n",
    "                pear_1 = pearson_r(Xhat_1.detach(), data_1.detach())\n",
    "                mse_1 = torch.mean(torch.mean((Xhat_1.detach() - data_1.detach()) ** 2, dim=1))\n",
    "                pear_2 = pearson_r(Xhat_2.detach(), data_2.detach())\n",
    "                mse_2 = torch.mean(torch.mean((Xhat_2.detach() - data_2.detach()) ** 2, dim=1))\n",
    "                trainLoss_ALL.append(loss.item())\n",
    "                    \n",
    "            if e%250==0:\n",
    "                outString = 'Cell-line : '+cell+', prior beta {:.2f}'.format(prior_beta)\n",
    "                outString += ', Split {:.0f}: Epoch={:.0f}/{:.0f}'.format(i + 1, e + 1, NUM_EPOCHS)\n",
    "                outString += ', pearson_1={:.4f}'.format(pear_1.item())\n",
    "                outString += ', MSE_1={:.4f}'.format(mse_1.item())\n",
    "                outString += ', pearson_2={:.4f}'.format(pear_2.item())\n",
    "                outString += ', MSE_2={:.4f}'.format(mse_2.item())\n",
    "                outString += ', MI Loss={:.4f}'.format(mi_loss.item())\n",
    "                outString += ', Prior Loss={:.4f}'.format(prior_loss.item())\n",
    "                #outString += ', Prior Loss 2={:.4f}'.format(prior_gen.item())\n",
    "                outString += ', Cosine ={:.4f}'.format(cosineLoss.item())\n",
    "                outString += ', silimalityLoss ={:.4f}'.format(silimalityLoss.item())\n",
    "                outString += ', loss={:.4f}'.format(loss.item())\n",
    "                print(outString)\n",
    "            scheduler.step()\n",
    "            trainLoss.append(np.mean(trainLoss_ALL))\n",
    "            trainLossSTD.append(np.std(trainLoss_ALL))\n",
    "        outString = 'Cell-line : '+cell+', prior beta {:.2f}'.format(prior_beta)\n",
    "        outString += ', Split {:.0f}: Epoch={:.0f}/{:.0f}'.format(i + 1, e + 1, NUM_EPOCHS)\n",
    "        outString += ', pearson_1={:.4f}'.format(pear_1.item())\n",
    "        outString += ', MSE_1={:.4f}'.format(mse_1.item())\n",
    "        outString += ', pearson_2={:.4f}'.format(pear_2.item())\n",
    "        outString += ', MSE_2={:.4f}'.format(mse_2.item())\n",
    "        outString += ', MI Loss={:.4f}'.format(mi_loss.item())\n",
    "        outString += ', Prior Loss={:.4f}'.format(prior_loss.item())\n",
    "        #outString += ', Prior Loss 2={:.4f}'.format(prior_gen.item())\n",
    "        outString += ', Cosine ={:.4f}'.format(cosineLoss.item())\n",
    "        outString += ', silimalityLoss ={:.4f}'.format(silimalityLoss.item())\n",
    "        outString += ', loss={:.4f}'.format(loss.item())\n",
    "        print(outString)\n",
    "        plt.figure()\n",
    "        plt.hist(torch.cat((z_1, z_2), 0).detach().flatten().cpu().numpy(),40)\n",
    "        plt.title('Prior beta %s fold %s'%(prior_beta,i))\n",
    "\n",
    "        encoder_1.eval()\n",
    "        decoder_1.eval()\n",
    "        encoder_2.eval()\n",
    "        decoder_2.eval()\n",
    "        prior_d.eval()\n",
    "        local_d.eval()\n",
    "            \n",
    "        print('Validation performance for cell %s for beta %s for split %s'%(cell,prior_beta,i+1))\n",
    "\n",
    "\n",
    "        X_1 = torch.tensor(cmap_val.loc[:,genes_1].values).float().to(device)\n",
    "        X_2 = torch.tensor(cmap_val.loc[:,genes_2].values).float().to(device)\n",
    "                    \n",
    "        z_1 = encoder_1(X_1)\n",
    "        z_2 = encoder_2(X_2)\n",
    "        Xhat_1 = decoder_1(z_1)\n",
    "        Xhat_2 = decoder_2(z_2)\n",
    "            \n",
    "        pear_1 = pearson_r(Xhat_1.detach(), X_1.detach())\n",
    "        pear_2 = pearson_r(Xhat_2.detach(), X_2.detach())\n",
    "        valPear_1.append(pear_1.item())\n",
    "        valPear_2.append(pear_2.item())\n",
    "\n",
    "        print('Pearson correlation 1: %s'%pear_1.item())\n",
    "        print('Pearson correlation 2: %s'%pear_2.item())\n",
    "    \n",
    "    \n",
    "        x_hat_2_equivalent = decoder_2(z_1).detach()\n",
    "        pearson_2 = pearson_r(x_hat_2_equivalent.detach(), X_2.detach())\n",
    "        print('Pearson correlation 1 to 2: %s'%pearson_2.item())\n",
    "        x_hat_1_equivalent = decoder_1(z_2).detach()\n",
    "        pearson_1 = pearson_r(x_hat_1_equivalent.detach(), X_1.detach())\n",
    "        print('Pearson correlation 2 to 1: %s'%pearson_1.item())\n",
    "        \n",
    "        valPear.append([pearson_2.item(),pearson_1.item()])\n",
    "            \n",
    "        torch.save(decoder_1,'../results/PriorLossAnalysis/models_AutoTransOp/'+cell+'_discr_uniform/decoder_1_fold%s_beta%s.pt'%(i,prior_beta))\n",
    "        torch.save(decoder_2,'../results/PriorLossAnalysis/models_AutoTransOp/'+cell+'_discr_uniform/decoder_2_fold%s_beta%s.pt'%(i,prior_beta))\n",
    "        torch.save(prior_d,'../results/PriorLossAnalysis/models_AutoTransOp/'+cell+'_discr_uniform/priorDiscr_fold%s_beta%s.pt'%(i,prior_beta))\n",
    "        torch.save(local_d,'../results/PriorLossAnalysis/models_AutoTransOp/'+cell+'_discr_uniform/localDiscr_fold%s_beta%s.pt'%(i,prior_beta))\n",
    "        torch.save(encoder_1,'../results/PriorLossAnalysis/models_AutoTransOp/'+cell+'_discr_uniform/encoder_1_fold%s_beta%s.pt'%(i,prior_beta))\n",
    "        torch.save(encoder_2,'../results/PriorLossAnalysis/models_AutoTransOp/'+cell+'_discr_uniform/encoder_2_fold%s_beta%s.pt'%(i,prior_beta))\n",
    "        \n",
    "    valPear = np.array(valPear)\n",
    "    df_result = pd.DataFrame({'model_pearson2to1':valPear[:,0],'model_pearson1to2':valPear[:,1],\n",
    "                              'recon_pear_2':valPear_2 ,'recon_pear_1':valPear_1})\n",
    "    df_result['model'] = 'model'\n",
    "    df_result['set'] = 'validation'\n",
    "    df_result['cell'] = cell\n",
    "    df_result['beta'] = prior_beta\n",
    "    df_result_all = df_result_all.append(df_result)\n",
    "    df_result_all.to_csv('../results/PriorLossAnalysis/translation_results_testytest_'+cell+'.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92730e08",
   "metadata": {},
   "source": [
    "# Embeed for uniform prior : uniformLoss, discriminator, KLD for beta =1 and 100, 10**4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c6d26f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cell = \"U2OS\"\n",
    "j = 3\n",
    "genes_1 = np.load('../results/SameCellimputationModel/genes_subsets/genes_1'+cell+'_iter'+str(j)+'.npy',allow_pickle=True)\n",
    "genes_2 = np.setdiff1d(cmap.columns.values,genes_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba40872",
   "metadata": {},
   "source": [
    "### First uniformLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "44994cda",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start beta: 1.0\n",
      "Finished fold 0\n",
      "Finished fold 1\n",
      "Finished fold 2\n",
      "Finished fold 3\n",
      "Finished fold 4\n",
      "Start beta: 100.0\n",
      "Finished fold 0\n",
      "Finished fold 1\n",
      "Finished fold 2\n",
      "Finished fold 3\n",
      "Finished fold 4\n",
      "Start beta: 10000.0\n",
      "Finished fold 0\n",
      "Finished fold 1\n",
      "Finished fold 2\n",
      "Finished fold 3\n",
      "Finished fold 4\n"
     ]
    }
   ],
   "source": [
    "betas = [1.,100.,10000.]\n",
    "for prior_beta in betas:\n",
    "    print('Start beta: %s'%prior_beta)\n",
    "    for i in range(model_params['no_folds']):\n",
    "        \n",
    "        trainInfo = pd.read_csv('../preprocessing/preprocessed_data/SameCellimputationModel/'+cell+'/train_'+str(i)+'.csv',index_col=0)\n",
    "        valInfo = pd.read_csv('../preprocessing/preprocessed_data/SameCellimputationModel/'+cell+'/val_'+str(i)+'.csv',index_col=0)\n",
    "        cmap_train = cmap.loc[trainInfo.sig_id,:]\n",
    "        cols = cmap_train.columns.values\n",
    "        cmap_val = cmap.loc[valInfo.sig_id,:]\n",
    "        \n",
    "        Xtrain_1 = torch.tensor(cmap_train.loc[:,genes_1].values).float().to(device)\n",
    "        Xtrain_2 = torch.tensor(cmap_train.loc[:,genes_2].values).float().to(device)\n",
    "        Xval_1 = torch.tensor(cmap_val.loc[:,genes_1].values).float().to(device)\n",
    "        Xval_2 = torch.tensor(cmap_val.loc[:,genes_2].values).float().to(device)\n",
    "        \n",
    "        encoder_1 = torch.load('../results/PriorLossAnalysis/models_AutoTransOp/'+cell+'_uniform_mselike/encoder_1_fold%s_beta%s.pt'%(i,prior_beta))\n",
    "        encoder_2 = torch.load('../results/PriorLossAnalysis/models_AutoTransOp/'+cell+'_uniform_mselike/encoder_2_fold%s_beta%s.pt'%(i,prior_beta))\n",
    "        encoder_1.eval()\n",
    "        encoder_2.eval()\n",
    "        \n",
    "        ztrain_1 = encoder_1(Xtrain_1)\n",
    "        ztrain_2 = encoder_2(Xtrain_2)\n",
    "        zval_1 = encoder_1(Xval_1)\n",
    "        zval_2 = encoder_2(Xval_2)\n",
    "        \n",
    "        ## Save train embeddings \n",
    "        df_train_1 = pd.DataFrame(ztrain_1.detach().cpu().numpy())\n",
    "        df_train_1.columns = ['z_'+str(n) for n in range(model_params['latent_dim'])]\n",
    "        df_train_1.index = trainInfo.sig_id\n",
    "        df_train_1.to_csv('../results/PriorLossAnalysis/embs_AutoTransOp/'+cell+'_uniform_mselike/train_embs1_fold%s_beta%s.csv'%(i,prior_beta))\n",
    "        df_train_2 = pd.DataFrame(ztrain_2.detach().cpu().numpy())\n",
    "        df_train_2.columns = ['z_'+str(n) for n in range(model_params['latent_dim'])]\n",
    "        df_train_2.index = trainInfo.sig_id\n",
    "        df_train_2.to_csv('../results/PriorLossAnalysis/embs_AutoTransOp/'+cell+'_uniform_mselike/train_embs2_fold%s_beta%s.csv'%(i,prior_beta))\n",
    "        \n",
    "        ## Save validation embeddings \n",
    "        df_val_1 = pd.DataFrame(zval_1.detach().cpu().numpy())\n",
    "        df_val_1.columns = ['z_'+str(n) for n in range(model_params['latent_dim'])]\n",
    "        df_val_1.index = valInfo.sig_id\n",
    "        df_val_1.to_csv('../results/PriorLossAnalysis/embs_AutoTransOp/'+cell+'_uniform_mselike/val_embs1_fold%s_beta%s.csv'%(i,prior_beta))\n",
    "        df_val_2 = pd.DataFrame(zval_2.detach().cpu().numpy())\n",
    "        df_val_2.columns = ['z_'+str(n) for n in range(model_params['latent_dim'])]\n",
    "        df_val_2.index = valInfo.sig_id\n",
    "        df_val_2.to_csv('../results/PriorLossAnalysis/embs_AutoTransOp/'+cell+'_uniform_mselike/val_embs2_fold%s_beta%s.csv'%(i,prior_beta))\n",
    "        #print('Finished fold %s'%i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b387065",
   "metadata": {},
   "source": [
    "### Secondly uniform discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "179d35e0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start beta: 1.0\n",
      "Start beta: 100.0\n",
      "Start beta: 10000.0\n"
     ]
    }
   ],
   "source": [
    "betas = [1.,100.,10000.]\n",
    "for prior_beta in betas:\n",
    "    print('Start beta: %s'%prior_beta)\n",
    "    for i in range(model_params['no_folds']):\n",
    "        \n",
    "        trainInfo = pd.read_csv('../preprocessing/preprocessed_data/SameCellimputationModel/'+cell+'/train_'+str(i)+'.csv',index_col=0)\n",
    "        valInfo = pd.read_csv('../preprocessing/preprocessed_data/SameCellimputationModel/'+cell+'/val_'+str(i)+'.csv',index_col=0)\n",
    "        cmap_train = cmap.loc[trainInfo.sig_id,:]\n",
    "        cols = cmap_train.columns.values\n",
    "        cmap_val = cmap.loc[valInfo.sig_id,:]\n",
    "        \n",
    "        Xtrain_1 = torch.tensor(cmap_train.loc[:,genes_1].values).float().to(device)\n",
    "        Xtrain_2 = torch.tensor(cmap_train.loc[:,genes_2].values).float().to(device)\n",
    "        Xval_1 = torch.tensor(cmap_val.loc[:,genes_1].values).float().to(device)\n",
    "        Xval_2 = torch.tensor(cmap_val.loc[:,genes_2].values).float().to(device)\n",
    "        \n",
    "        encoder_1 = torch.load('../results/PriorLossAnalysis/models_AutoTransOp/'+cell+'_discr_uniform/encoder_1_fold%s_beta%s.pt'%(i,prior_beta))\n",
    "        encoder_2 = torch.load('../results/PriorLossAnalysis/models_AutoTransOp/'+cell+'_discr_uniform/encoder_2_fold%s_beta%s.pt'%(i,prior_beta))\n",
    "        encoder_1.eval()\n",
    "        encoder_2.eval()\n",
    "        \n",
    "        ztrain_1 = encoder_1(Xtrain_1)\n",
    "        ztrain_2 = encoder_2(Xtrain_2)\n",
    "        zval_1 = encoder_1(Xval_1)\n",
    "        zval_2 = encoder_2(Xval_2)\n",
    "        \n",
    "        ## Save train embeddings \n",
    "        df_train_1 = pd.DataFrame(ztrain_1.detach().cpu().numpy())\n",
    "        df_train_1.columns = ['z_'+str(n) for n in range(model_params['latent_dim'])]\n",
    "        df_train_1.index = trainInfo.sig_id\n",
    "        df_train_1.to_csv('../results/PriorLossAnalysis/embs_AutoTransOp/'+cell+'_discr_uniform/train_embs1_fold%s_beta%s.csv'%(i,prior_beta))\n",
    "        df_train_2 = pd.DataFrame(ztrain_2.detach().cpu().numpy())\n",
    "        df_train_2.columns = ['z_'+str(n) for n in range(model_params['latent_dim'])]\n",
    "        df_train_2.index = trainInfo.sig_id\n",
    "        df_train_2.to_csv('../results/PriorLossAnalysis/embs_AutoTransOp/'+cell+'_discr_uniform/train_embs2_fold%s_beta%s.csv'%(i,prior_beta))\n",
    "        \n",
    "        ## Save validation embeddings \n",
    "        df_val_1 = pd.DataFrame(zval_1.detach().cpu().numpy())\n",
    "        df_val_1.columns = ['z_'+str(n) for n in range(model_params['latent_dim'])]\n",
    "        df_val_1.index = valInfo.sig_id\n",
    "        df_val_1.to_csv('../results/PriorLossAnalysis/embs_AutoTransOp/'+cell+'_discr_uniform/val_embs1_fold%s_beta%s.csv'%(i,prior_beta))\n",
    "        df_val_2 = pd.DataFrame(zval_2.detach().cpu().numpy())\n",
    "        df_val_2.columns = ['z_'+str(n) for n in range(model_params['latent_dim'])]\n",
    "        df_val_2.index = valInfo.sig_id\n",
    "        df_val_2.to_csv('../results/PriorLossAnalysis/embs_AutoTransOp/'+cell+'_discr_uniform/val_embs2_fold%s_beta%s.csv'%(i,prior_beta))\n",
    "        #print('Finished fold %s'%i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa69667d",
   "metadata": {},
   "source": [
    "### Thirdly current discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6ebb1767",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start beta: 1.0\n",
      "Start beta: 100.0\n",
      "Start beta: 10000.0\n"
     ]
    }
   ],
   "source": [
    "betas = [1.,100.,10000.]\n",
    "for prior_beta in betas:\n",
    "    print('Start beta: %s'%prior_beta)\n",
    "    for i in range(model_params['no_folds']):\n",
    "        \n",
    "        trainInfo = pd.read_csv('../preprocessing/preprocessed_data/SameCellimputationModel/'+cell+'/train_'+str(i)+'.csv',index_col=0)\n",
    "        valInfo = pd.read_csv('../preprocessing/preprocessed_data/SameCellimputationModel/'+cell+'/val_'+str(i)+'.csv',index_col=0)\n",
    "        cmap_train = cmap.loc[trainInfo.sig_id,:]\n",
    "        cols = cmap_train.columns.values\n",
    "        cmap_val = cmap.loc[valInfo.sig_id,:]\n",
    "        \n",
    "        Xtrain_1 = torch.tensor(cmap_train.loc[:,genes_1].values).float().to(device)\n",
    "        Xtrain_2 = torch.tensor(cmap_train.loc[:,genes_2].values).float().to(device)\n",
    "        Xval_1 = torch.tensor(cmap_val.loc[:,genes_1].values).float().to(device)\n",
    "        Xval_2 = torch.tensor(cmap_val.loc[:,genes_2].values).float().to(device)\n",
    "        \n",
    "        encoder_1 = torch.load('../results/PriorLossAnalysis/models_AutoTransOp/'+cell+'_current/encoder_1_fold%s_beta%s.pt'%(i,prior_beta))\n",
    "        encoder_2 = torch.load('../results/PriorLossAnalysis/models_AutoTransOp/'+cell+'_current/encoder_2_fold%s_beta%s.pt'%(i,prior_beta))\n",
    "        encoder_1.eval()\n",
    "        encoder_2.eval()\n",
    "        \n",
    "        ztrain_1 = encoder_1(Xtrain_1)\n",
    "        ztrain_2 = encoder_2(Xtrain_2)\n",
    "        zval_1 = encoder_1(Xval_1)\n",
    "        zval_2 = encoder_2(Xval_2)\n",
    "        \n",
    "        ## Save train embeddings \n",
    "        df_train_1 = pd.DataFrame(ztrain_1.detach().cpu().numpy())\n",
    "        df_train_1.columns = ['z_'+str(n) for n in range(model_params['latent_dim'])]\n",
    "        df_train_1.index = trainInfo.sig_id\n",
    "        df_train_1.to_csv('../results/PriorLossAnalysis/embs_AutoTransOp/'+cell+'_current/train_embs1_fold%s_beta%s.csv'%(i,prior_beta))\n",
    "        df_train_2 = pd.DataFrame(ztrain_2.detach().cpu().numpy())\n",
    "        df_train_2.columns = ['z_'+str(n) for n in range(model_params['latent_dim'])]\n",
    "        df_train_2.index = trainInfo.sig_id\n",
    "        df_train_2.to_csv('../results/PriorLossAnalysis/embs_AutoTransOp/'+cell+'_current/train_embs2_fold%s_beta%s.csv'%(i,prior_beta))\n",
    "        \n",
    "        ## Save validation embeddings \n",
    "        df_val_1 = pd.DataFrame(zval_1.detach().cpu().numpy())\n",
    "        df_val_1.columns = ['z_'+str(n) for n in range(model_params['latent_dim'])]\n",
    "        df_val_1.index = valInfo.sig_id\n",
    "        df_val_1.to_csv('../results/PriorLossAnalysis/embs_AutoTransOp/'+cell+'_current/val_embs1_fold%s_beta%s.csv'%(i,prior_beta))\n",
    "        df_val_2 = pd.DataFrame(zval_2.detach().cpu().numpy())\n",
    "        df_val_2.columns = ['z_'+str(n) for n in range(model_params['latent_dim'])]\n",
    "        df_val_2.index = valInfo.sig_id\n",
    "        df_val_2.to_csv('../results/PriorLossAnalysis/embs_AutoTransOp/'+cell+'_current/val_embs2_fold%s_beta%s.csv'%(i,prior_beta))\n",
    "        #print('Finished fold %s'%i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "751b06de",
   "metadata": {},
   "source": [
    "### Normal KLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3553851c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start beta: 1.0\n",
      "Start beta: 100.0\n",
      "Start beta: 10000.0\n"
     ]
    }
   ],
   "source": [
    "betas = [1.,100.,10000.]\n",
    "for prior_beta in betas:\n",
    "    print('Start beta: %s'%prior_beta)\n",
    "    for i in range(model_params['no_folds']):\n",
    "        \n",
    "        trainInfo = pd.read_csv('../preprocessing/preprocessed_data/SameCellimputationModel/'+cell+'/train_'+str(i)+'.csv',index_col=0)\n",
    "        valInfo = pd.read_csv('../preprocessing/preprocessed_data/SameCellimputationModel/'+cell+'/val_'+str(i)+'.csv',index_col=0)\n",
    "        cmap_train = cmap.loc[trainInfo.sig_id,:]\n",
    "        cols = cmap_train.columns.values\n",
    "        cmap_val = cmap.loc[valInfo.sig_id,:]\n",
    "        \n",
    "        Xtrain_1 = torch.tensor(cmap_train.loc[:,genes_1].values).float().to(device)\n",
    "        Xtrain_2 = torch.tensor(cmap_train.loc[:,genes_2].values).float().to(device)\n",
    "        Xval_1 = torch.tensor(cmap_val.loc[:,genes_1].values).float().to(device)\n",
    "        Xval_2 = torch.tensor(cmap_val.loc[:,genes_2].values).float().to(device)\n",
    "        \n",
    "        encoder_1 = torch.load('../results/PriorLossAnalysis/models_AutoTransOp/'+cell+'_KLD_normal/encoder_1_fold%s_beta%s.pt'%(i,prior_beta))\n",
    "        encoder_2 = torch.load('../results/PriorLossAnalysis/models_AutoTransOp/'+cell+'_KLD_normal/encoder_2_fold%s_beta%s.pt'%(i,prior_beta))\n",
    "        encoder_1.eval()\n",
    "        encoder_2.eval()\n",
    "        \n",
    "        ztrain_1 = encoder_1(Xtrain_1)\n",
    "        ztrain_2 = encoder_2(Xtrain_2)\n",
    "        zval_1 = encoder_1(Xval_1)\n",
    "        zval_2 = encoder_2(Xval_2)\n",
    "        \n",
    "        ## Save train embeddings \n",
    "        df_train_1 = pd.DataFrame(ztrain_1.detach().cpu().numpy())\n",
    "        df_train_1.columns = ['z_'+str(n) for n in range(model_params['latent_dim'])]\n",
    "        df_train_1.index = trainInfo.sig_id\n",
    "        df_train_1.to_csv('../results/PriorLossAnalysis/embs_AutoTransOp/'+cell+'_KLD_normal/train_embs1_fold%s_beta%s.csv'%(i,prior_beta))\n",
    "        df_train_2 = pd.DataFrame(ztrain_2.detach().cpu().numpy())\n",
    "        df_train_2.columns = ['z_'+str(n) for n in range(model_params['latent_dim'])]\n",
    "        df_train_2.index = trainInfo.sig_id\n",
    "        df_train_2.to_csv('../results/PriorLossAnalysis/embs_AutoTransOp/'+cell+'_KLD_normal/train_embs2_fold%s_beta%s.csv'%(i,prior_beta))\n",
    "        \n",
    "        ## Save validation embeddings \n",
    "        df_val_1 = pd.DataFrame(zval_1.detach().cpu().numpy())\n",
    "        df_val_1.columns = ['z_'+str(n) for n in range(model_params['latent_dim'])]\n",
    "        df_val_1.index = valInfo.sig_id\n",
    "        df_val_1.to_csv('../results/PriorLossAnalysis/embs_AutoTransOp/'+cell+'_KLD_normal/val_embs1_fold%s_beta%s.csv'%(i,prior_beta))\n",
    "        df_val_2 = pd.DataFrame(zval_2.detach().cpu().numpy())\n",
    "        df_val_2.columns = ['z_'+str(n) for n in range(model_params['latent_dim'])]\n",
    "        df_val_2.index = valInfo.sig_id\n",
    "        df_val_2.to_csv('../results/PriorLossAnalysis/embs_AutoTransOp/'+cell+'_KLD_normal/val_embs2_fold%s_beta%s.csv'%(i,prior_beta))\n",
    "        #print('Finished fold %s'%i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be95085e",
   "metadata": {},
   "source": [
    "### Normal discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ae5f7cf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start beta: 1.0\n",
      "Start beta: 100.0\n",
      "Start beta: 10000.0\n"
     ]
    }
   ],
   "source": [
    "betas = [1.,100.,10000.]\n",
    "for prior_beta in betas:\n",
    "    print('Start beta: %s'%prior_beta)\n",
    "    for i in range(model_params['no_folds']):\n",
    "        \n",
    "        trainInfo = pd.read_csv('../preprocessing/preprocessed_data/SameCellimputationModel/'+cell+'/train_'+str(i)+'.csv',index_col=0)\n",
    "        valInfo = pd.read_csv('../preprocessing/preprocessed_data/SameCellimputationModel/'+cell+'/val_'+str(i)+'.csv',index_col=0)\n",
    "        cmap_train = cmap.loc[trainInfo.sig_id,:]\n",
    "        cols = cmap_train.columns.values\n",
    "        cmap_val = cmap.loc[valInfo.sig_id,:]\n",
    "        \n",
    "        Xtrain_1 = torch.tensor(cmap_train.loc[:,genes_1].values).float().to(device)\n",
    "        Xtrain_2 = torch.tensor(cmap_train.loc[:,genes_2].values).float().to(device)\n",
    "        Xval_1 = torch.tensor(cmap_val.loc[:,genes_1].values).float().to(device)\n",
    "        Xval_2 = torch.tensor(cmap_val.loc[:,genes_2].values).float().to(device)\n",
    "        \n",
    "        encoder_1 = torch.load('../results/PriorLossAnalysis/models_AutoTransOp/'+cell+'_discr_normal/encoder_1_fold%s_beta%s.pt'%(i,prior_beta))\n",
    "        encoder_2 = torch.load('../results/PriorLossAnalysis/models_AutoTransOp/'+cell+'_discr_normal/encoder_2_fold%s_beta%s.pt'%(i,prior_beta))\n",
    "        encoder_1.eval()\n",
    "        encoder_2.eval()\n",
    "        \n",
    "        ztrain_1 = encoder_1(Xtrain_1)\n",
    "        ztrain_2 = encoder_2(Xtrain_2)\n",
    "        zval_1 = encoder_1(Xval_1)\n",
    "        zval_2 = encoder_2(Xval_2)\n",
    "        \n",
    "        ## Save train embeddings \n",
    "        df_train_1 = pd.DataFrame(ztrain_1.detach().cpu().numpy())\n",
    "        df_train_1.columns = ['z_'+str(n) for n in range(model_params['latent_dim'])]\n",
    "        df_train_1.index = trainInfo.sig_id\n",
    "        df_train_1.to_csv('../results/PriorLossAnalysis/embs_AutoTransOp/'+cell+'_discr_normal/train_embs1_fold%s_beta%s.csv'%(i,prior_beta))\n",
    "        df_train_2 = pd.DataFrame(ztrain_2.detach().cpu().numpy())\n",
    "        df_train_2.columns = ['z_'+str(n) for n in range(model_params['latent_dim'])]\n",
    "        df_train_2.index = trainInfo.sig_id\n",
    "        df_train_2.to_csv('../results/PriorLossAnalysis/embs_AutoTransOp/'+cell+'_discr_normal/train_embs2_fold%s_beta%s.csv'%(i,prior_beta))\n",
    "        \n",
    "        ## Save validation embeddings \n",
    "        df_val_1 = pd.DataFrame(zval_1.detach().cpu().numpy())\n",
    "        df_val_1.columns = ['z_'+str(n) for n in range(model_params['latent_dim'])]\n",
    "        df_val_1.index = valInfo.sig_id\n",
    "        df_val_1.to_csv('../results/PriorLossAnalysis/embs_AutoTransOp/'+cell+'_discr_normal/val_embs1_fold%s_beta%s.csv'%(i,prior_beta))\n",
    "        df_val_2 = pd.DataFrame(zval_2.detach().cpu().numpy())\n",
    "        df_val_2.columns = ['z_'+str(n) for n in range(model_params['latent_dim'])]\n",
    "        df_val_2.index = valInfo.sig_id\n",
    "        df_val_2.to_csv('../results/PriorLossAnalysis/embs_AutoTransOp/'+cell+'_discr_normal/val_embs2_fold%s_beta%s.csv'%(i,prior_beta))\n",
    "        #print('Finished fold %s'%i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1e1c1cc",
   "metadata": {},
   "source": [
    "### Normal normalLoss (mselike)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cebb35d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start beta: 1.0\n",
      "Start beta: 100.0\n",
      "Start beta: 10000.0\n"
     ]
    }
   ],
   "source": [
    "betas = [1.,100.,10000.]\n",
    "for prior_beta in betas:\n",
    "    print('Start beta: %s'%prior_beta)\n",
    "    for i in range(model_params['no_folds']):\n",
    "        \n",
    "        trainInfo = pd.read_csv('../preprocessing/preprocessed_data/SameCellimputationModel/'+cell+'/train_'+str(i)+'.csv',index_col=0)\n",
    "        valInfo = pd.read_csv('../preprocessing/preprocessed_data/SameCellimputationModel/'+cell+'/val_'+str(i)+'.csv',index_col=0)\n",
    "        cmap_train = cmap.loc[trainInfo.sig_id,:]\n",
    "        cols = cmap_train.columns.values\n",
    "        cmap_val = cmap.loc[valInfo.sig_id,:]\n",
    "        \n",
    "        Xtrain_1 = torch.tensor(cmap_train.loc[:,genes_1].values).float().to(device)\n",
    "        Xtrain_2 = torch.tensor(cmap_train.loc[:,genes_2].values).float().to(device)\n",
    "        Xval_1 = torch.tensor(cmap_val.loc[:,genes_1].values).float().to(device)\n",
    "        Xval_2 = torch.tensor(cmap_val.loc[:,genes_2].values).float().to(device)\n",
    "        \n",
    "        encoder_1 = torch.load('../results/PriorLossAnalysis/models_AutoTransOp/'+cell+'_normal_mselike/encoder_1_fold%s_beta%s.pt'%(i,prior_beta))\n",
    "        encoder_2 = torch.load('../results/PriorLossAnalysis/models_AutoTransOp/'+cell+'_normal_mselike/encoder_2_fold%s_beta%s.pt'%(i,prior_beta))\n",
    "        encoder_1.eval()\n",
    "        encoder_2.eval()\n",
    "        \n",
    "        ztrain_1 = encoder_1(Xtrain_1)\n",
    "        ztrain_2 = encoder_2(Xtrain_2)\n",
    "        zval_1 = encoder_1(Xval_1)\n",
    "        zval_2 = encoder_2(Xval_2)\n",
    "        \n",
    "        ## Save train embeddings \n",
    "        df_train_1 = pd.DataFrame(ztrain_1.detach().cpu().numpy())\n",
    "        df_train_1.columns = ['z_'+str(n) for n in range(model_params['latent_dim'])]\n",
    "        df_train_1.index = trainInfo.sig_id\n",
    "        df_train_1.to_csv('../results/PriorLossAnalysis/embs_AutoTransOp/'+cell+'_normal_mselike/train_embs1_fold%s_beta%s.csv'%(i,prior_beta))\n",
    "        df_train_2 = pd.DataFrame(ztrain_2.detach().cpu().numpy())\n",
    "        df_train_2.columns = ['z_'+str(n) for n in range(model_params['latent_dim'])]\n",
    "        df_train_2.index = trainInfo.sig_id\n",
    "        df_train_2.to_csv('../results/PriorLossAnalysis/embs_AutoTransOp/'+cell+'_normal_mselike/train_embs2_fold%s_beta%s.csv'%(i,prior_beta))\n",
    "        \n",
    "        ## Save validation embeddings \n",
    "        df_val_1 = pd.DataFrame(zval_1.detach().cpu().numpy())\n",
    "        df_val_1.columns = ['z_'+str(n) for n in range(model_params['latent_dim'])]\n",
    "        df_val_1.index = valInfo.sig_id\n",
    "        df_val_1.to_csv('../results/PriorLossAnalysis/embs_AutoTransOp/'+cell+'_normal_mselike/val_embs1_fold%s_beta%s.csv'%(i,prior_beta))\n",
    "        df_val_2 = pd.DataFrame(zval_2.detach().cpu().numpy())\n",
    "        df_val_2.columns = ['z_'+str(n) for n in range(model_params['latent_dim'])]\n",
    "        df_val_2.index = valInfo.sig_id\n",
    "        df_val_2.to_csv('../results/PriorLossAnalysis/embs_AutoTransOp/'+cell+'_normal_mselike/val_embs2_fold%s_beta%s.csv'%(i,prior_beta))\n",
    "        #print('Finished fold %s'%i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c64a75",
   "metadata": {},
   "source": [
    "# Train model without prior loss at all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cf2b913d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cell-line : U2OS, Split 1: Epoch=1/1000, pearson_1=0.1055, MSE_1=1.8041, pearson_2=0.1241, MSE_2=1.8318, MI Loss=0.9609, Cosine =0.5425, silimalityLoss =7.3028, loss=1969.3877\n",
      "Cell-line : U2OS, Split 1: Epoch=251/1000, pearson_1=0.8167, MSE_1=0.4746, pearson_2=0.8137, MSE_2=0.4854, MI Loss=-0.6925, Cosine =0.8854, silimalityLoss =2.5328, loss=442.6235\n",
      "Cell-line : U2OS, Split 1: Epoch=501/1000, pearson_1=0.8857, MSE_1=0.8606, pearson_2=0.8836, MSE_2=0.7829, MI Loss=-0.6951, Cosine =0.8983, silimalityLoss =1.7417, loss=766.3377\n",
      "Cell-line : U2OS, Split 1: Epoch=751/1000, pearson_1=0.7894, MSE_1=0.4339, pearson_2=0.7904, MSE_2=0.4593, MI Loss=-0.6884, Cosine =0.8785, silimalityLoss =1.7648, loss=398.6841\n",
      "Cell-line : U2OS, Split 1: Epoch=1000/1000, pearson_1=0.8217, MSE_1=0.4964, pearson_2=0.8215, MSE_2=0.5073, MI Loss=-0.7204, Cosine =0.8912, silimalityLoss =1.4892, loss=445.1685\n",
      "Validation performance for cell U2OS for split 1\n",
      "Pearson correlation 1: 0.8342245221138\n",
      "Pearson correlation 2: 0.8327469825744629\n",
      "Pearson correlation 1 to 2: 0.8054160475730896\n",
      "Pearson correlation 2 to 1: 0.809795081615448\n",
      "Cell-line : U2OS, Split 2: Epoch=1/1000, pearson_1=0.1034, MSE_1=1.9386, pearson_2=0.0893, MSE_2=1.9805, MI Loss=1.3712, Cosine =0.5216, silimalityLoss =7.3146, loss=2149.1868\n",
      "Cell-line : U2OS, Split 2: Epoch=251/1000, pearson_1=0.8415, MSE_1=0.4280, pearson_2=0.8368, MSE_2=0.4423, MI Loss=-0.6325, Cosine =0.8879, silimalityLoss =2.2006, loss=398.1782\n",
      "Cell-line : U2OS, Split 2: Epoch=501/1000, pearson_1=0.8542, MSE_1=0.3959, pearson_2=0.8532, MSE_2=0.4042, MI Loss=-0.6495, Cosine =0.8951, silimalityLoss =1.5432, loss=351.5906\n",
      "Cell-line : U2OS, Split 2: Epoch=751/1000, pearson_1=0.8593, MSE_1=0.3708, pearson_2=0.8572, MSE_2=0.3703, MI Loss=-0.6530, Cosine =0.9032, silimalityLoss =1.2515, loss=316.8592\n",
      "Cell-line : U2OS, Split 2: Epoch=1000/1000, pearson_1=0.8489, MSE_1=0.3639, pearson_2=0.8468, MSE_2=0.3718, MI Loss=-0.6565, Cosine =0.9014, silimalityLoss =1.1207, loss=310.7932\n",
      "Validation performance for cell U2OS for split 2\n",
      "Pearson correlation 1: 0.8364781737327576\n",
      "Pearson correlation 2: 0.8371546864509583\n",
      "Pearson correlation 1 to 2: 0.8059917092323303\n",
      "Pearson correlation 2 to 1: 0.8090580701828003\n",
      "Cell-line : U2OS, Split 3: Epoch=1/1000, pearson_1=0.1685, MSE_1=1.5333, pearson_2=0.1726, MSE_2=1.4812, MI Loss=0.6341, Cosine =0.5354, silimalityLoss =7.4554, loss=1634.4131\n",
      "Cell-line : U2OS, Split 3: Epoch=251/1000, pearson_1=0.8092, MSE_1=0.4910, pearson_2=0.8037, MSE_2=0.5248, MI Loss=-0.7205, Cosine =0.8859, silimalityLoss =2.6520, loss=468.3030\n",
      "Cell-line : U2OS, Split 3: Epoch=501/1000, pearson_1=0.8097, MSE_1=0.5073, pearson_2=0.8079, MSE_2=0.5254, MI Loss=-0.7126, Cosine =0.8753, silimalityLoss =2.1032, loss=469.9379\n",
      "Cell-line : U2OS, Split 3: Epoch=751/1000, pearson_1=0.8213, MSE_1=0.5337, pearson_2=0.8249, MSE_2=0.5389, MI Loss=-0.7251, Cosine =0.8842, silimalityLoss =1.7373, loss=482.7516\n",
      "Cell-line : U2OS, Split 3: Epoch=1000/1000, pearson_1=0.8042, MSE_1=0.4799, pearson_2=0.8034, MSE_2=0.4854, MI Loss=-0.7352, Cosine =0.8794, silimalityLoss =1.5870, loss=426.3677\n",
      "Validation performance for cell U2OS for split 3\n",
      "Pearson correlation 1: 0.8278998136520386\n",
      "Pearson correlation 2: 0.8295702338218689\n",
      "Pearson correlation 1 to 2: 0.8017429113388062\n",
      "Pearson correlation 2 to 1: 0.8016331791877747\n",
      "Cell-line : U2OS, Split 4: Epoch=1/1000, pearson_1=0.1891, MSE_1=1.2880, pearson_2=0.1762, MSE_2=1.3761, MI Loss=0.7190, Cosine =0.5410, silimalityLoss =7.2468, loss=1469.3700\n",
      "Cell-line : U2OS, Split 4: Epoch=251/1000, pearson_1=0.7945, MSE_1=0.4583, pearson_2=0.7711, MSE_2=0.4810, MI Loss=-0.6596, Cosine =0.8803, silimalityLoss =2.8780, loss=440.2823\n",
      "Cell-line : U2OS, Split 4: Epoch=501/1000, pearson_1=0.7827, MSE_1=0.6248, pearson_2=0.7763, MSE_2=0.6671, MI Loss=-0.7485, Cosine =0.8716, silimalityLoss =2.3702, loss=597.4812\n",
      "Cell-line : U2OS, Split 4: Epoch=751/1000, pearson_1=0.7703, MSE_1=0.8300, pearson_2=0.7922, MSE_2=0.7976, MI Loss=-0.7330, Cosine =0.8645, silimalityLoss =1.9760, loss=757.8379\n",
      "Cell-line : U2OS, Split 4: Epoch=1000/1000, pearson_1=0.8229, MSE_1=0.7135, pearson_2=0.7972, MSE_2=0.7082, MI Loss=-0.7630, Cosine =0.8685, silimalityLoss =1.7345, loss=650.3873\n",
      "Validation performance for cell U2OS for split 4\n",
      "Pearson correlation 1: 0.8259067535400391\n",
      "Pearson correlation 2: 0.8184747695922852\n",
      "Pearson correlation 1 to 2: 0.7914569973945618\n",
      "Pearson correlation 2 to 1: 0.7980985641479492\n",
      "Cell-line : U2OS, Split 5: Epoch=1/1000, pearson_1=0.1430, MSE_1=1.7175, pearson_2=0.1379, MSE_2=1.7814, MI Loss=0.6509, Cosine =0.5419, silimalityLoss =7.3118, loss=1871.4606\n",
      "Cell-line : U2OS, Split 5: Epoch=251/1000, pearson_1=0.7657, MSE_1=0.5983, pearson_2=0.7773, MSE_2=0.6020, MI Loss=-0.6807, Cosine =0.8567, silimalityLoss =2.9828, loss=566.3592\n",
      "Cell-line : U2OS, Split 5: Epoch=501/1000, pearson_1=0.8214, MSE_1=0.4750, pearson_2=0.8216, MSE_2=0.4985, MI Loss=-0.7125, Cosine =0.8908, silimalityLoss =2.0968, loss=440.9822\n",
      "Cell-line : U2OS, Split 5: Epoch=751/1000, pearson_1=0.7963, MSE_1=0.4273, pearson_2=0.7981, MSE_2=0.4357, MI Loss=-0.7086, Cosine =0.8814, silimalityLoss =1.7967, loss=382.6397\n",
      "Cell-line : U2OS, Split 5: Epoch=1000/1000, pearson_1=0.8487, MSE_1=0.6813, pearson_2=0.8373, MSE_2=0.7540, MI Loss=-0.6919, Cosine =0.8951, silimalityLoss =1.4681, loss=659.3083\n",
      "Validation performance for cell U2OS for split 5\n",
      "Pearson correlation 1: 0.8280360698699951\n",
      "Pearson correlation 2: 0.8297311067581177\n",
      "Pearson correlation 1 to 2: 0.7987622022628784\n",
      "Pearson correlation 2 to 1: 0.8022040128707886\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_result_all = pd.DataFrame({})\n",
    "cell = \"U2OS\"\n",
    "j = 3\n",
    "genes_1 = np.load('../results/SameCellimputationModel/genes_subsets/genes_1'+cell+'_iter'+str(j)+'.npy',allow_pickle=True)\n",
    "genes_2 = np.setdiff1d(cmap.columns.values,genes_1)\n",
    "num_genes = genes_1.shape[0]\n",
    "valPear = []\n",
    "valPear_1 = []\n",
    "valPear_2 = []\n",
    "for i in range(model_params['no_folds']):\n",
    "    trainInfo = pd.read_csv('../preprocessing/preprocessed_data/SameCellimputationModel/'+cell+'/train_'+str(i)+'.csv',index_col=0)\n",
    "    valInfo = pd.read_csv('../preprocessing/preprocessed_data/SameCellimputationModel/'+cell+'/val_'+str(i)+'.csv',index_col=0)\n",
    "            \n",
    "    if len(trainInfo)<950:\n",
    "        bs = 256\n",
    "    else:\n",
    "        bs = model_params['batch_size']\n",
    "            \n",
    "    cmap_train = cmap.loc[trainInfo.sig_id,:]\n",
    "    cols = cmap_train.columns.values\n",
    "    cmap_train_shuffled = cmap_train.sample(frac=1, axis=1)\n",
    "    cmap_train_shuffled.columns = cols\n",
    "    cmap_val = cmap.loc[valInfo.sig_id,:]\n",
    "    N = len(cmap_train)\n",
    "    \n",
    "    # Network\n",
    "    decoder_1 = Decoder(model_params['latent_dim'],model_params['decoder_1_hiddens'],num_genes,\n",
    "                        dropRate=model_params['dropout_decoder'], \n",
    "                        activation=model_params['decoder_activation']).to(device)\n",
    "    decoder_2 = Decoder(model_params['latent_dim'],model_params['decoder_2_hiddens'],num_genes,\n",
    "                        dropRate=model_params['dropout_decoder'], \n",
    "                        activation=model_params['decoder_activation']).to(device)\n",
    "    encoder_1 = SimpleEncoder(num_genes,model_params['encoder_1_hiddens'],model_params['latent_dim'],\n",
    "                              dropRate=model_params['dropout_encoder'], \n",
    "                                      activation=model_params['encoder_activation'],\n",
    "                              normalizeOutput=False).to(device)\n",
    "    encoder_2 = SimpleEncoder(num_genes,model_params['encoder_2_hiddens'],model_params['latent_dim'],\n",
    "                              dropRate=model_params['dropout_encoder'], \n",
    "                              activation=model_params['encoder_activation'],\n",
    "                              normalizeOutput=False).to(device)\n",
    "    local_d = LocalDiscriminator(model_params['latent_dim'],model_params['latent_dim']).to(device)\n",
    "\n",
    "    allParams = list(decoder_1.parameters()) + list(encoder_1.parameters())\n",
    "    allParams = allParams + list(decoder_2.parameters()) + list(encoder_2.parameters())\n",
    "    allParams = allParams  + list(local_d.parameters())\n",
    "    optimizer = torch.optim.Adam(allParams, lr=model_params['encoding_lr'])\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n",
    "                                                step_size=model_params['schedule_step_enc'],\n",
    "                                                gamma=model_params['gamma_enc'])\n",
    "    trainLoss = []\n",
    "    trainLossSTD = []\n",
    "    for e in range(NUM_EPOCHS):\n",
    "        encoder_1.train()\n",
    "        decoder_1.train()\n",
    "        encoder_2.train()\n",
    "        decoder_2.train()\n",
    "        local_d.train()\n",
    "\n",
    "        trainloader = getSamples(N, bs)\n",
    "        trainLoss_ALL = []\n",
    "        for dataIndex in trainloader:\n",
    "\n",
    "            data_1 = torch.tensor(cmap_train.loc[:,genes_1].values).float()\n",
    "            data_1 = data_1[dataIndex,:].to(device)\n",
    "            data_2 = torch.tensor(cmap_train.loc[:,genes_2].values).float()\n",
    "            data_2 = data_2[dataIndex,:].to(device)\n",
    "                    \n",
    "            conditions = trainInfo.conditionId.values[dataIndex]\n",
    "            conditions = np.concatenate((conditions,conditions))\n",
    "            size = conditions.size\n",
    "            conditions = conditions.reshape(size,1)\n",
    "            conditions = conditions == conditions.transpose()\n",
    "            conditions = conditions*1\n",
    "            mask = torch.tensor(conditions).to(device).detach()\n",
    "            pos_mask = mask\n",
    "            neg_mask = 1 - mask\n",
    "            log_2 = math.log(2.)\n",
    "                \n",
    "            optimizer.zero_grad()\n",
    "                \n",
    "            z_1 = encoder_1(data_1)\n",
    "            z_2 = encoder_2(data_2)\n",
    "                    \n",
    "            latent_vectors = torch.cat((z_1, z_2), 0)\n",
    "            z_un = local_d(latent_vectors)\n",
    "            res_un = torch.matmul(z_un, z_un.t())\n",
    "              \n",
    "            Xhat_1 = decoder_1(z_1)\n",
    "            Xhat_2 = decoder_2(z_2)\n",
    "            loss_1 = torch.mean(torch.sum((Xhat_1 - data_1)**2,dim=1)) + encoder_1.L2Regularization(model_params['enc_l2_reg']) + decoder_1.L2Regularization(model_params['dec_l2_reg'])\n",
    "            loss_2 = torch.mean(torch.sum((Xhat_2 - data_2)**2,dim=1)) +encoder_2.L2Regularization(model_params['enc_l2_reg']) + decoder_2.L2Regularization(model_params['dec_l2_reg'])\n",
    "                    \n",
    "            silimalityLoss = torch.sum(torch.cdist(latent_vectors, latent_vectors) * pos_mask.float()) / pos_mask.float().sum()\n",
    "            w1 = latent_vectors.norm(p=2, dim=1, keepdim=True)\n",
    "            w2 = latent_vectors.norm(p=2, dim=1, keepdim=True)\n",
    "            cosineLoss = torch.mm(latent_vectors, latent_vectors.t()) / (w1 * w2.t()).clamp(min=1e-6)\n",
    "            cosineLoss = torch.sum(cosineLoss * pos_mask.float()) / pos_mask.float().sum()\n",
    "\n",
    "            p_samples = res_un * pos_mask.float()\n",
    "            q_samples = res_un * neg_mask.float()\n",
    "\n",
    "            Ep = log_2 - F.softplus(- p_samples)\n",
    "            Eq = F.softplus(-q_samples) + q_samples - log_2\n",
    "\n",
    "            Ep = (Ep * pos_mask.float()).sum() / pos_mask.float().sum()\n",
    "            Eq = (Eq * neg_mask.float()).sum() / neg_mask.float().sum()\n",
    "            mi_loss = Eq - Ep\n",
    "            \n",
    "            loss = loss_1 + loss_2 + model_params[\n",
    "                'similarity_reg']*silimalityLoss - model_params[\n",
    "                'cosine_loss'] * cosineLoss  + model_params['lambda_mi_loss'] * mi_loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            pear_1 = pearson_r(Xhat_1.detach(), data_1.detach())\n",
    "            mse_1 = torch.mean(torch.mean((Xhat_1.detach() - data_1.detach()) ** 2, dim=1))\n",
    "            pear_2 = pearson_r(Xhat_2.detach(), data_2.detach())\n",
    "            mse_2 = torch.mean(torch.mean((Xhat_2.detach() - data_2.detach()) ** 2, dim=1))\n",
    "            trainLoss_ALL.append(loss.item())\n",
    "                    \n",
    "        if e%250==0:\n",
    "            outString = 'Cell-line : '+cell\n",
    "            outString += ', Split {:.0f}: Epoch={:.0f}/{:.0f}'.format(i + 1, e + 1, NUM_EPOCHS)\n",
    "            outString += ', pearson_1={:.4f}'.format(pear_1.item())\n",
    "            outString += ', MSE_1={:.4f}'.format(mse_1.item())\n",
    "            outString += ', pearson_2={:.4f}'.format(pear_2.item())\n",
    "            outString += ', MSE_2={:.4f}'.format(mse_2.item())\n",
    "            outString += ', MI Loss={:.4f}'.format(mi_loss.item())\n",
    "            outString += ', Cosine ={:.4f}'.format(cosineLoss.item())\n",
    "            outString += ', silimalityLoss ={:.4f}'.format(silimalityLoss.item())\n",
    "            outString += ', loss={:.4f}'.format(loss.item())\n",
    "            print(outString)\n",
    "        scheduler.step()\n",
    "        trainLoss.append(np.mean(trainLoss_ALL))\n",
    "        trainLossSTD.append(np.std(trainLoss_ALL))\n",
    "    outString = 'Cell-line : '+cell\n",
    "    outString += ', Split {:.0f}: Epoch={:.0f}/{:.0f}'.format(i + 1, e + 1, NUM_EPOCHS)\n",
    "    outString += ', pearson_1={:.4f}'.format(pear_1.item())\n",
    "    outString += ', MSE_1={:.4f}'.format(mse_1.item())\n",
    "    outString += ', pearson_2={:.4f}'.format(pear_2.item())\n",
    "    outString += ', MSE_2={:.4f}'.format(mse_2.item())\n",
    "    outString += ', MI Loss={:.4f}'.format(mi_loss.item())\n",
    "    outString += ', Cosine ={:.4f}'.format(cosineLoss.item())\n",
    "    outString += ', silimalityLoss ={:.4f}'.format(silimalityLoss.item())\n",
    "    outString += ', loss={:.4f}'.format(loss.item())\n",
    "    print(outString)\n",
    "    plt.figure()\n",
    "    plt.hist(torch.cat((z_1, z_2), 0).detach().flatten().cpu().numpy(),40)\n",
    "    \n",
    "    encoder_1.eval()\n",
    "    decoder_1.eval()\n",
    "    encoder_2.eval()\n",
    "    decoder_2.eval()\n",
    "    local_d.eval()    \n",
    "    \n",
    "    print('Validation performance for cell %s for split %s'%(cell,i+1))\n",
    "\n",
    "\n",
    "    X_1 = torch.tensor(cmap_val.loc[:,genes_1].values).float().to(device)\n",
    "    X_2 = torch.tensor(cmap_val.loc[:,genes_2].values).float().to(device)\n",
    "                    \n",
    "    z_1 = encoder_1(X_1)\n",
    "    z_2 = encoder_2(X_2)\n",
    "    Xhat_1 = decoder_1(z_1)\n",
    "    Xhat_2 = decoder_2(z_2)\n",
    "            \n",
    "    pear_1 = pearson_r(Xhat_1.detach(), X_1.detach())\n",
    "    pear_2 = pearson_r(Xhat_2.detach(), X_2.detach())\n",
    "    valPear_1.append(pear_1.item())\n",
    "    valPear_2.append(pear_2.item())\n",
    "\n",
    "    print('Pearson correlation 1: %s'%pear_1.item())\n",
    "    print('Pearson correlation 2: %s'%pear_2.item())\n",
    "    \n",
    "    \n",
    "    x_hat_2_equivalent = decoder_2(z_1).detach()\n",
    "    pearson_2 = pearson_r(x_hat_2_equivalent.detach(), X_2.detach())\n",
    "    print('Pearson correlation 1 to 2: %s'%pearson_2.item())\n",
    "    x_hat_1_equivalent = decoder_1(z_2).detach()\n",
    "    pearson_1 = pearson_r(x_hat_1_equivalent.detach(), X_1.detach())\n",
    "    print('Pearson correlation 2 to 1: %s'%pearson_1.item())\n",
    "        \n",
    "    valPear.append([pearson_2.item(),pearson_1.item()])\n",
    "            \n",
    "    torch.save(decoder_1,'../results/PriorLossAnalysis/models_AutoTransOp/'+cell+'_noprior/decoder_1_fold%s_noprior.pt'%i)\n",
    "    torch.save(decoder_2,'../results/PriorLossAnalysis/models_AutoTransOp/'+cell+'_noprior/decoder_2_fold%s_noprior.pt'%i)\n",
    "    torch.save(local_d,'../results/PriorLossAnalysis/models_AutoTransOp/'+cell+'_noprior/localDiscr_fold%s_noprior.pt'%i)\n",
    "    torch.save(encoder_1,'../results/PriorLossAnalysis/models_AutoTransOp/'+cell+'_noprior/encoder_1_fold%s_noprior.pt'%i)\n",
    "    torch.save(encoder_2,'../results/PriorLossAnalysis/models_AutoTransOp/'+cell+'_noprior/encoder_2_fold%s_noprior.pt'%i)\n",
    "        \n",
    "valPear = np.array(valPear)\n",
    "df_result = pd.DataFrame({'model_pearson2to1':valPear[:,0],'model_pearson1to2':valPear[:,1],\n",
    "                              'recon_pear_2':valPear_2 ,'recon_pear_1':valPear_1})\n",
    "df_result['model'] = 'model'\n",
    "df_result['set'] = 'validation'\n",
    "df_result['cell'] = cell\n",
    "df_result_all = df_result_all.append(df_result)\n",
    "df_result_all.to_csv('../results/PriorLossAnalysis/translation_results_noprior_'+cell+'.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e0fc404c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(model_params['no_folds']):\n",
    "    \n",
    "    trainInfo = pd.read_csv('../preprocessing/preprocessed_data/SameCellimputationModel/'+cell+'/train_'+str(i)+'.csv',index_col=0)\n",
    "    valInfo = pd.read_csv('../preprocessing/preprocessed_data/SameCellimputationModel/'+cell+'/val_'+str(i)+'.csv',index_col=0)\n",
    "    cmap_train = cmap.loc[trainInfo.sig_id,:]\n",
    "    cols = cmap_train.columns.values\n",
    "    cmap_val = cmap.loc[valInfo.sig_id,:]\n",
    "        \n",
    "    Xtrain_1 = torch.tensor(cmap_train.loc[:,genes_1].values).float().to(device)\n",
    "    Xtrain_2 = torch.tensor(cmap_train.loc[:,genes_2].values).float().to(device)\n",
    "    Xval_1 = torch.tensor(cmap_val.loc[:,genes_1].values).float().to(device)\n",
    "    Xval_2 = torch.tensor(cmap_val.loc[:,genes_2].values).float().to(device)\n",
    "        \n",
    "    encoder_1 = torch.load('../results/PriorLossAnalysis/models_AutoTransOp/'+cell+'_noprior/encoder_1_fold%s_noprior.pt'%i)\n",
    "    encoder_2 = torch.load('../results/PriorLossAnalysis/models_AutoTransOp/'+cell+'_noprior/encoder_2_fold%s_noprior.pt'%i)\n",
    "    encoder_1.eval()\n",
    "    encoder_2.eval()\n",
    "        \n",
    "    ztrain_1 = encoder_1(Xtrain_1)\n",
    "    ztrain_2 = encoder_2(Xtrain_2)\n",
    "    zval_1 = encoder_1(Xval_1)\n",
    "    zval_2 = encoder_2(Xval_2)\n",
    "        \n",
    "    ## Save train embeddings \n",
    "    df_train_1 = pd.DataFrame(ztrain_1.detach().cpu().numpy())\n",
    "    df_train_1.columns = ['z_'+str(n) for n in range(model_params['latent_dim'])]\n",
    "    df_train_1.index = trainInfo.sig_id\n",
    "    df_train_1.to_csv('../results/PriorLossAnalysis/embs_AutoTransOp/'+cell+'_noprior/train_embs1_fold%s.csv'%i)\n",
    "    df_train_2 = pd.DataFrame(ztrain_2.detach().cpu().numpy())\n",
    "    df_train_2.columns = ['z_'+str(n) for n in range(model_params['latent_dim'])]\n",
    "    df_train_2.index = trainInfo.sig_id\n",
    "    df_train_2.to_csv('../results/PriorLossAnalysis/embs_AutoTransOp/'+cell+'_noprior/train_embs2_fold%s.csv'%i)\n",
    "        \n",
    "    ## Save validation embeddings \n",
    "    df_val_1 = pd.DataFrame(zval_1.detach().cpu().numpy())\n",
    "    df_val_1.columns = ['z_'+str(n) for n in range(model_params['latent_dim'])]\n",
    "    df_val_1.index = valInfo.sig_id\n",
    "    df_val_1.to_csv('../results/PriorLossAnalysis/embs_AutoTransOp/'+cell+'_noprior/val_embs1_fold%s.csv'%i)\n",
    "    df_val_2 = pd.DataFrame(zval_2.detach().cpu().numpy())\n",
    "    df_val_2.columns = ['z_'+str(n) for n in range(model_params['latent_dim'])]\n",
    "    df_val_2.index = valInfo.sig_id\n",
    "    df_val_2.to_csv('../results/PriorLossAnalysis/embs_AutoTransOp/'+cell+'_noprior/val_embs2_fold%s.csv'%i)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eaaecc7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
