{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "44c0260f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division\n",
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from trainingUtils import MultipleOptimizer, MultipleScheduler, compute_kernel, compute_mmd\n",
    "from models import Encoder,Decoder,VAE,CellStateEncoder,\\\n",
    "                   CellStateDecoder, CellStateVAE,\\\n",
    "                   SimpleEncoder,LocalDiscriminator,PriorDiscriminator,\\\n",
    "                   EmbInfomax,MultiEncInfomax\n",
    "# import argparse\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "import random\n",
    "import os\n",
    "from IPython.display import clear_output\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "#from scipy.stats import pearsonr\n",
    "from scipy.stats import spearmanr\n",
    "from evaluationUtils import r_square,get_cindex,pearson_r,pseudoAccuracy\n",
    "import seaborn as sns\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e11c5049",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f08d1d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize environment and seeds for reproducability\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "\n",
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    \n",
    "# Read data\n",
    "cmap = pd.read_csv('../preprocessing/preprocessed_data/cmap_landmarks_HT29_A375.csv',index_col = 0)\n",
    "#cmap = pd.read_csv('cmap_HT29_A375.csv',index_col = 0)\n",
    "cmap_extra = pd.read_csv('../preprocessing/preprocessed_data/cmap_PC3_MCF7_landmarks.csv',index_col = 0)\n",
    "\n",
    "gene_size = len(cmap.columns)\n",
    "samples = cmap.index.values\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "97578e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a train generators\n",
    "def getSamples(N, batchSize):\n",
    "    order = np.random.permutation(N)\n",
    "    outList = []\n",
    "    while len(order)>0:\n",
    "        outList.append(order[0:batchSize])\n",
    "        order = order[batchSize:]\n",
    "    return outList"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9179208",
   "metadata": {},
   "source": [
    "### Run adding PC3 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "43d2c01b",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 2000\n",
    "# bs_pc3 = 200\n",
    "# bs_ht29_paired = 100\n",
    "# bs_a375_paired = 100\n",
    "bs_pc3 = 180\n",
    "bs_ht29_paired = 80\n",
    "bs_a375_paired = 80\n",
    "beta=1.0\n",
    "\n",
    "bs_a375 = 178\n",
    "bs_ht29 = 154\n",
    "bs_paired = 90"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b28f7c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainloader_pc3 = getSamples(N_pc3, bs_pc3)\n",
    "# len_pc3 = len(trainloader_pc3)\n",
    "# trainloader_paired_ht29 = getSamples(N_ht29_paired, bs_ht29_paired)\n",
    "# len_paired_ht29 = len(trainloader_paired_ht29)\n",
    "# trainloader_paired_a375 = getSamples(N_a375_paired, bs_a375_paired)\n",
    "# len_paired_a375 = len(trainloader_paired_a375)\n",
    "        \n",
    "# trainloader_a375 = getSamples(N_a375, bs_a375)\n",
    "# len_a375 = len(trainloader_a375)\n",
    "# trainloader_ht29 = getSamples(N_ht29, bs_ht29)\n",
    "# len_ht29 = len(trainloader_ht29)\n",
    "# trainloader_paired = getSamples(N_paired, bs_paired)\n",
    "# len_paired = len(trainloader_paired)\n",
    "\n",
    "# lens = [len_pc3,len_paired_ht29,len_paired_a375,len_a375,len_ht29,len_paired]\n",
    "# lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3019e12a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split 1: Epoch=1/2000, r2_pc3=-2.8406, pearson_pc3=0.2975, MSE_pc3=2.5531, r2_a375=-4.2090, pearson_a375=0.3238, MSE_a375=3.7447, r2_ht29=-3.3900, pearson_ht29=0.3632, MSE_ht29=3.2231, MI Loss=1.9141, Prior Loss=0.7044, loss=10596.6270\n",
      "Split 1: Epoch=251/2000, r2_pc3=0.5592, pearson_pc3=0.8295, MSE_pc3=0.8345, r2_a375=0.5897, pearson_a375=0.8395, MSE_a375=0.9896, r2_ht29=0.5605, pearson_ht29=0.8397, MSE_ht29=1.0148, MI Loss=-0.6513, Prior Loss=0.0000, loss=2926.9429\n",
      "Split 1: Epoch=501/2000, r2_pc3=0.6146, pearson_pc3=0.8600, MSE_pc3=0.7704, r2_a375=0.6917, pearson_a375=0.8553, MSE_a375=0.8149, r2_ht29=0.6563, pearson_ht29=0.8707, MSE_ht29=0.8762, MI Loss=-0.6607, Prior Loss=0.0000, loss=2527.2710\n",
      "Split 1: Epoch=751/2000, r2_pc3=0.6110, pearson_pc3=0.8584, MSE_pc3=0.7812, r2_a375=0.6859, pearson_a375=0.8836, MSE_a375=0.8894, r2_ht29=0.6835, pearson_ht29=0.8530, MSE_ht29=0.7648, MI Loss=-0.6992, Prior Loss=0.0000, loss=2492.6328\n",
      "Split 1: Epoch=1001/2000, r2_pc3=0.6531, pearson_pc3=0.8629, MSE_pc3=0.7242, r2_a375=0.7053, pearson_a375=0.8734, MSE_a375=0.7936, r2_ht29=0.6788, pearson_ht29=0.8710, MSE_ht29=0.7489, MI Loss=-0.6863, Prior Loss=0.0000, loss=2318.3853\n",
      "Split 1: Epoch=1251/2000, r2_pc3=0.6653, pearson_pc3=0.8604, MSE_pc3=0.6634, r2_a375=0.7098, pearson_a375=0.8660, MSE_a375=0.7320, r2_ht29=0.6907, pearson_ht29=0.8662, MSE_ht29=0.7563, MI Loss=-0.6853, Prior Loss=0.0000, loss=2197.3726\n",
      "Split 1: Epoch=1501/2000, r2_pc3=0.6603, pearson_pc3=0.8646, MSE_pc3=0.6845, r2_a375=0.6809, pearson_a375=0.8671, MSE_a375=0.8459, r2_ht29=0.7027, pearson_ht29=0.8831, MSE_ht29=0.7453, MI Loss=-0.6695, Prior Loss=0.0000, loss=2309.8350\n",
      "Split 1: Epoch=1751/2000, r2_pc3=0.6712, pearson_pc3=0.8563, MSE_pc3=0.6609, r2_a375=0.7142, pearson_a375=0.8777, MSE_a375=0.7685, r2_ht29=0.7419, pearson_ht29=0.8795, MSE_ht29=0.6683, MI Loss=-0.7057, Prior Loss=0.0000, loss=2133.2048\n",
      "Split 1: Epoch=2000/2000, r2_pc3=0.6824, pearson_pc3=0.8706, MSE_pc3=0.6600, r2_a375=0.7429, pearson_a375=0.8840, MSE_a375=0.6891, r2_ht29=0.7493, pearson_ht29=0.8982, MSE_ht29=0.6740, MI Loss=-0.6999, Prior Loss=0.0000, loss=2053.8857\n",
      "Pearson correlation PC3: 0.8238744735717773\n",
      "Spearman correlation PC3: 0.7931748455140645\n",
      "Pseudo-Accuracy PC3: 0.8114340738975915\n",
      "Pearson of direct A375 to PC3 translation: 0.4655536115169525\n",
      "Pearson correlation A375 to PC3: 0.6761610507965088\n",
      "Pseudo accuracy A375 to PC3: 0.7164621676891615\n",
      "Pearson correlation PC3 to A375: 0.5105640888214111\n",
      "Pseudo accuracy PC3 to A375: 0.7369120654396728\n",
      "Pearson of direct HT29 to PC3 translation: 0.43983644247055054\n",
      "Pearson correlation HT29 to PC3: 0.4773520827293396\n",
      "Pseudo accuracy HT29 to PC3: 0.7116564417177914\n",
      "Pearson correlation PC3 to HT29: 0.4818408787250519\n",
      "Pseudo accuracy PC3 to HT29: 0.7248466257668712\n",
      "Pearson correlation A375: 0.8410838842391968\n",
      "Spearman correlation A375: 0.7815745603763724\n",
      "Pseudo-Accuracy A375: 0.7978842221173509\n",
      "Pearson correlation HT29: 0.8530290126800537\n",
      "Spearman correlation HT29: 0.803898679881443\n",
      "Pseudo-Accuracy HT29: 0.8123466257668712\n",
      "Pearson correlation A375 to HT29: 0.7379741668701172\n",
      "Pseudo accuracy A375 to HT29: 0.7328476482617587\n",
      "Pearson correlation HT29 to A375: 0.5412670969963074\n",
      "Pseudo accuracy HT29 to A375: 0.7188139059304703\n",
      "Split 2: Epoch=1/2000, r2_pc3=-3.2136, pearson_pc3=0.3032, MSE_pc3=2.7100, r2_a375=-3.5636, pearson_a375=0.3268, MSE_a375=3.1322, r2_ht29=-2.9836, pearson_ht29=0.3634, MSE_ht29=2.9034, MI Loss=2.1621, Prior Loss=0.6800, loss=9868.3164\n",
      "Split 2: Epoch=251/2000, r2_pc3=0.5878, pearson_pc3=0.8392, MSE_pc3=0.8348, r2_a375=0.5812, pearson_a375=0.8311, MSE_a375=0.9795, r2_ht29=0.5967, pearson_ht29=0.8205, MSE_ht29=0.8684, MI Loss=-0.6544, Prior Loss=0.0000, loss=2781.9429\n",
      "Split 2: Epoch=501/2000, r2_pc3=0.6194, pearson_pc3=0.8484, MSE_pc3=0.7603, r2_a375=0.6679, pearson_a375=0.8533, MSE_a375=0.8511, r2_ht29=0.5971, pearson_ht29=0.8480, MSE_ht29=0.9309, MI Loss=-0.6646, Prior Loss=0.0000, loss=2616.0715\n",
      "Split 2: Epoch=751/2000, r2_pc3=0.6352, pearson_pc3=0.8598, MSE_pc3=0.7578, r2_a375=0.7015, pearson_a375=0.8631, MSE_a375=0.8415, r2_ht29=0.7049, pearson_ht29=0.8724, MSE_ht29=0.7632, MI Loss=-0.6806, Prior Loss=0.0000, loss=2419.6799\n",
      "Split 2: Epoch=1001/2000, r2_pc3=0.6245, pearson_pc3=0.8597, MSE_pc3=0.7656, r2_a375=0.7026, pearson_a375=0.8846, MSE_a375=0.8233, r2_ht29=0.6422, pearson_ht29=0.8316, MSE_ht29=0.8386, MI Loss=-0.6900, Prior Loss=0.0000, loss=2473.6226\n",
      "Split 2: Epoch=1251/2000, r2_pc3=0.6589, pearson_pc3=0.8654, MSE_pc3=0.6972, r2_a375=0.6984, pearson_a375=0.8678, MSE_a375=0.7615, r2_ht29=0.6261, pearson_ht29=0.8242, MSE_ht29=0.8370, MI Loss=-0.6920, Prior Loss=0.0000, loss=2339.8950\n",
      "Split 2: Epoch=1501/2000, r2_pc3=0.6432, pearson_pc3=0.8418, MSE_pc3=0.6981, r2_a375=0.6961, pearson_a375=0.8772, MSE_a375=0.7815, r2_ht29=0.6343, pearson_ht29=0.8782, MSE_ht29=0.9318, MI Loss=-0.6996, Prior Loss=0.0000, loss=2445.5281\n",
      "Split 2: Epoch=1751/2000, r2_pc3=0.6657, pearson_pc3=0.8707, MSE_pc3=0.6905, r2_a375=0.6943, pearson_a375=0.8489, MSE_a375=0.7527, r2_ht29=0.7132, pearson_ht29=0.8675, MSE_ht29=0.7114, MI Loss=-0.7038, Prior Loss=0.0000, loss=2188.8203\n",
      "Split 2: Epoch=2000/2000, r2_pc3=0.6689, pearson_pc3=0.8724, MSE_pc3=0.6921, r2_a375=0.6892, pearson_a375=0.8834, MSE_a375=0.8432, r2_ht29=0.6456, pearson_ht29=0.8460, MSE_ht29=0.7784, MI Loss=-0.6851, Prior Loss=0.0000, loss=2340.1816\n",
      "Pearson correlation PC3: 0.820963978767395\n",
      "Spearman correlation PC3: 0.7864045407810193\n",
      "Pseudo-Accuracy PC3: 0.8083844580777096\n",
      "Pearson of direct A375 to PC3 translation: 0.42136263847351074\n",
      "Pearson correlation A375 to PC3: 0.6569623351097107\n",
      "Pseudo accuracy A375 to PC3: 0.7125586430891375\n",
      "Pearson correlation PC3 to A375: 0.5107272863388062\n",
      "Pseudo accuracy PC3 to A375: 0.7286178274990978\n",
      "Pearson of direct HT29 to PC3 translation: 0.3221885561943054\n",
      "Pearson correlation HT29 to PC3: 0.36377790570259094\n",
      "Pseudo accuracy HT29 to PC3: 0.6812823288824732\n",
      "Pearson correlation PC3 to HT29: 0.3825327157974243\n",
      "Pseudo accuracy PC3 to HT29: 0.6752075063154096\n",
      "Pearson correlation A375: 0.8562297224998474\n",
      "Spearman correlation A375: 0.7843151215514158\n",
      "Pseudo-Accuracy A375: 0.8031436120191662\n",
      "Pearson correlation HT29: 0.8536646962165833\n",
      "Spearman correlation HT29: 0.7853393586040223\n",
      "Pseudo-Accuracy HT29: 0.8038661578345652\n",
      "Pearson correlation A375 to HT29: 0.7100074887275696\n",
      "Pseudo accuracy A375 to HT29: 0.7132157464212678\n",
      "Pearson correlation HT29 to A375: 0.5610650777816772\n",
      "Pseudo accuracy HT29 to A375: 0.7166666666666666\n",
      "Split 3: Epoch=1/2000, r2_pc3=-2.9008, pearson_pc3=0.3067, MSE_pc3=2.5521, r2_a375=-3.2376, pearson_a375=0.3205, MSE_a375=3.0601, r2_ht29=-3.7735, pearson_ht29=0.3325, MSE_ht29=3.3265, MI Loss=2.0869, Prior Loss=0.6638, loss=10026.4639\n",
      "Split 3: Epoch=251/2000, r2_pc3=0.5961, pearson_pc3=0.8374, MSE_pc3=0.8236, r2_a375=0.5692, pearson_a375=0.8000, MSE_a375=0.9976, r2_ht29=0.5197, pearson_ht29=0.7919, MSE_ht29=1.0206, MI Loss=-0.6510, Prior Loss=0.0000, loss=2934.2998\n",
      "Split 3: Epoch=501/2000, r2_pc3=0.6234, pearson_pc3=0.8479, MSE_pc3=0.7607, r2_a375=0.6121, pearson_a375=0.8406, MSE_a375=0.9056, r2_ht29=0.5939, pearson_ht29=0.7995, MSE_ht29=0.9097, MI Loss=-0.6622, Prior Loss=0.0000, loss=2649.2158\n",
      "Split 3: Epoch=751/2000, r2_pc3=0.6056, pearson_pc3=0.8513, MSE_pc3=0.7715, r2_a375=0.6684, pearson_a375=0.8826, MSE_a375=0.9491, r2_ht29=0.6505, pearson_ht29=0.8564, MSE_ht29=0.8279, MI Loss=-0.6611, Prior Loss=0.0000, loss=2608.7107\n",
      "Split 3: Epoch=1001/2000, r2_pc3=0.6251, pearson_pc3=0.8495, MSE_pc3=0.7092, r2_a375=0.6645, pearson_a375=0.8886, MSE_a375=0.9261, r2_ht29=0.6156, pearson_ht29=0.8222, MSE_ht29=0.8606, MI Loss=-0.6835, Prior Loss=0.0000, loss=2547.8123\n",
      "Split 3: Epoch=1251/2000, r2_pc3=0.6659, pearson_pc3=0.8675, MSE_pc3=0.7096, r2_a375=0.6863, pearson_a375=0.8833, MSE_a375=0.8490, r2_ht29=0.6673, pearson_ht29=0.8562, MSE_ht29=0.8216, MI Loss=-0.6856, Prior Loss=0.0000, loss=2421.4053\n",
      "Split 3: Epoch=1501/2000, r2_pc3=0.6611, pearson_pc3=0.8486, MSE_pc3=0.6808, r2_a375=0.7186, pearson_a375=0.8797, MSE_a375=0.7890, r2_ht29=0.6868, pearson_ht29=0.8558, MSE_ht29=0.8047, MI Loss=-0.7013, Prior Loss=0.0000, loss=2315.9458\n",
      "Split 3: Epoch=1751/2000, r2_pc3=0.6631, pearson_pc3=0.8649, MSE_pc3=0.6709, r2_a375=0.6822, pearson_a375=0.8834, MSE_a375=0.8528, r2_ht29=0.6592, pearson_ht29=0.8591, MSE_ht29=0.7662, MI Loss=-0.6787, Prior Loss=0.0000, loss=2324.1411\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split 3: Epoch=2000/2000, r2_pc3=0.6595, pearson_pc3=0.8729, MSE_pc3=0.6895, r2_a375=0.7090, pearson_a375=0.8927, MSE_a375=0.8268, r2_ht29=0.6234, pearson_ht29=0.8446, MSE_ht29=0.8090, MI Loss=-0.6782, Prior Loss=0.0000, loss=2353.3877\n",
      "Pearson correlation PC3: 0.8313519358634949\n",
      "Spearman correlation PC3: 0.7940753230928228\n",
      "Pseudo-Accuracy PC3: 0.811218909021734\n",
      "Pearson of direct A375 to PC3 translation: 0.4941678047180176\n",
      "Pearson correlation A375 to PC3: 0.748081386089325\n",
      "Pseudo accuracy A375 to PC3: 0.7240683935469212\n",
      "Pearson correlation PC3 to A375: 0.5450800061225891\n",
      "Pseudo accuracy PC3 to A375: 0.7494603499204726\n",
      "Pearson of direct HT29 to PC3 translation: 0.4745509922504425\n",
      "Pearson correlation HT29 to PC3: 0.5385561585426331\n",
      "Pseudo accuracy HT29 to PC3: 0.7255169279709156\n",
      "Pearson correlation PC3 to HT29: 0.6089714765548706\n",
      "Pseudo accuracy PC3 to HT29: 0.7645989547830039\n",
      "Pearson correlation A375: 0.8639929294586182\n",
      "Spearman correlation A375: 0.8037635824103981\n",
      "Pseudo-Accuracy A375: 0.810045356824498\n",
      "Pearson correlation HT29: 0.8455541133880615\n",
      "Spearman correlation HT29: 0.7858499735403329\n",
      "Pseudo-Accuracy HT29: 0.7999971986441438\n",
      "Pearson correlation A375 to HT29: 0.7937788367271423\n",
      "Pseudo accuracy A375 to HT29: 0.7446574642126789\n",
      "Pearson correlation HT29 to A375: 0.5791253447532654\n",
      "Pseudo accuracy HT29 to A375: 0.7405930470347648\n",
      "Split 4: Epoch=1/2000, r2_pc3=-2.7420, pearson_pc3=0.3130, MSE_pc3=2.4558, r2_a375=-4.3388, pearson_a375=0.3137, MSE_a375=3.6523, r2_ht29=-2.6579, pearson_ht29=0.3431, MSE_ht29=2.6530, MI Loss=2.1996, Prior Loss=0.6336, loss=9756.9199\n",
      "Split 4: Epoch=251/2000, r2_pc3=0.5474, pearson_pc3=0.8182, MSE_pc3=0.8242, r2_a375=0.5496, pearson_a375=0.8544, MSE_a375=1.1039, r2_ht29=0.5532, pearson_ht29=0.8112, MSE_ht29=0.9188, MI Loss=-0.6555, Prior Loss=0.0000, loss=2931.1816\n",
      "Split 4: Epoch=501/2000, r2_pc3=0.5864, pearson_pc3=0.8439, MSE_pc3=0.7757, r2_a375=0.6555, pearson_a375=0.8612, MSE_a375=0.8764, r2_ht29=0.5819, pearson_ht29=0.8556, MSE_ht29=1.0104, MI Loss=-0.6664, Prior Loss=0.0000, loss=2723.2102\n",
      "Split 4: Epoch=751/2000, r2_pc3=0.6396, pearson_pc3=0.8519, MSE_pc3=0.7669, r2_a375=0.6262, pearson_a375=0.8460, MSE_a375=0.8885, r2_ht29=0.6390, pearson_ht29=0.8463, MSE_ht29=0.7991, MI Loss=-0.6770, Prior Loss=0.0000, loss=2511.5000\n",
      "Split 4: Epoch=1001/2000, r2_pc3=0.6576, pearson_pc3=0.8615, MSE_pc3=0.7024, r2_a375=0.7022, pearson_a375=0.8631, MSE_a375=0.8017, r2_ht29=0.6357, pearson_ht29=0.8447, MSE_ht29=0.8513, MI Loss=-0.6952, Prior Loss=0.0000, loss=2402.5171\n",
      "Split 4: Epoch=1251/2000, r2_pc3=0.6378, pearson_pc3=0.8489, MSE_pc3=0.6962, r2_a375=0.6993, pearson_a375=0.8787, MSE_a375=0.8152, r2_ht29=0.6597, pearson_ht29=0.8395, MSE_ht29=0.7802, MI Loss=-0.6780, Prior Loss=0.0000, loss=2340.6890\n",
      "Split 4: Epoch=1501/2000, r2_pc3=0.6722, pearson_pc3=0.8490, MSE_pc3=0.6553, r2_a375=0.6971, pearson_a375=0.8740, MSE_a375=0.7957, r2_ht29=0.6895, pearson_ht29=0.8567, MSE_ht29=0.7102, MI Loss=-0.6814, Prior Loss=0.0000, loss=2201.2410\n",
      "Split 4: Epoch=1751/2000, r2_pc3=0.6817, pearson_pc3=0.8687, MSE_pc3=0.6742, r2_a375=0.7361, pearson_a375=0.8771, MSE_a375=0.7427, r2_ht29=0.6927, pearson_ht29=0.8569, MSE_ht29=0.7629, MI Loss=-0.6982, Prior Loss=0.0000, loss=2215.5002\n",
      "Split 4: Epoch=2000/2000, r2_pc3=0.6857, pearson_pc3=0.8628, MSE_pc3=0.6667, r2_a375=0.7055, pearson_a375=0.8704, MSE_a375=0.7702, r2_ht29=0.6928, pearson_ht29=0.8454, MSE_ht29=0.7592, MI Loss=-0.6786, Prior Loss=0.0000, loss=2230.0757\n",
      "Pearson correlation PC3: 0.8161377310752869\n",
      "Spearman correlation PC3: 0.7935168233573777\n",
      "Pseudo-Accuracy PC3: 0.8085581917223158\n",
      "Pearson of direct A375 to PC3 translation: 0.4388618469238281\n",
      "Pearson correlation A375 to PC3: 0.605840265750885\n",
      "Pseudo accuracy A375 to PC3: 0.6972903885480572\n",
      "Pearson correlation PC3 to A375: 0.4383711814880371\n",
      "Pseudo accuracy PC3 to A375: 0.7105316973415132\n",
      "Pearson of direct HT29 to PC3 translation: 0.43677911162376404\n",
      "Pearson correlation HT29 to PC3: 0.4228323996067047\n",
      "Pseudo accuracy HT29 to PC3: 0.6979038854805726\n",
      "Pearson correlation PC3 to HT29: 0.45847228169441223\n",
      "Pseudo accuracy PC3 to HT29: 0.7071063394683027\n",
      "Pearson correlation A375: 0.8278543949127197\n",
      "Spearman correlation A375: 0.7658740851115465\n",
      "Pseudo-Accuracy A375: 0.7920829681565879\n",
      "Pearson correlation HT29: 0.8315739631652832\n",
      "Spearman correlation HT29: 0.7684714658393104\n",
      "Pseudo-Accuracy HT29: 0.7944470662262074\n",
      "Pearson correlation A375 to HT29: 0.7138105630874634\n",
      "Pseudo accuracy A375 to HT29: 0.7072852760736196\n",
      "Pearson correlation HT29 to A375: 0.5534946322441101\n",
      "Pseudo accuracy HT29 to A375: 0.710097137014315\n",
      "Split 5: Epoch=1/2000, r2_pc3=-3.0054, pearson_pc3=0.2952, MSE_pc3=2.6559, r2_a375=-5.2509, pearson_a375=0.3292, MSE_a375=4.1997, r2_ht29=-2.5944, pearson_ht29=0.3043, MSE_ht29=2.4466, MI Loss=1.8843, Prior Loss=0.7291, loss=10274.5645\n",
      "Split 5: Epoch=251/2000, r2_pc3=0.5366, pearson_pc3=0.8122, MSE_pc3=0.8484, r2_a375=0.5799, pearson_a375=0.8367, MSE_a375=1.0147, r2_ht29=0.5399, pearson_ht29=0.8397, MSE_ht29=1.0704, MI Loss=-0.6599, Prior Loss=0.0000, loss=3020.3608\n",
      "Split 5: Epoch=501/2000, r2_pc3=0.5578, pearson_pc3=0.8461, MSE_pc3=0.8187, r2_a375=0.6075, pearson_a375=0.8650, MSE_a375=0.9694, r2_ht29=0.6203, pearson_ht29=0.8344, MSE_ht29=0.8610, MI Loss=-0.6659, Prior Loss=0.0000, loss=2725.1077\n",
      "Split 5: Epoch=751/2000, r2_pc3=0.5885, pearson_pc3=0.8548, MSE_pc3=0.8129, r2_a375=0.6773, pearson_a375=0.8782, MSE_a375=0.9365, r2_ht29=0.6248, pearson_ht29=0.8659, MSE_ht29=0.9360, MI Loss=-0.6689, Prior Loss=0.0000, loss=2734.7966\n",
      "Split 5: Epoch=1001/2000, r2_pc3=0.6197, pearson_pc3=0.8634, MSE_pc3=0.7602, r2_a375=0.6968, pearson_a375=0.8717, MSE_a375=0.8205, r2_ht29=0.6632, pearson_ht29=0.8866, MSE_ht29=0.7957, MI Loss=-0.6826, Prior Loss=0.0000, loss=2426.3350\n",
      "Split 5: Epoch=1251/2000, r2_pc3=0.6420, pearson_pc3=0.8551, MSE_pc3=0.6832, r2_a375=0.6936, pearson_a375=0.8796, MSE_a375=0.8452, r2_ht29=0.6556, pearson_ht29=0.8449, MSE_ht29=0.8224, MI Loss=-0.6517, Prior Loss=0.0000, loss=2394.1472\n",
      "Split 5: Epoch=1501/2000, r2_pc3=0.6606, pearson_pc3=0.8632, MSE_pc3=0.6859, r2_a375=0.7084, pearson_a375=0.8830, MSE_a375=0.7810, r2_ht29=0.6412, pearson_ht29=0.8631, MSE_ht29=0.9135, MI Loss=-0.6807, Prior Loss=0.0000, loss=2417.6265\n",
      "Split 5: Epoch=1751/2000, r2_pc3=0.6636, pearson_pc3=0.8666, MSE_pc3=0.6992, r2_a375=0.6883, pearson_a375=0.8613, MSE_a375=0.7924, r2_ht29=0.6027, pearson_ht29=0.8885, MSE_ht29=0.9736, MI Loss=-0.6841, Prior Loss=0.0000, loss=2497.1370\n",
      "Split 5: Epoch=2000/2000, r2_pc3=0.6684, pearson_pc3=0.8541, MSE_pc3=0.6397, r2_a375=0.7026, pearson_a375=0.8757, MSE_a375=0.7774, r2_ht29=0.6812, pearson_ht29=0.8621, MSE_ht29=0.8024, MI Loss=-0.6948, Prior Loss=0.0000, loss=2250.9102\n",
      "Pearson correlation PC3: 0.8482344150543213\n",
      "Spearman correlation PC3: 0.7973838266354386\n",
      "Pseudo-Accuracy PC3: 0.8136862208250942\n",
      "Pearson of direct A375 to PC3 translation: 0.6276248693466187\n",
      "Pearson correlation A375 to PC3: 0.7729102969169617\n",
      "Pseudo accuracy A375 to PC3: 0.7551855097867367\n",
      "Pearson correlation PC3 to A375: 0.6242673993110657\n",
      "Pseudo accuracy PC3 to A375: 0.7490505404615834\n",
      "Pearson of direct HT29 to PC3 translation: 0.6274711489677429\n",
      "Pearson correlation HT29 to PC3: 0.5852484107017517\n",
      "Pseudo accuracy HT29 to PC3: 0.7605901256208005\n",
      "Pearson correlation PC3 to HT29: 0.641341507434845\n",
      "Pseudo accuracy PC3 to HT29: 0.7785203038270522\n",
      "Pearson correlation A375: 0.8427232503890991\n",
      "Spearman correlation A375: 0.7753724537868417\n",
      "Pseudo-Accuracy A375: 0.7945959763444426\n",
      "Pearson correlation HT29: 0.8611692786216736\n",
      "Spearman correlation HT29: 0.8110465669788001\n",
      "Pseudo-Accuracy HT29: 0.8206706973711508\n",
      "Pearson correlation A375 to HT29: 0.7492952346801758\n",
      "Pseudo accuracy A375 to HT29: 0.725715746421268\n",
      "Pearson correlation HT29 to A375: 0.5555744171142578\n",
      "Pseudo accuracy HT29 to A375: 0.7151840490797545\n",
      "Split 6: Epoch=1/2000, r2_pc3=-2.3560, pearson_pc3=0.3047, MSE_pc3=2.2570, r2_a375=-3.0928, pearson_a375=0.2669, MSE_a375=2.7825, r2_ht29=-2.8565, pearson_ht29=0.3690, MSE_ht29=2.8370, MI Loss=2.2676, Prior Loss=0.6812, loss=8898.9941\n",
      "Split 6: Epoch=251/2000, r2_pc3=0.5612, pearson_pc3=0.8130, MSE_pc3=0.8255, r2_a375=0.6165, pearson_a375=0.8417, MSE_a375=0.9449, r2_ht29=0.6231, pearson_ht29=0.8450, MSE_ht29=0.8579, MI Loss=-0.6613, Prior Loss=0.0000, loss=2724.8601\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split 6: Epoch=501/2000, r2_pc3=0.6021, pearson_pc3=0.8383, MSE_pc3=0.8076, r2_a375=0.6798, pearson_a375=0.8611, MSE_a375=0.8759, r2_ht29=0.6341, pearson_ht29=0.8497, MSE_ht29=0.9292, MI Loss=-0.6644, Prior Loss=0.0000, loss=2681.3425\n",
      "Split 6: Epoch=751/2000, r2_pc3=0.6355, pearson_pc3=0.8542, MSE_pc3=0.7700, r2_a375=0.6876, pearson_a375=0.8703, MSE_a375=0.8717, r2_ht29=0.6843, pearson_ht29=0.8544, MSE_ht29=0.7519, MI Loss=-0.6743, Prior Loss=0.0000, loss=2453.6443\n",
      "Split 6: Epoch=1001/2000, r2_pc3=0.6292, pearson_pc3=0.8553, MSE_pc3=0.7240, r2_a375=0.7163, pearson_a375=0.8864, MSE_a375=0.8339, r2_ht29=0.6806, pearson_ht29=0.8700, MSE_ht29=0.7294, MI Loss=-0.6920, Prior Loss=0.0000, loss=2337.7126\n",
      "Split 6: Epoch=1251/2000, r2_pc3=0.6602, pearson_pc3=0.8662, MSE_pc3=0.6990, r2_a375=0.6981, pearson_a375=0.8685, MSE_a375=0.7859, r2_ht29=0.6909, pearson_ht29=0.8617, MSE_ht29=0.7846, MI Loss=-0.6957, Prior Loss=0.0000, loss=2311.7981\n",
      "Split 6: Epoch=1501/2000, r2_pc3=0.6611, pearson_pc3=0.8503, MSE_pc3=0.6806, r2_a375=0.7164, pearson_a375=0.8705, MSE_a375=0.7847, r2_ht29=0.6775, pearson_ht29=0.8809, MSE_ht29=0.8574, MI Loss=-0.6878, Prior Loss=0.0000, loss=2360.4985\n",
      "Split 6: Epoch=1751/2000, r2_pc3=0.6699, pearson_pc3=0.8778, MSE_pc3=0.7084, r2_a375=0.7298, pearson_a375=0.8760, MSE_a375=0.7568, r2_ht29=0.6541, pearson_ht29=0.8816, MSE_ht29=0.9494, MI Loss=-0.7054, Prior Loss=0.0000, loss=2442.4712\n",
      "Split 6: Epoch=2000/2000, r2_pc3=0.6677, pearson_pc3=0.8618, MSE_pc3=0.6771, r2_a375=0.7213, pearson_a375=0.8693, MSE_a375=0.7640, r2_ht29=0.6750, pearson_ht29=0.8846, MSE_ht29=0.8264, MI Loss=-0.6959, Prior Loss=0.0000, loss=2295.6072\n",
      "Pearson correlation PC3: 0.8206081390380859\n",
      "Spearman correlation PC3: 0.7741412211493031\n",
      "Pseudo-Accuracy PC3: 0.8002726653033401\n",
      "Pearson of direct A375 to PC3 translation: 0.41284602880477905\n",
      "Pearson correlation A375 to PC3: 0.6237788796424866\n",
      "Pseudo accuracy A375 to PC3: 0.6827539195637354\n",
      "Pearson correlation PC3 to A375: 0.5224406123161316\n",
      "Pseudo accuracy PC3 to A375: 0.7045671438309475\n",
      "Pearson of direct HT29 to PC3 translation: 0.4249288737773895\n",
      "Pearson correlation HT29 to PC3: 0.44722992181777954\n",
      "Pseudo accuracy HT29 to PC3: 0.6811179277436946\n",
      "Pearson correlation PC3 to HT29: 0.513923704624176\n",
      "Pseudo accuracy PC3 to HT29: 0.6975460122699387\n",
      "Pearson correlation A375: 0.8365975618362427\n",
      "Spearman correlation A375: 0.7883010088836009\n",
      "Pseudo-Accuracy A375: 0.806513671135348\n",
      "Pearson correlation HT29: 0.847429633140564\n",
      "Spearman correlation HT29: 0.7933967943438175\n",
      "Pseudo-Accuracy HT29: 0.8129897750511248\n",
      "Pearson correlation A375 to HT29: 0.6937752366065979\n",
      "Pseudo accuracy A375 to HT29: 0.7156441717791411\n",
      "Pearson correlation HT29 to A375: 0.5203040838241577\n",
      "Pseudo accuracy HT29 to A375: 0.7028118609406953\n",
      "Split 7: Epoch=1/2000, r2_pc3=-2.8576, pearson_pc3=0.2958, MSE_pc3=2.5092, r2_a375=-3.7758, pearson_a375=0.2933, MSE_a375=3.2308, r2_ht29=-4.0307, pearson_ht29=0.3922, MSE_ht29=3.8966, MI Loss=2.1150, Prior Loss=0.6363, loss=10872.8438\n",
      "Split 7: Epoch=251/2000, r2_pc3=0.5752, pearson_pc3=0.8174, MSE_pc3=0.8349, r2_a375=0.6269, pearson_a375=0.8477, MSE_a375=0.9423, r2_ht29=0.5825, pearson_ht29=0.8154, MSE_ht29=0.9988, MI Loss=-0.6584, Prior Loss=0.0000, loss=2872.4028\n",
      "Split 7: Epoch=501/2000, r2_pc3=0.5853, pearson_pc3=0.8312, MSE_pc3=0.7981, r2_a375=0.6114, pearson_a375=0.8521, MSE_a375=1.0267, r2_ht29=0.5433, pearson_ht29=0.8072, MSE_ht29=1.0253, MI Loss=-0.6602, Prior Loss=0.0000, loss=2915.8560\n",
      "Split 7: Epoch=751/2000, r2_pc3=0.6288, pearson_pc3=0.8424, MSE_pc3=0.7485, r2_a375=0.6583, pearson_a375=0.8373, MSE_a375=0.8840, r2_ht29=0.6251, pearson_ht29=0.8716, MSE_ht29=1.0150, MI Loss=-0.6632, Prior Loss=0.0000, loss=2702.3525\n",
      "Split 7: Epoch=1001/2000, r2_pc3=0.6356, pearson_pc3=0.8606, MSE_pc3=0.7135, r2_a375=0.6677, pearson_a375=0.8779, MSE_a375=0.9267, r2_ht29=0.6420, pearson_ht29=0.8344, MSE_ht29=0.8769, MI Loss=-0.6725, Prior Loss=0.0000, loss=2570.6299\n",
      "Split 7: Epoch=1251/2000, r2_pc3=0.6699, pearson_pc3=0.8721, MSE_pc3=0.7149, r2_a375=0.6699, pearson_a375=0.8783, MSE_a375=0.9176, r2_ht29=0.6640, pearson_ht29=0.8632, MSE_ht29=0.8112, MI Loss=-0.6819, Prior Loss=0.0000, loss=2488.4707\n",
      "Split 7: Epoch=1501/2000, r2_pc3=0.6564, pearson_pc3=0.8620, MSE_pc3=0.6895, r2_a375=0.6891, pearson_a375=0.8711, MSE_a375=0.8764, r2_ht29=0.6531, pearson_ht29=0.8239, MSE_ht29=0.8290, MI Loss=-0.6877, Prior Loss=0.0000, loss=2435.1892\n",
      "Split 7: Epoch=1751/2000, r2_pc3=0.6662, pearson_pc3=0.8663, MSE_pc3=0.6930, r2_a375=0.7131, pearson_a375=0.8974, MSE_a375=0.8600, r2_ht29=0.6128, pearson_ht29=0.8402, MSE_ht29=0.8810, MI Loss=-0.6912, Prior Loss=0.0000, loss=2463.3125\n",
      "Split 7: Epoch=2000/2000, r2_pc3=0.6639, pearson_pc3=0.8599, MSE_pc3=0.6651, r2_a375=0.7239, pearson_a375=0.8847, MSE_a375=0.7982, r2_ht29=0.6637, pearson_ht29=0.8511, MSE_ht29=0.8168, MI Loss=-0.6958, Prior Loss=0.0000, loss=2311.7495\n",
      "Pearson correlation PC3: 0.8122512698173523\n",
      "Spearman correlation PC3: 0.7791608309747007\n",
      "Pseudo-Accuracy PC3: 0.801973859695919\n",
      "Pearson of direct A375 to PC3 translation: 0.45338431000709534\n",
      "Pearson correlation A375 to PC3: 0.6713624596595764\n",
      "Pseudo accuracy A375 to PC3: 0.711377579475739\n",
      "Pearson correlation PC3 to A375: 0.48240774869918823\n",
      "Pseudo accuracy PC3 to A375: 0.7025469418107456\n",
      "Pearson of direct HT29 to PC3 translation: 0.45094799995422363\n",
      "Pearson correlation HT29 to PC3: 0.43546146154403687\n",
      "Pseudo accuracy HT29 to PC3: 0.7009202453987728\n",
      "Pearson correlation PC3 to HT29: 0.493642121553421\n",
      "Pseudo accuracy PC3 to HT29: 0.707566462167689\n",
      "Pearson correlation A375: 0.8249807357788086\n",
      "Spearman correlation A375: 0.769733352286129\n",
      "Pseudo-Accuracy A375: 0.7958034505602121\n",
      "Pearson correlation HT29: 0.8526087999343872\n",
      "Spearman correlation HT29: 0.7970572156089186\n",
      "Pseudo-Accuracy HT29: 0.8132087748652166\n",
      "Pearson correlation A375 to HT29: 0.6737468242645264\n",
      "Pseudo accuracy A375 to HT29: 0.7012781186094068\n",
      "Pearson correlation HT29 to A375: 0.4546765089035034\n",
      "Pseudo accuracy HT29 to A375: 0.6834867075664621\n",
      "Split 8: Epoch=1/2000, r2_pc3=-2.6610, pearson_pc3=0.3097, MSE_pc3=2.4601, r2_a375=-4.1665, pearson_a375=0.2859, MSE_a375=3.5157, r2_ht29=-3.4979, pearson_ht29=0.3155, MSE_ht29=3.0692, MI Loss=2.0375, Prior Loss=0.6799, loss=10153.7529\n",
      "Split 8: Epoch=251/2000, r2_pc3=0.5538, pearson_pc3=0.8336, MSE_pc3=0.8754, r2_a375=0.5765, pearson_a375=0.8061, MSE_a375=1.0639, r2_ht29=0.5748, pearson_ht29=0.8274, MSE_ht29=0.9562, MI Loss=-0.6478, Prior Loss=0.0000, loss=2985.9272\n",
      "Split 8: Epoch=501/2000, r2_pc3=0.6040, pearson_pc3=0.8495, MSE_pc3=0.7710, r2_a375=0.6447, pearson_a375=0.8699, MSE_a375=0.9722, r2_ht29=0.6448, pearson_ht29=0.8694, MSE_ht29=0.9416, MI Loss=-0.6609, Prior Loss=0.0000, loss=2745.6438\n",
      "Split 8: Epoch=751/2000, r2_pc3=0.6272, pearson_pc3=0.8443, MSE_pc3=0.7135, r2_a375=0.7027, pearson_a375=0.8661, MSE_a375=0.8234, r2_ht29=0.6095, pearson_ht29=0.8658, MSE_ht29=1.0575, MI Loss=-0.6740, Prior Loss=0.0000, loss=2649.3218\n",
      "Split 8: Epoch=1001/2000, r2_pc3=0.6489, pearson_pc3=0.8518, MSE_pc3=0.7028, r2_a375=0.7018, pearson_a375=0.8676, MSE_a375=0.8143, r2_ht29=0.6111, pearson_ht29=0.8369, MSE_ht29=0.9129, MI Loss=-0.6661, Prior Loss=0.0000, loss=2480.8542\n",
      "Split 8: Epoch=1251/2000, r2_pc3=0.6647, pearson_pc3=0.8577, MSE_pc3=0.6734, r2_a375=0.7100, pearson_a375=0.8728, MSE_a375=0.7672, r2_ht29=0.6955, pearson_ht29=0.8533, MSE_ht29=0.7230, MI Loss=-0.7072, Prior Loss=0.0000, loss=2214.3586\n",
      "Split 8: Epoch=1501/2000, r2_pc3=0.6747, pearson_pc3=0.8767, MSE_pc3=0.6866, r2_a375=0.7011, pearson_a375=0.8891, MSE_a375=0.9156, r2_ht29=0.6990, pearson_ht29=0.8855, MSE_ht29=0.8044, MI Loss=-0.6846, Prior Loss=0.0000, loss=2441.2874\n",
      "Split 8: Epoch=1751/2000, r2_pc3=0.6759, pearson_pc3=0.8761, MSE_pc3=0.6789, r2_a375=0.7057, pearson_a375=0.8641, MSE_a375=0.8019, r2_ht29=0.6929, pearson_ht29=0.8692, MSE_ht29=0.8055, MI Loss=-0.6778, Prior Loss=0.0000, loss=2320.5063\n",
      "Split 8: Epoch=2000/2000, r2_pc3=0.6656, pearson_pc3=0.8609, MSE_pc3=0.6629, r2_a375=0.7268, pearson_a375=0.8884, MSE_a375=0.7461, r2_ht29=0.6738, pearson_ht29=0.8739, MSE_ht29=0.8319, MI Loss=-0.7462, Prior Loss=0.0000, loss=2269.2717\n",
      "Pearson correlation PC3: 0.8087067604064941\n",
      "Spearman correlation PC3: 0.7709853350375871\n",
      "Pseudo-Accuracy PC3: 0.7955196133110244\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson of direct A375 to PC3 translation: 0.4160148501396179\n",
      "Pearson correlation A375 to PC3: 0.6158961653709412\n",
      "Pseudo accuracy A375 to PC3: 0.7068447010706123\n",
      "Pearson correlation PC3 to A375: 0.42419320344924927\n",
      "Pseudo accuracy PC3 to A375: 0.7028750150366896\n",
      "Pearson of direct HT29 to PC3 translation: 0.4505714774131775\n",
      "Pearson correlation HT29 to PC3: 0.46902546286582947\n",
      "Pseudo accuracy HT29 to PC3: 0.7092505713942018\n",
      "Pearson correlation PC3 to HT29: 0.5666565895080566\n",
      "Pseudo accuracy PC3 to HT29: 0.7340310357271743\n",
      "Pearson correlation A375: 0.8229271769523621\n",
      "Spearman correlation A375: 0.7812864776180969\n",
      "Pseudo-Accuracy A375: 0.8028525368322063\n",
      "Pearson correlation HT29: 0.8522698879241943\n",
      "Spearman correlation HT29: 0.8030321716131084\n",
      "Pseudo-Accuracy HT29: 0.8183018533726228\n",
      "Pearson correlation A375 to HT29: 0.7421985864639282\n",
      "Pseudo accuracy A375 to HT29: 0.7502556237218814\n",
      "Pearson correlation HT29 to A375: 0.5328363180160522\n",
      "Pseudo accuracy HT29 to A375: 0.7197597137014314\n",
      "Split 9: Epoch=1/2000, r2_pc3=-2.8644, pearson_pc3=0.2953, MSE_pc3=2.5153, r2_a375=-3.4840, pearson_a375=0.3094, MSE_a375=3.1501, r2_ht29=-3.0688, pearson_ht29=0.3526, MSE_ht29=2.9691, MI Loss=1.8628, Prior Loss=0.6894, loss=9639.8809\n",
      "Split 9: Epoch=251/2000, r2_pc3=0.5269, pearson_pc3=0.8329, MSE_pc3=0.8708, r2_a375=0.5932, pearson_a375=0.8361, MSE_a375=1.0473, r2_ht29=0.6309, pearson_ht29=0.8480, MSE_ht29=0.9053, MI Loss=-0.6535, Prior Loss=0.0000, loss=2924.1677\n",
      "Split 9: Epoch=501/2000, r2_pc3=0.5600, pearson_pc3=0.8395, MSE_pc3=0.7988, r2_a375=0.6194, pearson_a375=0.8630, MSE_a375=0.9928, r2_ht29=0.6166, pearson_ht29=0.8502, MSE_ht29=0.9629, MI Loss=-0.6687, Prior Loss=0.0000, loss=2815.8210\n",
      "Split 9: Epoch=751/2000, r2_pc3=0.6183, pearson_pc3=0.8620, MSE_pc3=0.7511, r2_a375=0.6663, pearson_a375=0.8700, MSE_a375=0.8951, r2_ht29=0.6774, pearson_ht29=0.8795, MSE_ht29=0.8672, MI Loss=-0.6903, Prior Loss=0.0000, loss=2568.9087\n",
      "Split 9: Epoch=1001/2000, r2_pc3=0.6383, pearson_pc3=0.8624, MSE_pc3=0.7265, r2_a375=0.6870, pearson_a375=0.8855, MSE_a375=0.9133, r2_ht29=0.6871, pearson_ht29=0.8604, MSE_ht29=0.7968, MI Loss=-0.7070, Prior Loss=0.0000, loss=2485.9922\n",
      "Split 9: Epoch=1251/2000, r2_pc3=0.6608, pearson_pc3=0.8510, MSE_pc3=0.6919, r2_a375=0.6793, pearson_a375=0.8524, MSE_a375=0.8054, r2_ht29=0.6308, pearson_ht29=0.8853, MSE_ht29=1.0457, MI Loss=-0.6959, Prior Loss=0.0000, loss=2582.3530\n",
      "Split 9: Epoch=1501/2000, r2_pc3=0.6660, pearson_pc3=0.8460, MSE_pc3=0.6571, r2_a375=0.7264, pearson_a375=0.8920, MSE_a375=0.7860, r2_ht29=0.7114, pearson_ht29=0.8755, MSE_ht29=0.7663, MI Loss=-0.6894, Prior Loss=0.0000, loss=2250.0671\n",
      "Split 9: Epoch=1751/2000, r2_pc3=0.6690, pearson_pc3=0.8703, MSE_pc3=0.6746, r2_a375=0.7166, pearson_a375=0.8831, MSE_a375=0.7854, r2_ht29=0.6885, pearson_ht29=0.8640, MSE_ht29=0.7898, MI Loss=-0.6957, Prior Loss=0.0000, loss=2281.8477\n",
      "Split 9: Epoch=2000/2000, r2_pc3=0.6661, pearson_pc3=0.8672, MSE_pc3=0.6795, r2_a375=0.7215, pearson_a375=0.8876, MSE_a375=0.7722, r2_ht29=0.6868, pearson_ht29=0.8633, MSE_ht29=0.7798, MI Loss=-0.6882, Prior Loss=0.0000, loss=2260.2209\n",
      "Pearson correlation PC3: 0.8403966426849365\n",
      "Spearman correlation PC3: 0.7892185043166711\n",
      "Pseudo-Accuracy PC3: 0.8059511078591396\n",
      "Pearson of direct A375 to PC3 translation: 0.4453020691871643\n",
      "Pearson correlation A375 to PC3: 0.6400906443595886\n",
      "Pseudo accuracy A375 to PC3: 0.7027735173824131\n",
      "Pearson correlation PC3 to A375: 0.5312076210975647\n",
      "Pseudo accuracy PC3 to A375: 0.7021344580777096\n",
      "Pearson of direct HT29 to PC3 translation: 0.43419843912124634\n",
      "Pearson correlation HT29 to PC3: 0.44126877188682556\n",
      "Pseudo accuracy HT29 to PC3: 0.6960633946830266\n",
      "Pearson correlation PC3 to HT29: 0.487213671207428\n",
      "Pseudo accuracy PC3 to HT29: 0.6982361963190185\n",
      "Pearson correlation A375: 0.8296940922737122\n",
      "Spearman correlation A375: 0.7845337611882023\n",
      "Pseudo-Accuracy A375: 0.8033501744255984\n",
      "Pearson correlation HT29: 0.8352234363555908\n",
      "Spearman correlation HT29: 0.7892829078345284\n",
      "Pseudo-Accuracy HT29: 0.806521245171552\n",
      "Pearson correlation A375 to HT29: 0.6572285890579224\n",
      "Pseudo accuracy A375 to HT29: 0.6811094069529652\n",
      "Pearson correlation HT29 to A375: 0.42411214113235474\n",
      "Pseudo accuracy HT29 to A375: 0.6700153374233129\n",
      "Split 10: Epoch=1/2000, r2_pc3=-2.8037, pearson_pc3=0.3269, MSE_pc3=2.5912, r2_a375=-3.8274, pearson_a375=0.3539, MSE_a375=3.5500, r2_ht29=-2.8907, pearson_ht29=0.4006, MSE_ht29=2.9596, MI Loss=2.2695, Prior Loss=0.6713, loss=10278.3184\n",
      "Split 10: Epoch=251/2000, r2_pc3=0.5509, pearson_pc3=0.8287, MSE_pc3=0.8408, r2_a375=0.5974, pearson_a375=0.8288, MSE_a375=0.9793, r2_ht29=0.5448, pearson_ht29=0.8108, MSE_ht29=0.9861, MI Loss=-0.6504, Prior Loss=0.0000, loss=2896.1636\n",
      "Split 10: Epoch=501/2000, r2_pc3=0.5981, pearson_pc3=0.8442, MSE_pc3=0.7708, r2_a375=0.6320, pearson_a375=0.8543, MSE_a375=0.9736, r2_ht29=0.6696, pearson_ht29=0.8505, MSE_ht29=0.8245, MI Loss=-0.6722, Prior Loss=0.0000, loss=2634.3118\n",
      "Split 10: Epoch=751/2000, r2_pc3=0.6340, pearson_pc3=0.8496, MSE_pc3=0.7368, r2_a375=0.6986, pearson_a375=0.8761, MSE_a375=0.8536, r2_ht29=0.6631, pearson_ht29=0.8385, MSE_ht29=0.8556, MI Loss=-0.6846, Prior Loss=0.0000, loss=2504.0786\n",
      "Split 10: Epoch=1001/2000, r2_pc3=0.6313, pearson_pc3=0.8722, MSE_pc3=0.7669, r2_a375=0.6745, pearson_a375=0.8507, MSE_a375=0.8427, r2_ht29=0.7168, pearson_ht29=0.8661, MSE_ht29=0.7407, MI Loss=-0.6872, Prior Loss=0.0000, loss=2402.5278\n",
      "Split 10: Epoch=1251/2000, r2_pc3=0.6736, pearson_pc3=0.8636, MSE_pc3=0.6936, r2_a375=0.6921, pearson_a375=0.8593, MSE_a375=0.8037, r2_ht29=0.6567, pearson_ht29=0.8563, MSE_ht29=0.8141, MI Loss=-0.6893, Prior Loss=0.0000, loss=2361.8047\n",
      "Split 10: Epoch=1501/2000, r2_pc3=0.6698, pearson_pc3=0.8507, MSE_pc3=0.6665, r2_a375=0.7157, pearson_a375=0.8853, MSE_a375=0.8204, r2_ht29=0.6733, pearson_ht29=0.8753, MSE_ht29=0.8367, MI Loss=-0.7133, Prior Loss=0.0000, loss=2362.8345\n",
      "Split 10: Epoch=1751/2000, r2_pc3=0.6691, pearson_pc3=0.8553, MSE_pc3=0.6718, r2_a375=0.7063, pearson_a375=0.8686, MSE_a375=0.7660, r2_ht29=0.6638, pearson_ht29=0.8847, MSE_ht29=0.8552, MI Loss=-0.7347, Prior Loss=0.0000, loss=2326.9927\n",
      "Split 10: Epoch=2000/2000, r2_pc3=0.6759, pearson_pc3=0.8747, MSE_pc3=0.6865, r2_a375=0.6818, pearson_a375=0.8565, MSE_a375=0.8268, r2_ht29=0.6841, pearson_ht29=0.8782, MSE_ht29=0.8150, MI Loss=-0.7145, Prior Loss=0.0000, loss=2355.1880\n",
      "Pearson correlation PC3: 0.8366516828536987\n",
      "Spearman correlation PC3: 0.7938388150050264\n",
      "Pseudo-Accuracy PC3: 0.8090237476740545\n",
      "Pearson of direct A375 to PC3 translation: 0.4736286997795105\n",
      "Pearson correlation A375 to PC3: 0.6409469246864319\n",
      "Pseudo accuracy A375 to PC3: 0.6896728016359918\n",
      "Pearson correlation PC3 to A375: 0.48303160071372986\n",
      "Pseudo accuracy PC3 to A375: 0.7054646671211089\n",
      "Pearson of direct HT29 to PC3 translation: 0.4850884675979614\n",
      "Pearson correlation HT29 to PC3: 0.46999576687812805\n",
      "Pseudo accuracy HT29 to PC3: 0.6998409452397183\n",
      "Pearson correlation PC3 to HT29: 0.4818861484527588\n",
      "Pseudo accuracy PC3 to HT29: 0.716144058168598\n",
      "Pearson correlation A375: 0.817374587059021\n",
      "Spearman correlation A375: 0.7741785977218903\n",
      "Pseudo-Accuracy A375: 0.7995169082125604\n",
      "Pearson correlation HT29: 0.8421979546546936\n",
      "Spearman correlation HT29: 0.777765953410325\n",
      "Pseudo-Accuracy HT29: 0.7989439544989775\n",
      "Pearson correlation A375 to HT29: 0.646115243434906\n",
      "Pseudo accuracy A375 to HT29: 0.6993609406952965\n",
      "Pearson correlation HT29 to A375: 0.444542795419693\n",
      "Pseudo accuracy HT29 to A375: 0.6901584867075664\n"
     ]
    }
   ],
   "source": [
    "valR2 = []\n",
    "valPear = []\n",
    "valMSE =[]\n",
    "valSpear = []\n",
    "valAccuracy = []\n",
    "\n",
    "\n",
    "valPearDirect = []\n",
    "valSpearDirect = []\n",
    "valAccDirect = []\n",
    "\n",
    "valR2_pc3 = []\n",
    "valPear_pc3 = []\n",
    "valMSE_pc3 =[]\n",
    "valSpear_pc3 = []\n",
    "valAccuracy_pc3 = []\n",
    "\n",
    "\n",
    "valR2_a375 = []\n",
    "valPear_a375 = []\n",
    "valMSE_a375 =[]\n",
    "valSpear_a375 = []\n",
    "valAccuracy_a375 = []\n",
    "\n",
    "valR2_ht29 = []\n",
    "valPear_ht29 = []\n",
    "valMSE_ht29 =[]\n",
    "valSpear_ht29 = []\n",
    "valAccuracy_ht29 = []\n",
    "\n",
    "# decoder_a375 = torch.load('../results/MI_results/models/AllData_Model/decoder_a375.pt')\n",
    "# decoder_ht29 = torch.load('../results/MI_results/models/AllData_Model/decoder_ht29.pt')\n",
    "# prior_d = torch.load('../results/MI_results/models/AllData_Model/priorDiscr.pt')\n",
    "# local_d = torch.load('../results/MI_results/models/AllData_Model/localDiscr.pt')\n",
    "# encoder_a375 = torch.load('../results/MI_results/models/AllData_Model/encoder_a375.pt')\n",
    "# encoder_ht29 = torch.load('../results/MI_results/models/AllData_Model/encoder_ht29.pt')\n",
    "\n",
    "# decoder_a375.eval()\n",
    "# decoder_ht29.eval()\n",
    "# encoder_a375.eval()\n",
    "# encoder_ht29.eval()\n",
    "\n",
    "# for param in decoder_a375.parameters():\n",
    "#     param.requires_grad = False\n",
    "# for param in decoder_ht29.parameters():\n",
    "#     param.requires_grad = False\n",
    "# for param in encoder_a375.parameters():\n",
    "#     param.requires_grad = False\n",
    "# for param in encoder_ht29.parameters():\n",
    "#     param.requires_grad = False\n",
    "        \n",
    "\n",
    "#model_mi.eval()\n",
    "#trainLoss = []\n",
    "for i in range(10):\n",
    "    \n",
    "    #decoder_a375 = torch.load('../results/MI_results/models/TwoEncoders_TwoDecoders_MI2/decoder_a375_%s.pt'%i)\n",
    "    #decoder_ht29 = torch.load('../results/MI_results/models/TwoEncoders_TwoDecoders_MI2/decoder_ht29_%s.pt'%i)\n",
    "    #prior_d = torch.load('../results/MI_results/models/TwoEncoders_TwoDecoders_MI2/priorDiscr_%s.pt'%i)\n",
    "    #local_d = torch.load('../results/MI_results/models/TwoEncoders_TwoDecoders_MI2/localDiscr_%s.pt'%i)\n",
    "    #encoder_a375 = torch.load('../results/MI_results/models/TwoEncoders_TwoDecoders_MI2/encoder_a375_%s.pt'%i)\n",
    "    #encoder_ht29 = torch.load('../results/MI_results/models/TwoEncoders_TwoDecoders_MI2/encoder_ht29_%s.pt'%i)\n",
    "    \n",
    "    decoder_a375 = Decoder(292,[384,640],gene_size,dropRate=0.2, activation=torch.nn.ELU()).to(device)\n",
    "    decoder_ht29 = Decoder(292,[384,640],gene_size,dropRate=0.2, activation=torch.nn.ELU()).to(device)\n",
    "    prior_d = PriorDiscriminator(292).to(device)\n",
    "    local_d = LocalDiscriminator(292,292).to(device)\n",
    "    encoder_a375 = SimpleEncoder(gene_size,[640,384],292,dropRate=0.1, activation=torch.nn.ELU()).to(device)\n",
    "    encoder_ht29 = SimpleEncoder(gene_size,[640,384],292,dropRate=0.1, activation=torch.nn.ELU()).to(device)\n",
    "    \n",
    "    #decoder_a375.eval()\n",
    "    #decoder_ht29.eval()\n",
    "    #encoder_a375.eval()\n",
    "    #encoder_ht29.eval()\n",
    "    \n",
    "    #for param in decoder_a375.parameters():\n",
    "    #    param.requires_grad = False\n",
    "    #for param in decoder_ht29.parameters():\n",
    "    #    param.requires_grad = False\n",
    "    #for param in encoder_a375.parameters():\n",
    "    #    param.requires_grad = False\n",
    "    #for param in encoder_ht29.parameters():\n",
    "    #    param.requires_grad = False\n",
    "    \n",
    "    \n",
    "    # Network\n",
    "    #decoder_pc3 = Decoder(292,[384,640],gene_size,dropRate=0.2, activation=torch.nn.ELU()).to(device)\n",
    "    decoder_pc3 = Decoder(292,[384,640],gene_size,dropRate=0.2, activation=torch.nn.ELU()).to(device)\n",
    "\n",
    "    #master_encoder = SimpleEncoder(gene_size,[640,384],292,dropRate=0.1, activation=torch.nn.ELU())#.to(device)\n",
    "    #encoder_pc3 = SimpleEncoder(gene_size,[640,384],292,dropRate=0.3, activation=torch.nn.ELU()).to(device)\n",
    "    encoder_pc3 = SimpleEncoder(gene_size,[640,384],292,dropRate=0.1, activation=torch.nn.ELU()).to(device)\n",
    "    #model = EmbInfomax(292,master_encoder)\n",
    "    \n",
    "    #model = MultiEncInfomax(292,[encoder_a375,encoder_ht29])    \n",
    "    #model = model.to(device)\n",
    "    \n",
    "    trainInfo_paired_ht29 = pd.read_csv('../preprocessing/preprocessed_data/10fold_validation_spit/add_pc3/train_paired_ht29_%s.csv'%i,index_col=0)\n",
    "    trainInfo_paired_a375 = pd.read_csv('../preprocessing/preprocessed_data/10fold_validation_spit/add_pc3/train_paired_a375_%s.csv'%i,index_col=0)\n",
    "    trainInfo_pc3 = pd.read_csv('../preprocessing/preprocessed_data/10fold_validation_spit/add_pc3/train_pc3_%s.csv'%i,index_col=0)\n",
    "    \n",
    "    trainInfo_paired = pd.read_csv('../preprocessing/preprocessed_data/10fold_validation_spit/train_paired_%s.csv'%i,index_col=0)\n",
    "    trainInfo_a375 = pd.read_csv('../preprocessing/preprocessed_data/10fold_validation_spit/train_a375_%s.csv'%i,index_col=0)\n",
    "    trainInfo_ht29 = pd.read_csv('../preprocessing/preprocessed_data/10fold_validation_spit/train_ht29_%s.csv'%i,index_col=0)\n",
    "    \n",
    "    valInfo_paired_pc3 = pd.read_csv('../preprocessing/preprocessed_data/10fold_validation_spit/add_pc3/val_paired_%s.csv'%i,index_col=0)\n",
    "    valInfo_pc3 = pd.read_csv('../preprocessing/preprocessed_data/10fold_validation_spit/add_pc3/val_pc3_%s.csv'%i,index_col=0)    \n",
    "    \n",
    "    valInfo_paired = pd.read_csv('../preprocessing/preprocessed_data/10fold_validation_spit/val_paired_%s.csv'%i,index_col=0)\n",
    "    valInfo_a375 = pd.read_csv('../preprocessing/preprocessed_data/10fold_validation_spit/val_a375_%s.csv'%i,index_col=0)\n",
    "    valInfo_ht29 = pd.read_csv('../preprocessing/preprocessed_data/10fold_validation_spit/val_ht29_%s.csv'%i,index_col=0)\n",
    "    \n",
    "    N_ht29_paired = len(trainInfo_paired_ht29)\n",
    "    N_a375_paired = len(trainInfo_paired_a375)\n",
    "    N_pc3 = len(trainInfo_pc3)\n",
    "    \n",
    "    N_paired = len(trainInfo_paired)\n",
    "    N_a375 = len(trainInfo_a375)\n",
    "    N_ht29 = len(trainInfo_ht29)\n",
    "    \n",
    "    allParams = list(decoder_pc3.parameters())\n",
    "    allParams = allParams + list(encoder_pc3.parameters())\n",
    "    allParams = allParams + list(prior_d.parameters()) + list(local_d.parameters())\n",
    "    \n",
    "    allParams = allParams + list(decoder_a375.parameters()) + list(decoder_ht29.parameters())\n",
    "    allParams = allParams + list(encoder_a375.parameters()) + list(encoder_ht29.parameters())\n",
    "    \n",
    "    optimizer = torch.optim.Adam(allParams, lr= 0.001, weight_decay=0)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer,step_size=300,gamma=0.8)\n",
    "\n",
    "    for e in range(0, NUM_EPOCHS):\n",
    "        decoder_a375.train()\n",
    "        decoder_ht29.train()\n",
    "        encoder_a375.train()\n",
    "        encoder_ht29.train()\n",
    "        decoder_pc3.train()\n",
    "        encoder_pc3.train()\n",
    "        prior_d.train()\n",
    "        local_d.train()\n",
    "        #model.train()\n",
    "        #master_encoder.train()\n",
    "        \n",
    "        trainloader_pc3 = getSamples(N_pc3, bs_pc3)\n",
    "        len_pc3 = len(trainloader_pc3)\n",
    "        trainloader_paired_ht29 = getSamples(N_ht29_paired, bs_ht29_paired)\n",
    "        len_paired_ht29 = len(trainloader_paired_ht29)\n",
    "        trainloader_paired_a375 = getSamples(N_a375_paired, bs_a375_paired)\n",
    "        len_paired_a375 = len(trainloader_paired_a375)\n",
    "        \n",
    "        trainloader_a375 = getSamples(N_a375, bs_a375)\n",
    "        len_a375 = len(trainloader_a375)\n",
    "        trainloader_ht29 = getSamples(N_ht29, bs_ht29)\n",
    "        len_ht29 = len(trainloader_ht29)\n",
    "        trainloader_paired = getSamples(N_paired, bs_paired)\n",
    "        len_paired = len(trainloader_paired)\n",
    "\n",
    "        lens = [len_pc3,len_paired_ht29,len_paired_a375,len_a375,len_ht29,len_paired]\n",
    "        #lens = [len_pc3,len_paired_ht29,len_paired_a375]\n",
    "        maxLen = np.max(lens)\n",
    "\n",
    "        if maxLen>lens[0]:\n",
    "            while maxLen>len(trainloader_pc3):\n",
    "                trainloader_suppl = getSamples(N_pc3, bs_pc3)\n",
    "                trainloader_pc3 = trainloader_pc3 + trainloader_suppl\n",
    "        \n",
    "        if maxLen>lens[1]:\n",
    "            while maxLen>len(trainloader_paired_ht29):\n",
    "                trainloader_suppl = getSamples(N_ht29_paired, bs_ht29_paired)\n",
    "                trainloader_paired_ht29 = trainloader_paired_ht29 + trainloader_suppl\n",
    "        \n",
    "        if maxLen>lens[2]:\n",
    "            while maxLen>len(trainloader_paired_a375):\n",
    "                trainloader_suppl = getSamples(N_a375_paired, bs_a375_paired)\n",
    "                trainloader_paired_a375 = trainloader_paired_a375 + trainloader_suppl\n",
    "                \n",
    "        if maxLen>lens[3]:\n",
    "            while maxLen>len(trainloader_a375):\n",
    "                trainloader_suppl = getSamples(N_a375, bs_a375)\n",
    "                trainloader_a375 = trainloader_a375 + trainloader_suppl\n",
    "        \n",
    "        if maxLen>lens[4]:\n",
    "            while maxLen>len(trainloader_ht29):\n",
    "                trainloader_suppl = getSamples(N_ht29, bs_ht29)\n",
    "                trainloader_ht29 = trainloader_ht29 + trainloader_suppl\n",
    "        \n",
    "        if maxLen>lens[5]:\n",
    "            while maxLen>len(trainloader_paired):\n",
    "                trainloader_suppl = getSamples(N_paired, bs_paired)\n",
    "                trainloader_paired = trainloader_paired + trainloader_suppl\n",
    "                \n",
    "        for j in range(maxLen):\n",
    "            dataIndex_paired_a375 = trainloader_paired_a375[j]\n",
    "            dataIndex__paired_ht29 = trainloader_paired_ht29[j]\n",
    "            dataIndex_pc3 = trainloader_pc3[j]\n",
    "            \n",
    "            dataIndex_a375 = trainloader_a375[j]\n",
    "            dataIndex_ht29 = trainloader_ht29[j]\n",
    "            dataIndex_paired = trainloader_paired[j]\n",
    "            \n",
    "            df_pairs = trainInfo_paired.iloc[dataIndex_paired,:]\n",
    "            df_a375 = trainInfo_a375.iloc[dataIndex_a375,:]\n",
    "            df_ht29 = trainInfo_ht29.iloc[dataIndex_ht29,:]\n",
    "            paired_inds = len(df_pairs)\n",
    "            \n",
    "            df_pairs_ht29 = trainInfo_paired_ht29.iloc[dataIndex__paired_ht29,:]\n",
    "            df_pairs_a375 = trainInfo_paired_a375.iloc[dataIndex_paired_a375,:]\n",
    "            df_pc3 = trainInfo_pc3.iloc[dataIndex_pc3,:]\n",
    "            paired_inds_ht29 = len(df_pairs_ht29)\n",
    "            paired_inds_a375 = len(df_pairs_a375)\n",
    "            \n",
    "            X_a375 = torch.tensor(np.concatenate((cmap.loc[df_pairs_a375['sig_id.x']].values,\n",
    "                                                  cmap.loc[df_pairs['sig_id.x']].values,\n",
    "                                                  cmap.loc[df_a375.sig_id].values))).float().to(device)\n",
    "            X_ht29 = torch.tensor(np.concatenate((cmap.loc[df_pairs_ht29['sig_id.x']].values,\n",
    "                                                  cmap.loc[df_pairs['sig_id.y']].values,\n",
    "                                                  cmap.loc[df_ht29.sig_id].values))).float().to(device)\n",
    "                 \n",
    "            #X_a375 = torch.tensor(cmap.loc[df_pairs_a375['sig_id.x']].values).float().to(device)\n",
    "            #X_ht29 = torch.tensor(cmap.loc[df_pairs_ht29['sig_id.x']].values).float().to(device)\n",
    "            #z_a375 = encoder_a375(X_a375)\n",
    "            #z_ht29 = encoder_ht29(X_ht29)\n",
    "            \n",
    "            X_pc3 = torch.tensor(np.concatenate((cmap_extra.loc[df_pairs_a375['sig_id.y']].values,\n",
    "                                                 cmap_extra.loc[df_pairs_ht29['sig_id.y']].values,\n",
    "                                                 cmap_extra.loc[df_pc3.sig_id].values))).float().to(device)\n",
    "                        \n",
    "            #conditions = np.concatenate((df_pairs_a375.conditionId.values,\n",
    "            #                             df_pairs_ht29.conditionId.values,\n",
    "            #                             df_pairs_a375.conditionId.values,\n",
    "            #                             df_pairs_ht29.conditionId.values,\n",
    "            #                             df_pc3.conditionId.values))\n",
    "            conditions = np.concatenate((df_pairs_a375.conditionId.values,\n",
    "                                         df_pairs.conditionId.values,\n",
    "                                         df_a375.conditionId.values,\n",
    "                                         df_pairs_ht29.conditionId.values,\n",
    "                                         df_pairs.conditionId.values,\n",
    "                                         df_ht29.conditionId.values,\n",
    "                                         df_pairs_a375.conditionId.values,\n",
    "                                         df_pairs_ht29.conditionId.values,\n",
    "                                         df_pc3.conditionId.values))\n",
    "            \n",
    "            size = conditions.size\n",
    "            conditions = conditions.reshape(size,1)\n",
    "            conditions = conditions == conditions.transpose()\n",
    "            conditions = conditions*1\n",
    "            mask = torch.tensor(conditions).to(device).detach()\n",
    "            pos_mask = mask\n",
    "            neg_mask = 1 - mask\n",
    "            log_2 = math.log(2.)\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            z_pc3 = encoder_pc3(X_pc3)\n",
    "            z_a375 = encoder_a375(X_a375)\n",
    "            z_ht29 = encoder_ht29(X_ht29)\n",
    "            \n",
    "            z_un = local_d(torch.cat((z_a375, z_ht29,z_pc3), 0))\n",
    "            #z_un = local_d(z_pc3)\n",
    "            res_un = torch.matmul(z_un, z_un.t())\n",
    "            \n",
    "            y_pred_pc3 = decoder_pc3(z_pc3)\n",
    "            fitLoss_pc3 = torch.mean(torch.sum((y_pred_pc3 - X_pc3)**2,dim=1))\n",
    "            L2Loss_pc3 = decoder_pc3.L2Regularization(0.01) + encoder_pc3.L2Regularization(0.01)\n",
    "            loss_pc3 = fitLoss_pc3 + L2Loss_pc3\n",
    "            \n",
    "            y_pred_a375 = decoder_a375(z_a375)\n",
    "            fitLoss_a375 = torch.mean(torch.sum((y_pred_a375 - X_a375)**2,dim=1))\n",
    "            L2Loss_a375 = decoder_a375.L2Regularization(0.01) + encoder_a375.L2Regularization(0.01)\n",
    "            loss_a375 = fitLoss_a375 + L2Loss_a375\n",
    "            \n",
    "            y_pred_ht29 = decoder_ht29(z_ht29)\n",
    "            fitLoss_ht29 = torch.mean(torch.sum((y_pred_ht29 - X_ht29)**2,dim=1))\n",
    "            L2Loss_ht29 = decoder_ht29.L2Regularization(0.01) + encoder_ht29.L2Regularization(0.01)\n",
    "            loss_ht29 = fitLoss_ht29 + L2Loss_ht29\n",
    "            \n",
    "            \n",
    "            #silimalityLoss1 = torch.mean(torch.sum((z_a375 - z_pc3[0:paired_inds_a375,:])**2,dim=-1))\n",
    "            #silimalityLoss2 = torch.mean(torch.sum((z_ht29- z_pc3[paired_inds_a375:(paired_inds_a375+paired_inds_ht29),:])**2,dim=-1))\n",
    "            silimalityLoss1 = torch.mean(torch.sum((z_a375[0:paired_inds_a375,:] - z_pc3[0:paired_inds_a375,:])**2,dim=-1))\n",
    "            silimalityLoss2 = torch.mean(torch.sum((z_ht29[0:paired_inds_ht29,:]- z_pc3[paired_inds_a375:(paired_inds_a375+paired_inds_ht29),:])**2,dim=-1))\n",
    "            silimalityLoss3 = torch.mean(torch.sum((z_ht29[paired_inds_ht29:(paired_inds_ht29+paired_inds),:]- z_a375[paired_inds_a375:(paired_inds_a375+paired_inds),:])**2,dim=-1))\n",
    "            silimalityLoss = silimalityLoss1 + silimalityLoss2 + silimalityLoss3\n",
    "            \n",
    "            p_samples = res_un * pos_mask\n",
    "            q_samples = res_un * neg_mask\n",
    "\n",
    "            Ep = log_2 - F.softplus(- p_samples)\n",
    "            Eq = F.softplus(-q_samples) + q_samples - log_2\n",
    "\n",
    "            Ep = (Ep * pos_mask).sum() / pos_mask.sum()\n",
    "            Eq = (Eq * neg_mask).sum() / neg_mask.sum()\n",
    "            mi_loss = Eq - Ep\n",
    "\n",
    "            #prior = torch.rand_like(z_pc3)\n",
    "            prior = torch.rand_like(torch.cat((z_a375, z_ht29, z_pc3), 0))\n",
    "\n",
    "            term_a = torch.log(prior_d(prior)).mean()\n",
    "            term_b = torch.log(1.0 - prior_d(torch.cat((z_a375, z_ht29, z_pc3), 0))).mean()\n",
    "            #term_b = torch.log(1.0 - prior_d(z_pc3)).mean()\n",
    "            prior_loss = -(term_a + term_b) * beta\n",
    "            \n",
    "            loss = loss_pc3  + mi_loss + prior_loss + silimalityLoss + loss_a375 + loss_ht29\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "            pearson_pc3 = pearson_r(y_pred_pc3.detach().flatten(), X_pc3.detach().flatten())\n",
    "            r2_pc3 = r_square(y_pred_pc3.detach().flatten(), X_pc3.detach().flatten())\n",
    "            mse_pc3 = torch.mean(torch.mean((y_pred_pc3.detach() - X_pc3.detach())**2,dim=1))\n",
    "            \n",
    "            pearson_a375 = pearson_r(y_pred_a375.detach().flatten(), X_a375.detach().flatten())\n",
    "            r2_a375 = r_square(y_pred_a375.detach().flatten(), X_a375.detach().flatten())\n",
    "            mse_a375 = torch.mean(torch.mean((y_pred_a375.detach() - X_a375.detach())**2,dim=1))\n",
    "        \n",
    "            pearson_ht29 = pearson_r(y_pred_ht29.detach().flatten(), X_ht29.detach().flatten())\n",
    "            r2_ht29 = r_square(y_pred_ht29.detach().flatten(), X_ht29.detach().flatten())\n",
    "            mse_ht29 = torch.mean(torch.mean((y_pred_ht29.detach() - X_ht29.detach())**2,dim=1))\n",
    "            \n",
    "            \n",
    "        scheduler.step()\n",
    "        outString = 'Split {:.0f}: Epoch={:.0f}/{:.0f}'.format(i+1,e+1,NUM_EPOCHS)\n",
    "        outString += ', r2_pc3={:.4f}'.format(r2_pc3.item())\n",
    "        outString += ', pearson_pc3={:.4f}'.format(pearson_pc3.item())\n",
    "        outString += ', MSE_pc3={:.4f}'.format(mse_pc3.item())\n",
    "        outString += ', r2_a375={:.4f}'.format(r2_a375.item())\n",
    "        outString += ', pearson_a375={:.4f}'.format(pearson_a375.item())\n",
    "        outString += ', MSE_a375={:.4f}'.format(mse_a375.item())\n",
    "        outString += ', r2_ht29={:.4f}'.format(r2_ht29.item())\n",
    "        outString += ', pearson_ht29={:.4f}'.format(pearson_ht29.item())\n",
    "        outString += ', MSE_ht29={:.4f}'.format(mse_ht29.item())\n",
    "        outString += ', MI Loss={:.4f}'.format(mi_loss.item())\n",
    "        outString += ', Prior Loss={:.4f}'.format(prior_loss.item())\n",
    "        outString += ', loss={:.4f}'.format(loss.item())\n",
    "        if (e%250==0):\n",
    "            print(outString)\n",
    "    print(outString)\n",
    "    #trainLoss.append(splitLoss)\n",
    "    decoder_a375.eval()\n",
    "    decoder_ht29.eval()\n",
    "    encoder_a375.eval()\n",
    "    encoder_ht29.eval()\n",
    "    prior_d.eval()\n",
    "    local_d.eval()\n",
    "    decoder_pc3.eval()\n",
    "    encoder_pc3.eval()\n",
    "    #model.eval()\n",
    "    #master_encoder.eval()\n",
    "                               \n",
    "    \n",
    "    paired_val_inds_pc3 = len(valInfo_paired_pc3)\n",
    "    paired_val_inds = len(valInfo_paired)\n",
    "    #paired_val_ctrl_inds = len(valInfo_paired_untreat)\n",
    "    \n",
    "    x_a375 = torch.tensor(np.concatenate((cmap.loc[valInfo_paired_pc3['sig_id.x']].values,\n",
    "                                          cmap.loc[valInfo_paired['sig_id.x']].values,\n",
    "                                          cmap.loc[valInfo_a375.sig_id].values))).float().to(device)\n",
    "    x_ht29 = torch.tensor(np.concatenate((cmap.loc[valInfo_paired_pc3['sig_id.y']].values,\n",
    "                                          cmap.loc[valInfo_paired['sig_id.y']].values,\n",
    "                                          cmap.loc[valInfo_ht29.sig_id].values))).float().to(device)        \n",
    "                 \n",
    "    #x_a375 = torch.tensor(cmap.loc[paired_val_inds_pc3['sig_id.x']].values).float().to(device)\n",
    "    #x_ht29 = torch.tensor(cmap.loc[paired_val_inds_pc3['sig_id.y']].values).float().to(device)\n",
    "    z_a375 = encoder_a375(x_a375)\n",
    "    z_ht29 = encoder_ht29(x_ht29)\n",
    "            \n",
    "    x_pc3 = torch.tensor(np.concatenate((cmap_extra.loc[valInfo_paired_pc3['sig_id']].values,\n",
    "                                         cmap_extra.loc[valInfo_pc3.sig_id].values))).float().to(device)\n",
    "    \n",
    "    z_latent_pc3 = encoder_pc3(x_pc3)\n",
    "    xhat_pc3 = decoder_pc3(z_latent_pc3)\n",
    "    \n",
    "    x_pc3_equivalent = x_pc3[0:paired_val_inds_pc3,:]\n",
    "    z_equiv_pc3 = encoder_pc3(x_pc3_equivalent)\n",
    "    \n",
    "    r2_pc3 = r_square(xhat_pc3.detach().flatten(), x_pc3.detach().flatten())\n",
    "    pearson_pc3 = pearson_r(xhat_pc3.detach().flatten(), x_pc3.detach().flatten())\n",
    "    mse_pc3 = torch.mean(torch.mean((xhat_pc3 - x_pc3)**2,dim=1))\n",
    "    rhos = []\n",
    "    for jj in range(xhat_pc3.shape[0]):\n",
    "        rho,p = spearmanr(x_pc3[jj,:].detach().cpu().numpy(),xhat_pc3[jj,:].detach().cpu().numpy())\n",
    "        rhos.append(rho)\n",
    "    valSpear_pc3.append(np.mean(rhos))\n",
    "    acc = pseudoAccuracy(x_pc3.detach().cpu(),xhat_pc3.detach().cpu(),eps=1e-6)\n",
    "    valAccuracy_pc3.append(np.mean(acc))\n",
    "    \n",
    "    valR2_pc3.append(r2_pc3.item())\n",
    "    valPear_pc3.append(pearson_pc3.item())\n",
    "    valMSE_pc3.append(mse_pc3.item())\n",
    "    print('Pearson correlation PC3: %s'%pearson_pc3.item())\n",
    "    print('Spearman correlation PC3: %s'%valSpear_pc3[i])\n",
    "    print('Pseudo-Accuracy PC3: %s'%valAccuracy_pc3[i])\n",
    "    \n",
    "    pearDirect_ht29 = pearson_r(x_pc3_equivalent.detach().flatten(), x_ht29[0:paired_val_inds_pc3,:].detach().flatten())\n",
    "    pearDirect_a375 = pearson_r(x_pc3_equivalent.detach().flatten(), x_a375[0:paired_val_inds_pc3,:].detach().flatten())\n",
    "    rhos = []\n",
    "    for jj in range(x_pc3_equivalent.shape[0]):\n",
    "        rho,p = spearmanr(x_pc3_equivalent[jj,:].detach().cpu().numpy(),x_ht29[jj,:].detach().cpu().numpy())\n",
    "        rhos.append(rho)\n",
    "    spearDirect_ht29 = np.mean(rhos)\n",
    "    for jj in range(x_pc3_equivalent.shape[0]):\n",
    "        rho,p = spearmanr(x_pc3_equivalent[jj,:].detach().cpu().numpy(),x_a375[jj,:].detach().cpu().numpy())\n",
    "        rhos.append(rho)\n",
    "    spearDirect_a375 = np.mean(rhos)\n",
    "    \n",
    "    accDirect_ht29_to_pc3 = np.mean(pseudoAccuracy(x_pc3_equivalent.detach().cpu(),x_ht29[0:paired_val_inds_pc3,:].detach().cpu(),eps=1e-6))\n",
    "    accDirect_pc3_to_ht29 = np.mean(pseudoAccuracy(x_ht29[0:paired_val_inds_pc3,:].detach().cpu(),x_pc3_equivalent.detach().cpu(),eps=1e-6))\n",
    "    accDirect_pc3_to_a375 = np.mean(pseudoAccuracy(x_a375[0:paired_val_inds_pc3,:].detach().cpu(),x_pc3_equivalent.detach().cpu(),eps=1e-6))\n",
    "    accDirect_a375_to_pc3 = np.mean(pseudoAccuracy(x_pc3_equivalent.detach().cpu(),x_a375[0:paired_val_inds_pc3,:].detach().cpu(),eps=1e-6))\n",
    "    \n",
    "    x_hat_a375_to_pc3 = decoder_pc3(z_a375[0:paired_val_inds_pc3,:]).detach()\n",
    "    pearson_a375_to_pc3 = pearson_r(x_hat_a375_to_pc3.detach().flatten(), x_pc3_equivalent.detach().flatten())\n",
    "    rhos = []\n",
    "    for jj in range(x_hat_a375_to_pc3.shape[0]):\n",
    "        rho,p = spearmanr(x_pc3_equivalent[jj,:].detach().cpu().numpy(),x_hat_a375_to_pc3[jj,:].detach().cpu().numpy())\n",
    "        rhos.append(rho)\n",
    "    rho_a375_to_pc3 = np.mean(rhos)\n",
    "    acc_a375_to_pc3 = np.mean(pseudoAccuracy(x_pc3_equivalent.detach().cpu(),x_hat_a375_to_pc3.detach().cpu(),eps=1e-6))\n",
    "    \n",
    "    print('Pearson of direct A375 to PC3 translation: %s'%pearDirect_a375.item())\n",
    "    print('Pearson correlation A375 to PC3: %s'%pearson_a375_to_pc3.item())\n",
    "    print('Pseudo accuracy A375 to PC3: %s'%acc_a375_to_pc3)\n",
    "    \n",
    "    x_hat_pc3_to_a375 = decoder_a375(z_equiv_pc3).detach()\n",
    "    pearson_pc3_to_a375 = pearson_r(x_hat_pc3_to_a375.detach(), x_a375[0:paired_val_inds_pc3,:].detach())\n",
    "    rhos = []\n",
    "    for jj in range(x_hat_pc3_to_a375.shape[0]):\n",
    "        rho,p = spearmanr(x_a375[jj,:].detach().cpu().numpy(),x_hat_pc3_to_a375[jj,:].detach().cpu().numpy())\n",
    "        rhos.append(rho)\n",
    "    rho_pc3_to_a375 = np.mean(rhos)\n",
    "    acc_pc3_to_a375 = np.mean(pseudoAccuracy(x_a375[0:paired_val_inds_pc3,:].detach().cpu(),x_hat_pc3_to_a375.detach().cpu(),eps=1e-6))\n",
    "    \n",
    "    print('Pearson correlation PC3 to A375: %s'%pearson_pc3_to_a375.item())\n",
    "    print('Pseudo accuracy PC3 to A375: %s'%acc_pc3_to_a375)\n",
    "    \n",
    "    x_hat_ht29_to_pc3 = decoder_pc3(z_ht29[0:paired_val_inds_pc3,:]).detach()\n",
    "    pearson_ht29_to_pc3 = pearson_r(x_hat_ht29_to_pc3.detach(), x_pc3_equivalent.detach())\n",
    "    rhos = []\n",
    "    for jj in range(x_hat_ht29_to_pc3.shape[0]):\n",
    "        rho,p = spearmanr(x_pc3_equivalent[jj,:].detach().cpu().numpy(),x_hat_ht29_to_pc3[jj,:].detach().cpu().numpy())\n",
    "        rhos.append(rho)\n",
    "    rho_ht29_to_pc3 = np.mean(rhos)\n",
    "    acc_ht29_to_pc3 = np.mean(pseudoAccuracy(x_pc3_equivalent.detach().cpu(),x_hat_ht29_to_pc3.detach().cpu(),eps=1e-6))\n",
    "    \n",
    "    print('Pearson of direct HT29 to PC3 translation: %s'%pearDirect_ht29.item())\n",
    "    print('Pearson correlation HT29 to PC3: %s'%pearson_ht29_to_pc3.item())\n",
    "    print('Pseudo accuracy HT29 to PC3: %s'%acc_ht29_to_pc3)\n",
    "    \n",
    "    \n",
    "    x_hat_pc3_to_ht29 = decoder_ht29(z_equiv_pc3).detach()\n",
    "    pearson_pc3_to_ht29 = pearson_r(x_hat_pc3_to_ht29.detach(), x_ht29[0:paired_val_inds_pc3,:].detach())\n",
    "    rhos = []\n",
    "    for jj in range(x_hat_pc3_to_ht29.shape[0]):\n",
    "        rho,p = spearmanr(x_ht29[jj,:].detach().cpu().numpy(),x_hat_pc3_to_ht29[jj,:].detach().cpu().numpy())\n",
    "        rhos.append(rho)\n",
    "    rho_pc3_to_ht29 = np.mean(rhos)\n",
    "    acc_pc3_to_ht29 = np.mean(pseudoAccuracy(x_ht29[0:paired_val_inds_pc3,:].detach().cpu(),x_hat_pc3_to_ht29.detach().cpu(),eps=1e-6))\n",
    "    \n",
    "    print('Pearson correlation PC3 to HT29: %s'%pearson_pc3_to_ht29.item())\n",
    "    print('Pseudo accuracy PC3 to HT29: %s'%acc_pc3_to_ht29)\n",
    "    \n",
    "    \n",
    "    xhat_a375 = decoder_a375(z_a375)\n",
    "    xhat_ht29 = decoder_ht29(z_ht29)\n",
    "    \n",
    "    pearson_a375 = pearson_r(xhat_a375.detach().flatten(), x_a375.detach().flatten())\n",
    "    pearson_ht29 = pearson_r(xhat_ht29.detach().flatten(), x_ht29.detach().flatten())\n",
    "    rhos = []\n",
    "    for jj in range(xhat_a375.shape[0]):\n",
    "        rho,p = spearmanr(x_a375[jj,:].detach().cpu().numpy(),xhat_a375[jj,:].detach().cpu().numpy())\n",
    "        rhos.append(rho)\n",
    "    valSpear_a375.append(np.mean(rhos))\n",
    "    acc = pseudoAccuracy(x_a375.detach().cpu(),xhat_a375.detach().cpu(),eps=1e-6)\n",
    "    valAccuracy_a375.append(np.mean(acc))\n",
    "    rhos = []\n",
    "    for jj in range(xhat_ht29.shape[0]):\n",
    "        rho,p = spearmanr(x_ht29[jj,:].detach().cpu().numpy(),xhat_ht29[jj,:].detach().cpu().numpy())\n",
    "        rhos.append(rho)\n",
    "    valSpear_ht29.append(np.mean(rhos))\n",
    "    acc = pseudoAccuracy(x_ht29.detach().cpu(),xhat_ht29.detach().cpu(),eps=1e-6)\n",
    "    valAccuracy_ht29.append(np.mean(acc))\n",
    "    \n",
    "    print('Pearson correlation A375: %s'%pearson_a375.item())\n",
    "    print('Spearman correlation A375: %s'%valSpear_a375[i])\n",
    "    print('Pseudo-Accuracy A375: %s'%valAccuracy_a375[i])\n",
    "    print('Pearson correlation HT29: %s'%pearson_ht29.item())\n",
    "    print('Spearman correlation HT29: %s'%valSpear_ht29[i])\n",
    "    print('Pseudo-Accuracy HT29: %s'%valAccuracy_ht29[i])\n",
    "    \n",
    "    x_a375_equivalent = x_a375[paired_val_inds_pc3:(paired_val_inds_pc3+paired_val_inds),:]\n",
    "    x_ht29_equivalent = x_ht29[paired_val_inds_pc3:(paired_val_inds_pc3+paired_val_inds),:]\n",
    "    \n",
    "    z_latent_a375_equivalent  = encoder_a375(x_a375_equivalent)\n",
    "    x_hat_ht29_equivalent = decoder_ht29(z_latent_a375_equivalent).detach()\n",
    "    pearson_ht29 = pearson_r(x_hat_ht29_equivalent.detach().flatten(), x_ht29_equivalent.detach().flatten())\n",
    "    rhos = []\n",
    "    for jj in range(x_hat_ht29_equivalent.shape[0]):\n",
    "        rho,p = spearmanr(x_ht29_equivalent[jj,:].detach().cpu().numpy(),x_hat_ht29_equivalent[jj,:].detach().cpu().numpy())\n",
    "        rhos.append(rho)\n",
    "    rho_ht29 = np.mean(rhos)\n",
    "    acc_ht29 = np.mean(pseudoAccuracy(x_ht29_equivalent.detach().cpu(),x_hat_ht29_equivalent.detach().cpu(),eps=1e-6))\n",
    "    print('Pearson correlation A375 to HT29: %s'%pearson_ht29.item())\n",
    "    print('Pseudo accuracy A375 to HT29: %s'%acc_ht29)\n",
    "    \n",
    "    z_latent_ht29_equivalent  = encoder_ht29(x_ht29_equivalent)\n",
    "    x_hat_a375_equivalent = decoder_a375(z_latent_ht29_equivalent).detach()\n",
    "    pearson_a375 = pearson_r(x_hat_a375_equivalent.detach(), x_a375_equivalent.detach())\n",
    "    rhos = []\n",
    "    for jj in range(x_hat_a375_equivalent.shape[0]):\n",
    "        rho,p = spearmanr(x_a375_equivalent[jj,:].detach().cpu().numpy(),x_hat_a375_equivalent[jj,:].detach().cpu().numpy())\n",
    "        rhos.append(rho)\n",
    "    rho_a375 = np.mean(rhos)\n",
    "    acc_a375 = np.mean(pseudoAccuracy(x_a375_equivalent.detach().cpu(),x_hat_a375_equivalent.detach().cpu(),eps=1e-6))\n",
    "    print('Pearson correlation HT29 to A375: %s'%pearson_a375.item())\n",
    "    print('Pseudo accuracy HT29 to A375: %s'%acc_a375)\n",
    "    \n",
    "    \n",
    "    valPear.append([pearson_ht29_to_pc3.item(),pearson_pc3_to_ht29.item(),\n",
    "                    pearson_a375_to_pc3.item(),pearson_pc3_to_a375.item()])\n",
    "    valSpear.append([rho_ht29_to_pc3,rho_pc3_to_ht29,\n",
    "                    rho_a375_to_pc3,rho_pc3_to_a375])\n",
    "    valAccuracy.append([acc_ht29_to_pc3,acc_pc3_to_ht29,\n",
    "                    acc_a375_to_pc3,acc_pc3_to_a375])    \n",
    "        \n",
    "    valPearDirect.append([pearDirect_ht29.item(),pearDirect_a375.item()])\n",
    "    valSpearDirect.append([spearDirect_ht29,spearDirect_a375])\n",
    "    valAccDirect.append([accDirect_ht29_to_pc3,accDirect_pc3_to_ht29,\n",
    "                         accDirect_a375_to_pc3,accDirect_pc3_to_a375])\n",
    "    \n",
    "    torch.save(decoder_pc3,'../results/MI_results/models/TwoEncoders_TwoDecoders_MI_withPC3/decoder_pc3_%s.pt'%i)\n",
    "    torch.save(prior_d,'../results/MI_results/models/TwoEncoders_TwoDecoders_MI_withPC3/priorDiscr_%s.pt'%i)\n",
    "    torch.save(local_d,'../results/MI_results/models/TwoEncoders_TwoDecoders_MI_withPC3/localDiscr_%s.pt'%i)\n",
    "    torch.save(encoder_pc3,'../results/MI_results/models/TwoEncoders_TwoDecoders_MI_withPC3/encoder_pc3_%s.pt'%i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "465d75a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<seaborn.axisgrid.FacetGrid at 0x257c67dceb0>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "sns.set()\n",
    "sns.displot(z_a375.flatten().detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e0664f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "valPear = np.array(valPear)\n",
    "valPearDirect = np.array(valPearDirect)\n",
    "valSpear = np.array(valSpear)\n",
    "valAccuracy= np.array(valAccuracy)\n",
    "valSpearDirect= np.array(valSpearDirect)\n",
    "valAccDirect= np.array(valAccDirect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "095f4069",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.46507483 0.51164811 0.66520304 0.50722907]\n",
      "[0.45465615 0.46487467]\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(valPear,axis=0))\n",
    "print(np.mean(valPearDirect,axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bfcd7c89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.58533753 0.62360328 0.59590191 0.62110303]\n",
      "[0.40613578 0.40984133]\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(valSpear,axis=0))\n",
    "print(np.mean(valSpearDirect,axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bf52abc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.70641428 0.72038035 0.70989876 0.71921607]\n",
      "[0.64109136 0.64109136 0.64215531 0.64215531]\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(valAccuracy,axis=0))\n",
    "print(np.mean(valAccDirect,axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "11cf4e22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pearson_ht29_to_pc3</th>\n",
       "      <th>pearson_pc3_to_ht29</th>\n",
       "      <th>pearson_a375_to_pc3</th>\n",
       "      <th>pearson_pc3_to_a375</th>\n",
       "      <th>spear_ht29_to_pc3</th>\n",
       "      <th>spear_pc3_to_ht29</th>\n",
       "      <th>spear_a375_to_pc3</th>\n",
       "      <th>spear_pc3_to_a375</th>\n",
       "      <th>acc_ht29_to_pc3</th>\n",
       "      <th>acc_pc3_to_ht29</th>\n",
       "      <th>...</th>\n",
       "      <th>recon_spear</th>\n",
       "      <th>recon_acc</th>\n",
       "      <th>Direct_pearson_ht29</th>\n",
       "      <th>Direct_pearson_a375</th>\n",
       "      <th>Direct_spearman_ht29</th>\n",
       "      <th>Direct_spearman_a375</th>\n",
       "      <th>DirectAcc_ht29_to_pc3</th>\n",
       "      <th>DirectAcc_pc3_to_ht29</th>\n",
       "      <th>DirectAcc_a375_to_pc3</th>\n",
       "      <th>DirectAcc_pc3_to_a375</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.477352</td>\n",
       "      <td>0.481841</td>\n",
       "      <td>0.676161</td>\n",
       "      <td>0.510564</td>\n",
       "      <td>0.594422</td>\n",
       "      <td>0.628105</td>\n",
       "      <td>0.613387</td>\n",
       "      <td>0.657143</td>\n",
       "      <td>0.711656</td>\n",
       "      <td>0.724847</td>\n",
       "      <td>...</td>\n",
       "      <td>0.793175</td>\n",
       "      <td>0.811434</td>\n",
       "      <td>0.439836</td>\n",
       "      <td>0.465554</td>\n",
       "      <td>0.417261</td>\n",
       "      <td>0.416814</td>\n",
       "      <td>0.647444</td>\n",
       "      <td>0.647444</td>\n",
       "      <td>0.642536</td>\n",
       "      <td>0.642536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.363778</td>\n",
       "      <td>0.382533</td>\n",
       "      <td>0.656962</td>\n",
       "      <td>0.510727</td>\n",
       "      <td>0.512618</td>\n",
       "      <td>0.515155</td>\n",
       "      <td>0.594779</td>\n",
       "      <td>0.631919</td>\n",
       "      <td>0.681282</td>\n",
       "      <td>0.675208</td>\n",
       "      <td>...</td>\n",
       "      <td>0.786405</td>\n",
       "      <td>0.808384</td>\n",
       "      <td>0.322189</td>\n",
       "      <td>0.421363</td>\n",
       "      <td>0.327459</td>\n",
       "      <td>0.367538</td>\n",
       "      <td>0.612715</td>\n",
       "      <td>0.612715</td>\n",
       "      <td>0.640623</td>\n",
       "      <td>0.640623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.538556</td>\n",
       "      <td>0.608971</td>\n",
       "      <td>0.748081</td>\n",
       "      <td>0.545080</td>\n",
       "      <td>0.656210</td>\n",
       "      <td>0.741779</td>\n",
       "      <td>0.648656</td>\n",
       "      <td>0.705986</td>\n",
       "      <td>0.725517</td>\n",
       "      <td>0.764599</td>\n",
       "      <td>...</td>\n",
       "      <td>0.794075</td>\n",
       "      <td>0.811219</td>\n",
       "      <td>0.474551</td>\n",
       "      <td>0.494168</td>\n",
       "      <td>0.398426</td>\n",
       "      <td>0.409614</td>\n",
       "      <td>0.627925</td>\n",
       "      <td>0.627925</td>\n",
       "      <td>0.635878</td>\n",
       "      <td>0.635878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.422832</td>\n",
       "      <td>0.458472</td>\n",
       "      <td>0.605840</td>\n",
       "      <td>0.438371</td>\n",
       "      <td>0.563689</td>\n",
       "      <td>0.585709</td>\n",
       "      <td>0.554364</td>\n",
       "      <td>0.584598</td>\n",
       "      <td>0.697904</td>\n",
       "      <td>0.707106</td>\n",
       "      <td>...</td>\n",
       "      <td>0.793517</td>\n",
       "      <td>0.808558</td>\n",
       "      <td>0.436779</td>\n",
       "      <td>0.438862</td>\n",
       "      <td>0.406538</td>\n",
       "      <td>0.404856</td>\n",
       "      <td>0.645706</td>\n",
       "      <td>0.645706</td>\n",
       "      <td>0.637935</td>\n",
       "      <td>0.637935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.585248</td>\n",
       "      <td>0.641342</td>\n",
       "      <td>0.772910</td>\n",
       "      <td>0.624267</td>\n",
       "      <td>0.690651</td>\n",
       "      <td>0.728996</td>\n",
       "      <td>0.684524</td>\n",
       "      <td>0.680367</td>\n",
       "      <td>0.760590</td>\n",
       "      <td>0.778520</td>\n",
       "      <td>...</td>\n",
       "      <td>0.797384</td>\n",
       "      <td>0.813686</td>\n",
       "      <td>0.627471</td>\n",
       "      <td>0.627625</td>\n",
       "      <td>0.550474</td>\n",
       "      <td>0.543776</td>\n",
       "      <td>0.709100</td>\n",
       "      <td>0.709100</td>\n",
       "      <td>0.690695</td>\n",
       "      <td>0.690695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.447230</td>\n",
       "      <td>0.513924</td>\n",
       "      <td>0.623779</td>\n",
       "      <td>0.522441</td>\n",
       "      <td>0.526192</td>\n",
       "      <td>0.597197</td>\n",
       "      <td>0.523332</td>\n",
       "      <td>0.581903</td>\n",
       "      <td>0.681118</td>\n",
       "      <td>0.697546</td>\n",
       "      <td>...</td>\n",
       "      <td>0.774141</td>\n",
       "      <td>0.800273</td>\n",
       "      <td>0.424929</td>\n",
       "      <td>0.412846</td>\n",
       "      <td>0.381743</td>\n",
       "      <td>0.370042</td>\n",
       "      <td>0.624881</td>\n",
       "      <td>0.624881</td>\n",
       "      <td>0.622563</td>\n",
       "      <td>0.622563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.435461</td>\n",
       "      <td>0.493642</td>\n",
       "      <td>0.671362</td>\n",
       "      <td>0.482408</td>\n",
       "      <td>0.577412</td>\n",
       "      <td>0.591574</td>\n",
       "      <td>0.604649</td>\n",
       "      <td>0.594431</td>\n",
       "      <td>0.700920</td>\n",
       "      <td>0.707566</td>\n",
       "      <td>...</td>\n",
       "      <td>0.779161</td>\n",
       "      <td>0.801974</td>\n",
       "      <td>0.450948</td>\n",
       "      <td>0.453384</td>\n",
       "      <td>0.397864</td>\n",
       "      <td>0.397833</td>\n",
       "      <td>0.635713</td>\n",
       "      <td>0.635713</td>\n",
       "      <td>0.638502</td>\n",
       "      <td>0.638502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.469025</td>\n",
       "      <td>0.566657</td>\n",
       "      <td>0.615896</td>\n",
       "      <td>0.424193</td>\n",
       "      <td>0.607523</td>\n",
       "      <td>0.644934</td>\n",
       "      <td>0.611231</td>\n",
       "      <td>0.591890</td>\n",
       "      <td>0.709251</td>\n",
       "      <td>0.734031</td>\n",
       "      <td>...</td>\n",
       "      <td>0.770985</td>\n",
       "      <td>0.795520</td>\n",
       "      <td>0.450571</td>\n",
       "      <td>0.416015</td>\n",
       "      <td>0.396771</td>\n",
       "      <td>0.394426</td>\n",
       "      <td>0.635631</td>\n",
       "      <td>0.635631</td>\n",
       "      <td>0.632984</td>\n",
       "      <td>0.632984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.441269</td>\n",
       "      <td>0.487214</td>\n",
       "      <td>0.640091</td>\n",
       "      <td>0.531208</td>\n",
       "      <td>0.555763</td>\n",
       "      <td>0.587370</td>\n",
       "      <td>0.575217</td>\n",
       "      <td>0.589182</td>\n",
       "      <td>0.696063</td>\n",
       "      <td>0.698236</td>\n",
       "      <td>...</td>\n",
       "      <td>0.789219</td>\n",
       "      <td>0.805951</td>\n",
       "      <td>0.434198</td>\n",
       "      <td>0.445302</td>\n",
       "      <td>0.380316</td>\n",
       "      <td>0.386712</td>\n",
       "      <td>0.629218</td>\n",
       "      <td>0.629218</td>\n",
       "      <td>0.636120</td>\n",
       "      <td>0.636120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.469996</td>\n",
       "      <td>0.481886</td>\n",
       "      <td>0.640947</td>\n",
       "      <td>0.483032</td>\n",
       "      <td>0.568895</td>\n",
       "      <td>0.615212</td>\n",
       "      <td>0.548879</td>\n",
       "      <td>0.593611</td>\n",
       "      <td>0.699841</td>\n",
       "      <td>0.716144</td>\n",
       "      <td>...</td>\n",
       "      <td>0.793839</td>\n",
       "      <td>0.809024</td>\n",
       "      <td>0.485088</td>\n",
       "      <td>0.473629</td>\n",
       "      <td>0.404505</td>\n",
       "      <td>0.406804</td>\n",
       "      <td>0.642581</td>\n",
       "      <td>0.642581</td>\n",
       "      <td>0.643717</td>\n",
       "      <td>0.643717</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows  23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   pearson_ht29_to_pc3  pearson_pc3_to_ht29  pearson_a375_to_pc3  \\\n",
       "0             0.477352             0.481841             0.676161   \n",
       "1             0.363778             0.382533             0.656962   \n",
       "2             0.538556             0.608971             0.748081   \n",
       "3             0.422832             0.458472             0.605840   \n",
       "4             0.585248             0.641342             0.772910   \n",
       "5             0.447230             0.513924             0.623779   \n",
       "6             0.435461             0.493642             0.671362   \n",
       "7             0.469025             0.566657             0.615896   \n",
       "8             0.441269             0.487214             0.640091   \n",
       "9             0.469996             0.481886             0.640947   \n",
       "\n",
       "   pearson_pc3_to_a375  spear_ht29_to_pc3  spear_pc3_to_ht29  \\\n",
       "0             0.510564           0.594422           0.628105   \n",
       "1             0.510727           0.512618           0.515155   \n",
       "2             0.545080           0.656210           0.741779   \n",
       "3             0.438371           0.563689           0.585709   \n",
       "4             0.624267           0.690651           0.728996   \n",
       "5             0.522441           0.526192           0.597197   \n",
       "6             0.482408           0.577412           0.591574   \n",
       "7             0.424193           0.607523           0.644934   \n",
       "8             0.531208           0.555763           0.587370   \n",
       "9             0.483032           0.568895           0.615212   \n",
       "\n",
       "   spear_a375_to_pc3  spear_pc3_to_a375  acc_ht29_to_pc3  acc_pc3_to_ht29  \\\n",
       "0           0.613387           0.657143         0.711656         0.724847   \n",
       "1           0.594779           0.631919         0.681282         0.675208   \n",
       "2           0.648656           0.705986         0.725517         0.764599   \n",
       "3           0.554364           0.584598         0.697904         0.707106   \n",
       "4           0.684524           0.680367         0.760590         0.778520   \n",
       "5           0.523332           0.581903         0.681118         0.697546   \n",
       "6           0.604649           0.594431         0.700920         0.707566   \n",
       "7           0.611231           0.591890         0.709251         0.734031   \n",
       "8           0.575217           0.589182         0.696063         0.698236   \n",
       "9           0.548879           0.593611         0.699841         0.716144   \n",
       "\n",
       "   ...  recon_spear  recon_acc  Direct_pearson_ht29  Direct_pearson_a375  \\\n",
       "0  ...     0.793175   0.811434             0.439836             0.465554   \n",
       "1  ...     0.786405   0.808384             0.322189             0.421363   \n",
       "2  ...     0.794075   0.811219             0.474551             0.494168   \n",
       "3  ...     0.793517   0.808558             0.436779             0.438862   \n",
       "4  ...     0.797384   0.813686             0.627471             0.627625   \n",
       "5  ...     0.774141   0.800273             0.424929             0.412846   \n",
       "6  ...     0.779161   0.801974             0.450948             0.453384   \n",
       "7  ...     0.770985   0.795520             0.450571             0.416015   \n",
       "8  ...     0.789219   0.805951             0.434198             0.445302   \n",
       "9  ...     0.793839   0.809024             0.485088             0.473629   \n",
       "\n",
       "   Direct_spearman_ht29  Direct_spearman_a375  DirectAcc_ht29_to_pc3  \\\n",
       "0              0.417261              0.416814               0.647444   \n",
       "1              0.327459              0.367538               0.612715   \n",
       "2              0.398426              0.409614               0.627925   \n",
       "3              0.406538              0.404856               0.645706   \n",
       "4              0.550474              0.543776               0.709100   \n",
       "5              0.381743              0.370042               0.624881   \n",
       "6              0.397864              0.397833               0.635713   \n",
       "7              0.396771              0.394426               0.635631   \n",
       "8              0.380316              0.386712               0.629218   \n",
       "9              0.404505              0.406804               0.642581   \n",
       "\n",
       "   DirectAcc_pc3_to_ht29  DirectAcc_a375_to_pc3  DirectAcc_pc3_to_a375  \n",
       "0               0.647444               0.642536               0.642536  \n",
       "1               0.612715               0.640623               0.640623  \n",
       "2               0.627925               0.635878               0.635878  \n",
       "3               0.645706               0.637935               0.637935  \n",
       "4               0.709100               0.690695               0.690695  \n",
       "5               0.624881               0.622563               0.622563  \n",
       "6               0.635713               0.638502               0.638502  \n",
       "7               0.635631               0.632984               0.632984  \n",
       "8               0.629218               0.636120               0.636120  \n",
       "9               0.642581               0.643717               0.643717  \n",
       "\n",
       "[10 rows x 23 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_result = pd.DataFrame({'pearson_ht29_to_pc3':valPear[:,0],'pearson_pc3_to_ht29':valPear[:,1],\n",
    "                          'pearson_a375_to_pc3':valPear[:,2],'pearson_pc3_to_a375':valPear[:,3],\n",
    "                          'spear_ht29_to_pc3':valSpear[:,0],'spear_pc3_to_ht29':valSpear[:,1],\n",
    "                          'spear_a375_to_pc3':valSpear[:,2],'spear_pc3_to_a375':valSpear[:,3],\n",
    "                          'acc_ht29_to_pc3':valAccuracy[:,0],'acc_pc3_to_ht29':valAccuracy[:,1],\n",
    "                          'acc_a375_to_pc3':valAccuracy[:,2],'acc_pc3_to_a375':valAccuracy[:,3],\n",
    "                          'recon_pear':valPear_pc3,'recon_spear':valSpear_pc3 ,'recon_acc':valAccuracy_pc3,\n",
    "                          'Direct_pearson_ht29':valPearDirect[:,0],'Direct_pearson_a375':valPearDirect[:,1],\n",
    "                          'Direct_spearman_ht29':valSpearDirect[:,0],'Direct_spearman_a375':valSpearDirect[:,1],\n",
    "                          'DirectAcc_ht29_to_pc3':valAccDirect[:,0],'DirectAcc_pc3_to_ht29':valAccDirect[:,1],\n",
    "                          'DirectAcc_a375_to_pc3':valAccDirect[:,2],'DirectAcc_pc3_to_a375':valAccDirect[:,3]})\n",
    "df_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a0902183",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result.to_csv('../results/MI_results/retrain_landmarks_10foldvalidation_MIuniform_and_l2sim_2000ep_withpc3.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
