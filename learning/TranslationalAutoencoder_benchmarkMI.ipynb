{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44c0260f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division\n",
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from trainingUtils import MultipleOptimizer, MultipleScheduler, compute_kernel, compute_mmd\n",
    "from models import Encoder,Decoder,GaussianDecoder,VAE,CellStateEncoder,\\\n",
    "                   CellStateDecoder, CellStateVAE,\\\n",
    "                   SimpleEncoder,LocalDiscriminator,PriorDiscriminator,\\\n",
    "                   EmbInfomax,MultiEncInfomax,Classifier,\\\n",
    "                   SpeciesCovariate,GaussianDecoder\n",
    "# import argparse\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "import random\n",
    "import os\n",
    "from IPython.display import clear_output\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "#from scipy.stats import pearsonr\n",
    "from sklearn.metrics import silhouette_score,confusion_matrix\n",
    "from scipy.stats import spearmanr\n",
    "from evaluationUtils import r_square,get_cindex,pearson_r,pseudoAccuracy\n",
    "import seaborn as sns\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e11c5049",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f08d1d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize environment and seeds for reproducability\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "\n",
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    \n",
    "# Read data\n",
    "cmap = pd.read_csv('../preprocessing/preprocessed_data/cmap_landmarks_HT29_A375.csv',index_col = 0)\n",
    "# cmap = pd.read_csv('../preprocessing/preprocessed_data/cmap_landmarks_HA1E_PC3.csv',index_col = 0)\n",
    "#cmap = pd.read_csv('../preprocessing/preprocessed_data/cmap_HT29_A375.csv',index_col = 0)\n",
    "#cmap_tf = pd.read_csv('../../../L1000_2021_11_23/cmap_compounds_tfs_repq1_tas03.tsv',\n",
    "#                       sep='\\t', low_memory=False, index_col=0)\n",
    "\n",
    "gene_size = len(cmap.columns)\n",
    "samples = cmap.index.values\n",
    "# gene_size = len(cmap_tf.columns)\n",
    "# samples = cmap_tf.index.values\n",
    "\n",
    "# sampleInfo = pd.read_csv('../preprocessing/preprocessed_data/conditions_HT29_A375.csv',index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "97578e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a train generators\n",
    "def getSamples(N, batchSize):\n",
    "    order = np.random.permutation(N)\n",
    "    outList = []\n",
    "    while len(order)>0:\n",
    "        outList.append(order[0:batchSize])\n",
    "        order = order[batchSize:]\n",
    "    return outList"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "450d2946",
   "metadata": {},
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "88147589",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_params = {'encoder_1_hiddens':[640,384],\n",
    "                'encoder_2_hiddens':[640,384],\n",
    "                'latent_dim': 292,\n",
    "                'decoder_1_hiddens':[384,640],\n",
    "                'decoder_2_hiddens':[384,640],\n",
    "                'dropout_decoder':0.2,\n",
    "                'dropout_encoder':0.1,\n",
    "                'encoder_activation':torch.nn.ELU(),\n",
    "                'decoder_activation':torch.nn.ELU(),\n",
    "                'V_dropout':0.25,\n",
    "                'state_class_hidden':[256,128,64],\n",
    "                'state_class_drop_in':0.5,\n",
    "                'state_class_drop':0.25,\n",
    "                'no_states':2,\n",
    "                'adv_class_hidden':[256,128,64],\n",
    "                'adv_class_drop_in':0.3,\n",
    "                'adv_class_drop':0.1,\n",
    "                'no_adv_class':2,\n",
    "                'encoding_lr':0.001,\n",
    "                'adv_lr':0.001,\n",
    "                'schedule_step_adv':200,\n",
    "                'gamma_adv':0.5,\n",
    "                'schedule_step_enc':200,\n",
    "                'gamma_enc':0.8,\n",
    "                'batch_size_1':178,\n",
    "                'batch_size_2':154,\n",
    "                'batch_size_paired':90,\n",
    "                'epochs':1000,\n",
    "                'prior_beta':1.0,\n",
    "                'no_folds':10,\n",
    "                'v_reg':1e-04,\n",
    "                'state_class_reg':1e-02,\n",
    "                'enc_l2_reg':0.01,\n",
    "                'dec_l2_reg':0.01,\n",
    "                'lambda_mi_loss':100,\n",
    "                'effsize_reg': 100,\n",
    "                'cosine_loss': 10,\n",
    "                'adv_penalnty':100,\n",
    "                'reg_adv':1000,\n",
    "                'reg_classifier': 1000,\n",
    "                'similarity_reg' : 10,\n",
    "                'adversary_steps':4,\n",
    "                'autoencoder_wd': 0.,\n",
    "                'adversary_wd': 0.}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a54949",
   "metadata": {},
   "source": [
    "### Pre-train encoder and then classifier to have apre-trained discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3ca4127d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class_criterion = torch.nn.CrossEntropyLoss()\n",
    "# NUM_EPOCHS= 2000\n",
    "# #bs = 512\n",
    "# bs_1 = model_params['batch_size_1']\n",
    "# bs_2 =  model_params['batch_size_2']\n",
    "# bs_paired =  model_params['batch_size_paired']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8a6e4ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(10):\n",
    "#     # Network\n",
    "#     encoder = SimpleEncoder(gene_size,model_params['encoder_1_hiddens'],model_params['latent_dim'],\n",
    "#                               dropRate=model_params['dropout_encoder'], \n",
    "#                               activation=model_params['encoder_activation']).to(device)\n",
    "#     prior_d = PriorDiscriminator(model_params['latent_dim']).to(device)\n",
    "#     local_d = LocalDiscriminator(model_params['latent_dim'],model_params['latent_dim']).to(device)\n",
    "    \n",
    "#     adverse_classifier = Classifier(in_channel=model_params['latent_dim'],\n",
    "#                                     hidden_layers=model_params['adv_class_hidden'],\n",
    "#                                     num_classes=model_params['no_adv_class'],\n",
    "#                                     drop_in=0.5,\n",
    "#                                     drop=0.3).to(device)\n",
    "    \n",
    "#     trainInfo_paired = pd.read_csv('10fold_validation_spit/train_paired_%s.csv'%i,index_col=0)\n",
    "#     trainInfo_1 = pd.read_csv('10fold_validation_spit/train_a375_%s.csv'%i,index_col=0)\n",
    "#     trainInfo_2 = pd.read_csv('10fold_validation_spit/train_ht29_%s.csv'%i,index_col=0)\n",
    "    \n",
    "#     valInfo_paired = pd.read_csv('10fold_validation_spit/val_paired_%s.csv'%i,index_col=0)\n",
    "#     valInfo_1 = pd.read_csv('10fold_validation_spit/val_a375_%s.csv'%i,index_col=0)\n",
    "#     valInfo_2 = pd.read_csv('10fold_validation_spit/val_ht29_%s.csv'%i,index_col=0)\n",
    "    \n",
    "#     N_paired = len(trainInfo_paired)\n",
    "#     N_1 = len(trainInfo_1)\n",
    "#     N_2 = len(trainInfo_2)\n",
    "#     N = N_1\n",
    "#     if N_2>N:\n",
    "#         N=N_2\n",
    "    \n",
    "#     allParams = list(encoder.parameters())\n",
    "#     allParams = allParams + list(prior_d.parameters()) + list(local_d.parameters())\n",
    "#     allParams = allParams + list(adverse_classifier.parameters())\n",
    "#     optimizer = torch.optim.Adam(allParams, lr= model_params['encoding_lr'], weight_decay=0)\n",
    "#     scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n",
    "#                                                 step_size=model_params['schedule_step_enc'],\n",
    "#                                                 gamma=model_params['gamma_enc'])\n",
    "#     for e in range(0, NUM_EPOCHS):\n",
    "#         encoder.train()\n",
    "#         prior_d.train()\n",
    "#         local_d.train()\n",
    "#         adverse_classifier.train()\n",
    "        \n",
    "#         trainloader_1 = getSamples(N_1, bs_1)\n",
    "#         len_1 = len(trainloader_1)\n",
    "#         trainloader_2 = getSamples(N_2, bs_2)\n",
    "#         len_2 = len(trainloader_2)\n",
    "#         trainloader_paired = getSamples(N_paired, bs_paired)\n",
    "#         len_paired = len(trainloader_paired)\n",
    "\n",
    "#         lens = [len_1,len_2,len_paired]\n",
    "#         maxLen = np.max(lens)\n",
    "        \n",
    "#         if maxLen>lens[0]:\n",
    "#             trainloader_suppl = getSamples(N_1, bs_1)\n",
    "#             for jj in range(maxLen-lens[0]):\n",
    "#                 trainloader_1.insert(jj,trainloader_suppl[jj])\n",
    "        \n",
    "#         if maxLen>lens[1]:\n",
    "#             trainloader_suppl = getSamples(N_2, bs_2)\n",
    "#             for jj in range(maxLen-lens[1]):\n",
    "#                 trainloader_2.insert(jj,trainloader_suppl[jj])\n",
    "        \n",
    "#         if maxLen>lens[2]:\n",
    "#             trainloader_suppl = getSamples(N_paired, bs_paired)\n",
    "#             for jj in range(maxLen-lens[2]):\n",
    "#                 trainloader_paired.insert(jj,trainloader_suppl[jj])\n",
    "        \n",
    "#         for j in range(maxLen):\n",
    "#             dataIndex_1 = trainloader_1[j]\n",
    "#             dataIndex_2 = trainloader_2[j]\n",
    "#             dataIndex_paired = trainloader_paired[j]\n",
    "            \n",
    "#             df_pairs = trainInfo_paired.iloc[dataIndex_paired,:]\n",
    "#             df_1 = trainInfo_1.iloc[dataIndex_1,:]\n",
    "#             df_2 = trainInfo_2.iloc[dataIndex_2,:]\n",
    "#             paired_inds = len(df_pairs)\n",
    "            \n",
    "            \n",
    "#             X_1 = torch.tensor(np.concatenate((cmap.loc[df_pairs['sig_id.x']].values,\n",
    "#                                                  cmap.loc[df_1.sig_id].values))).float().to(device)\n",
    "#             X_2 = torch.tensor(np.concatenate((cmap.loc[df_pairs['sig_id.y']].values,\n",
    "#                                                  cmap.loc[df_2.sig_id].values))).float().to(device)\n",
    "            \n",
    "            \n",
    "#             conditions = np.concatenate((df_pairs.conditionId.values,\n",
    "#                                          df_1.conditionId.values,\n",
    "#                                          df_pairs.conditionId.values,\n",
    "#                                          df_2.conditionId.values))\n",
    "#             size = conditions.size\n",
    "#             conditions = conditions.reshape(size,1)\n",
    "#             conditions = conditions == conditions.transpose()\n",
    "#             conditions = conditions*1\n",
    "#             mask = torch.tensor(conditions).to(device).detach()\n",
    "#             pos_mask = mask\n",
    "#             neg_mask = 1 - mask\n",
    "#             log_2 = math.log(2.)\n",
    "#             optimizer.zero_grad()\n",
    "                        \n",
    "#             #if iteration % model_params[\"adversary_steps\"] == 0:            \n",
    "#             z_1 = encoder(X_1)\n",
    "#             z_2 = encoder(X_2)\n",
    "#             latent_vectors = torch.cat((z_1, z_2), 0)\n",
    "#             labels_adv = adverse_classifier(latent_vectors)\n",
    "#             true_labels = torch.cat((torch.ones(z_1.shape[0]),\n",
    "#                                      torch.zeros(z_2.shape[0])),0).long().to(device)\n",
    "#             adv_entropy = class_criterion(labels_adv,true_labels)\n",
    "#             _, predicted = torch.max(labels_adv, 1)\n",
    "#             predicted = predicted.cpu().numpy()\n",
    "#             cf_matrix = confusion_matrix(true_labels.cpu().numpy(),predicted)\n",
    "#             tn, fp, fn, tp = cf_matrix.ravel()\n",
    "#             f1 = 2*tp/(2*tp+fp+fn)\n",
    "            \n",
    "            \n",
    "#             #z_un = local_d(torch.cat((z_1, z_2), 0))\n",
    "#             z_un = local_d(latent_vectors)\n",
    "#             res_un = torch.matmul(z_un, z_un.t())\n",
    "            \n",
    "#             p_samples = res_un * pos_mask.float()\n",
    "#             q_samples = res_un * neg_mask.float()\n",
    "\n",
    "#             Ep = log_2 - F.softplus(- p_samples)\n",
    "#             Eq = F.softplus(-q_samples) + q_samples - log_2\n",
    "\n",
    "#             Ep = (Ep * pos_mask.float()).sum() / pos_mask.float().sum()\n",
    "#             Eq = (Eq * neg_mask.float()).sum() / neg_mask.float().sum()\n",
    "#             mi_loss = Eq - Ep\n",
    "\n",
    "#             #prior = torch.rand_like(torch.cat((z_1, z_2), 0))\n",
    "#             prior = torch.rand_like(latent_vectors)\n",
    "\n",
    "#             term_a = torch.log(prior_d(prior)).mean()\n",
    "#             term_b = torch.log(1.0 - prior_d(latent_vectors)).mean()\n",
    "#             prior_loss = -(term_a + term_b) * model_params['prior_beta']\n",
    "            \n",
    "#             # Remove signal from z_basal\n",
    "#             loss = mi_loss + prior_loss + adv_entropy +\\\n",
    "#                    adverse_classifier.L2Regularization(model_params['state_class_reg']) +\\\n",
    "#                    encoder.L2Regularization(model_params['enc_l2_reg'])\n",
    "                   \n",
    "\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "            \n",
    "#         scheduler.step()\n",
    "#         outString = 'Split {:.0f}: Epoch={:.0f}/{:.0f}'.format(i+1,e+1,NUM_EPOCHS)\n",
    "#         outString += ', MI Loss={:.4f}'.format(mi_loss.item())\n",
    "#         outString += ', Prior Loss={:.4f}'.format(prior_loss.item())\n",
    "#         outString += ', Entropy Loss={:.4f}'.format(adv_entropy.item())\n",
    "#         outString += ', loss={:.4f}'.format(loss.item())\n",
    "#         outString += ', F1 score={:.4f}'.format(f1)\n",
    "#         if (e%250==0):\n",
    "#             print(outString)\n",
    "#     print(outString)\n",
    "#     encoder.eval()\n",
    "#     prior_d.eval()\n",
    "#     local_d.eval()\n",
    "#     adverse_classifier.eval()\n",
    "    \n",
    "#     paired_val_inds = len(valInfo_paired)\n",
    "#     x_1 = torch.tensor(np.concatenate((cmap.loc[valInfo_paired['sig_id.x']].values,\n",
    "#                                           cmap.loc[valInfo_1.sig_id].values))).float().to(device)\n",
    "#     x_2 = torch.tensor(np.concatenate((cmap.loc[valInfo_paired['sig_id.y']].values,\n",
    "#                                           cmap.loc[valInfo_2.sig_id].values))).float().to(device)\n",
    "    \n",
    "    \n",
    "#     z_latent_1 = encoder(x_1)\n",
    "#     z_latent_2 = encoder(x_2)\n",
    "    \n",
    "    \n",
    "#     labels = adverse_classifier(torch.cat((z_latent_1, z_latent_2), 0))\n",
    "#     true_labels = torch.cat((torch.ones(z_latent_1.shape[0]).view(z_latent_1.shape[0],1),\n",
    "#                              torch.zeros(z_latent_2.shape[0]).view(z_latent_2.shape[0],1)),0).long()\n",
    "#     _, predicted = torch.max(labels, 1)\n",
    "#     predicted = predicted.cpu().numpy()\n",
    "#     cf_matrix = confusion_matrix(true_labels.numpy(),predicted)\n",
    "#     tn, fp, fn, tp = cf_matrix.ravel()\n",
    "#     class_acc = (tp+tn)/predicted.size\n",
    "#     f1 = 2*tp/(2*tp+fp+fn)\n",
    "        \n",
    "#     print('Classification accuracy: %s'%class_acc)\n",
    "#     print('Classification F1 score: %s'%f1)\n",
    "    \n",
    "#     torch.save(encoder,'../results/MI_results/models/CPA_approach/pre_trained_master_encoder_%s.pt'%i)\n",
    "#     torch.save(adverse_classifier,'../results/MI_results/models/CPA_approach/pre_trained_classifier_adverse_%s.pt'%i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab1b3b00",
   "metadata": {},
   "source": [
    "### Train the whole model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0160db4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradients(output, input):\n",
    "    grads = torch.autograd.grad(output, input, create_graph=True)\n",
    "    grads = grads[0].pow(2).mean()\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "080e7659",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_criterion = torch.nn.CrossEntropyLoss()\n",
    "NUM_EPOCHS= model_params['epochs']\n",
    "#bs = 512\n",
    "bs_1 = model_params['batch_size_1']\n",
    "bs_2 =  model_params['batch_size_2']\n",
    "bs_paired =  model_params['batch_size_paired']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "800e4027",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NUM_EPOCHS = 300\n",
    "# #bs = 128\n",
    "# bs_a375 = 45\n",
    "# bs_ht29 = 39\n",
    "# bs_paired = 22\n",
    "# beta = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3019e12a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "valR2 = []\n",
    "valPear = []\n",
    "valMSE =[]\n",
    "valSpear = []\n",
    "valAccuracy = []\n",
    "\n",
    "\n",
    "valPearDirect = []\n",
    "valSpearDirect = []\n",
    "valAccDirect = []\n",
    "\n",
    "valR2_1 = []\n",
    "valPear_1 = []\n",
    "valMSE_1 =[]\n",
    "valSpear_1 = []\n",
    "valAccuracy_1 = []\n",
    "\n",
    "valR2_2 = []\n",
    "valPear_2 = []\n",
    "valMSE_2 =[]\n",
    "valSpear_2 = []\n",
    "valAccuracy_2 = []\n",
    "\n",
    "crossCorrelation = []\n",
    "\n",
    "valF1 = []\n",
    "valClassAcc = []\n",
    "\n",
    "for i in range(10):\n",
    "    # Network\n",
    "    #encoder = torch.load('../results/MI_results/models/CPA_approach/pre_trained_master_encoder_2.pt')\n",
    "    #master_encoder = SimpleEncoder(gene_size,[640,384],292,dropRate=0.1, activation=torch.nn.ELU())#.to(device)\n",
    "    decoder_1 = Decoder(model_params['latent_dim'],model_params['decoder_1_hiddens'],gene_size,\n",
    "                        dropRate=model_params['dropout_decoder'], \n",
    "                        activation=model_params['decoder_activation']).to(device)\n",
    "    decoder_2 = Decoder(model_params['latent_dim'],model_params['decoder_2_hiddens'],gene_size,\n",
    "                        dropRate=model_params['dropout_decoder'], \n",
    "                        activation=model_params['decoder_activation']).to(device)\n",
    "    encoder_1 = SimpleEncoder(gene_size,model_params['encoder_1_hiddens'],model_params['latent_dim'],\n",
    "                              dropRate=model_params['dropout_encoder'], \n",
    "                              activation=model_params['encoder_activation']).to(device)\n",
    "    #encoder_1.load_state_dict(encoder.state_dict())\n",
    "    encoder_2 = SimpleEncoder(gene_size,model_params['encoder_2_hiddens'],model_params['latent_dim'],\n",
    "                                  dropRate=model_params['dropout_encoder'], \n",
    "                                  activation=model_params['encoder_activation']).to(device)\n",
    "    #encoder_2.load_state_dict(encoder.state_dict())\n",
    "    prior_d = PriorDiscriminator(model_params['latent_dim']).to(device)\n",
    "    local_d = LocalDiscriminator(model_params['latent_dim'],model_params['latent_dim']).to(device)\n",
    "    \n",
    "    classifier = Classifier(in_channel=model_params['latent_dim'],\n",
    "                            hidden_layers=model_params['state_class_hidden'],\n",
    "                            num_classes=model_params['no_states'],\n",
    "                            drop_in=model_params['state_class_drop_in'],\n",
    "                            drop=model_params['state_class_drop']).to(device)\n",
    "    adverse_classifier = Classifier(in_channel=model_params['latent_dim'],\n",
    "                                    hidden_layers=model_params['adv_class_hidden'],\n",
    "                                    num_classes=model_params['no_adv_class'],\n",
    "                                    drop_in=model_params['adv_class_drop_in'],\n",
    "                                    drop=model_params['adv_class_drop']).to(device)\n",
    "    pretrained_adv_class = torch.load('../results/MI_results/models/CPA_approach/pre_trained_classifier_adverse_0.pt')\n",
    "    adverse_classifier.load_state_dict(pretrained_adv_class.state_dict())\n",
    "    \n",
    "    Vsp = SpeciesCovariate(2,model_params['latent_dim'],dropRate=model_params['V_dropout']).to(device)\n",
    "    \n",
    "    trainInfo_paired = pd.read_csv('../preprocessing/preprocessed_data/10fold_validation_spit/train_paired_%s.csv'%i,index_col=0)\n",
    "    trainInfo_1 = pd.read_csv('../preprocessing/preprocessed_data/10fold_validation_spit/train_a375_%s.csv'%i,index_col=0)\n",
    "    trainInfo_2 = pd.read_csv('../preprocessing/preprocessed_data/10fold_validation_spit/train_ht29_%s.csv'%i,index_col=0)\n",
    "    \n",
    "    valInfo_paired = pd.read_csv('../preprocessing/preprocessed_data/10fold_validation_spit/val_paired_%s.csv'%i,index_col=0)\n",
    "    valInfo_1 = pd.read_csv('../preprocessing/preprocessed_data/10fold_validation_spit/val_a375_%s.csv'%i,index_col=0)\n",
    "    valInfo_2 = pd.read_csv('../preprocessing/preprocessed_data/10fold_validation_spit/val_ht29_%s.csv'%i,index_col=0)\n",
    "\n",
    "    #trainInfo_2 = pd.concat((trainInfo_2,\n",
    "    #                         valInfo_2,\n",
    "    #                         valInfo_paired.loc[:,['sig_id.y','cell_iname.y','conditionId']].rename(columns = {'sig_id.y':'sig_id','cell_iname.y':'cell_iname'})),0)\n",
    "    \n",
    "    N_paired = len(trainInfo_paired)\n",
    "    N_1 = len(trainInfo_1)\n",
    "    N_2 = len(trainInfo_2)\n",
    "    N = N_1\n",
    "    if N_2>N:\n",
    "        N=N_2\n",
    "    \n",
    "    allParams = list(decoder_1.parameters()) +list(decoder_2.parameters())\n",
    "    allParams = allParams + list(encoder_1.parameters()) +list(encoder_2.parameters())\n",
    "    allParams = allParams + list(prior_d.parameters()) + list(local_d.parameters())\n",
    "    allParams = allParams + list(classifier.parameters())\n",
    "    allParams = allParams + list(Vsp.parameters())\n",
    "    optimizer = torch.optim.Adam(allParams, lr= model_params['encoding_lr'],\n",
    "                                 weight_decay=model_params['autoencoder_wd'])\n",
    "    optimizer_adv = torch.optim.Adam(adverse_classifier.parameters(), lr= model_params['adv_lr'], \n",
    "                                     weight_decay=model_params['adversary_wd'])\n",
    "    if model_params['schedule_step_adv'] is not None:\n",
    "        scheduler_adv = torch.optim.lr_scheduler.StepLR(optimizer_adv,\n",
    "                                                        step_size=model_params['schedule_step_adv'],\n",
    "                                                        gamma=model_params['gamma_adv'])\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n",
    "                                                step_size=model_params['schedule_step_enc'],\n",
    "                                                gamma=model_params['gamma_enc'])\n",
    "    iteration = 1\n",
    "    for e in range(0, NUM_EPOCHS):\n",
    "        decoder_1.train()\n",
    "        decoder_2.train()\n",
    "        encoder_1.train()\n",
    "        encoder_2.train()\n",
    "        prior_d.train()\n",
    "        local_d.train()\n",
    "        classifier.train()\n",
    "        adverse_classifier.train()\n",
    "        Vsp.train()\n",
    "        #master_encoder.train()\n",
    "        \n",
    "        trainloader_1 = getSamples(N_1, bs_1)\n",
    "        len_1 = len(trainloader_1)\n",
    "        trainloader_2 = getSamples(N_2, bs_2)\n",
    "        len_2 = len(trainloader_2)\n",
    "        trainloader_paired = getSamples(N_paired, bs_paired)\n",
    "        len_paired = len(trainloader_paired)\n",
    "\n",
    "        lens = [len_1,len_2,len_paired]\n",
    "        maxLen = np.max(lens)\n",
    "        \n",
    "        iteration = 1\n",
    "\n",
    "        if maxLen>lens[0]:\n",
    "            trainloader_suppl = getSamples(N_1, bs_1)\n",
    "            for jj in range(maxLen-lens[0]):\n",
    "                trainloader_1.insert(jj,trainloader_suppl[jj])\n",
    "        \n",
    "        if maxLen>lens[1]:\n",
    "            trainloader_suppl = getSamples(N_2, bs_2)\n",
    "            for jj in range(maxLen-lens[1]):\n",
    "                trainloader_2.insert(jj,trainloader_suppl[jj])\n",
    "        \n",
    "        if maxLen>lens[2]:\n",
    "            trainloader_suppl = getSamples(N_paired, bs_paired)\n",
    "            for jj in range(maxLen-lens[2]):\n",
    "                trainloader_paired.insert(jj,trainloader_suppl[jj])\n",
    "        #for dataIndex in trainloader:\n",
    "        for j in range(maxLen):\n",
    "            dataIndex_1 = trainloader_1[j]\n",
    "            dataIndex_2 = trainloader_2[j]\n",
    "            dataIndex_paired = trainloader_paired[j]\n",
    "            \n",
    "            df_pairs = trainInfo_paired.iloc[dataIndex_paired,:]\n",
    "            df_1 = trainInfo_1.iloc[dataIndex_1,:]\n",
    "            df_2 = trainInfo_2.iloc[dataIndex_2,:]\n",
    "            paired_inds = len(df_pairs)\n",
    "            \n",
    "            \n",
    "            X_1 = torch.tensor(np.concatenate((cmap.loc[df_pairs['sig_id.x']].values,\n",
    "                                                 cmap.loc[df_1.sig_id].values))).float().to(device)\n",
    "            X_2 = torch.tensor(np.concatenate((cmap.loc[df_pairs['sig_id.y']].values,\n",
    "                                                 cmap.loc[df_2.sig_id].values))).float().to(device)\n",
    "            \n",
    "            z_species_1 = torch.cat((torch.ones(X_1.shape[0],1),\n",
    "                                     torch.zeros(X_1.shape[0],1)),1).to(device)\n",
    "            z_species_2 = torch.cat((torch.zeros(X_2.shape[0],1),\n",
    "                                     torch.ones(X_2.shape[0],1)),1).to(device)\n",
    "            \n",
    "            \n",
    "            conditions = np.concatenate((df_pairs.conditionId.values,\n",
    "                                         df_1.conditionId.values,\n",
    "                                         df_pairs.conditionId.values,\n",
    "                                         df_2.conditionId.values))\n",
    "            size = conditions.size\n",
    "            conditions = conditions.reshape(size,1)\n",
    "            conditions = conditions == conditions.transpose()\n",
    "            conditions = conditions*1\n",
    "            mask = torch.tensor(conditions).to(device).detach()\n",
    "            pos_mask = mask\n",
    "            neg_mask = 1 - mask\n",
    "            log_2 = math.log(2.)\n",
    "            optimizer.zero_grad()\n",
    "            optimizer_adv.zero_grad()\n",
    "                        \n",
    "            #if e % model_params['adversary_steps']==0:      \n",
    "            z_base_1 = encoder_1(X_1)\n",
    "            z_base_2 = encoder_2(X_2)\n",
    "            latent_base_vectors = torch.cat((z_base_1, z_base_2), 0)\n",
    "            labels_adv = adverse_classifier(latent_base_vectors)\n",
    "            true_labels = torch.cat((torch.ones(z_base_1.shape[0]),\n",
    "                                         torch.zeros(z_base_2.shape[0])),0).long().to(device)\n",
    "            _, predicted = torch.max(labels_adv, 1)\n",
    "            predicted = predicted.cpu().numpy()\n",
    "            cf_matrix = confusion_matrix(true_labels.cpu().numpy(),predicted)\n",
    "            tn, fp, fn, tp = cf_matrix.ravel()\n",
    "            f1_basal_trained = 2*tp/(2*tp+fp+fn)\n",
    "            adv_entropy = class_criterion(labels_adv,true_labels)\n",
    "            adversary_drugs_penalty = compute_gradients(labels_adv.sum(), latent_base_vectors)\n",
    "            loss_adv = adv_entropy + model_params['adv_penalnty'] * adversary_drugs_penalty\n",
    "            loss_adv.backward()\n",
    "            optimizer_adv.step()\n",
    "            #else:\n",
    "            optimizer.zero_grad()\n",
    "            #f1_basal_trained = None\n",
    "            z_base_1 = encoder_1(X_1)\n",
    "            z_base_2 = encoder_2(X_2)\n",
    "            latent_base_vectors = torch.cat((z_base_1, z_base_2), 0)\n",
    "            \n",
    "            #z_un = local_d(torch.cat((z_1, z_2), 0))\n",
    "            z_un = local_d(latent_base_vectors)\n",
    "            res_un = torch.matmul(z_un, z_un.t())\n",
    "            \n",
    "            z_1 = Vsp(z_base_1,z_species_1)\n",
    "            z_2 = Vsp(z_base_2,z_species_2)\n",
    "            latent_vectors = torch.cat((z_1, z_2), 0)\n",
    "            \n",
    "            y_pred_1 = decoder_1(z_1)\n",
    "            fitLoss_1 = torch.mean(torch.sum((y_pred_1 - X_1)**2,dim=1))\n",
    "            L2Loss_1 = decoder_1.L2Regularization(model_params['dec_l2_reg']) + encoder_1.L2Regularization(model_params['enc_l2_reg'])\n",
    "            loss_1 = fitLoss_1 + L2Loss_1\n",
    "            \n",
    "            y_pred_2 = decoder_2(z_2)\n",
    "            fitLoss_2 = torch.mean(torch.sum((y_pred_2 - X_2)**2,dim=1))\n",
    "            L2Loss_2 = decoder_2.L2Regularization(model_params['dec_l2_reg']) + encoder_2.L2Regularization(model_params['enc_l2_reg'])\n",
    "            loss_2 = fitLoss_2 + L2Loss_2\n",
    "\n",
    "            #silimalityLoss = np.sqrt(paired_inds)*torch.mean(torch.sum((z_base_1[0:paired_inds,:] - z_base_2[0:paired_inds,:])**2,\n",
    "            #                                      dim=-1))/torch.std(torch.sum((z_base_1[0:paired_inds,:] - z_base_2[0:paired_inds,:])**2,\n",
    "            #                                                                   dim=-1))\n",
    "            silimalityLoss = torch.mean(torch.sum((z_base_1[0:paired_inds,:] - z_base_2[0:paired_inds,:])**2,dim=-1))\n",
    "            #NN1 = z_base_1[paired_inds:,:].shape[0]\n",
    "            #NN2 = z_base_2[paired_inds:,:].shape[0]\n",
    "            #if NN1>NN2:\n",
    "            #    n1 = NN2\n",
    "            #else:\n",
    "            #    n1 = NN1\n",
    "            #effect_size = torch.mean(torch.sum((z_base_1[paired_inds:(paired_inds+n1),:].detach() - z_base_2[paired_inds:(paired_inds+n1),:].detach())**2,dim=-1)).detach() - silimalityLoss.detach()\n",
    "            #s1 = torch.std(torch.sum((z_base_1[paired_inds:(paired_inds+n1),:].detach() - z_base_2[paired_inds:(paired_inds+n1),:].detach())**2,dim=-1)).detach()\n",
    "            #s2 = torch.std(torch.sum((z_base_1[0:paired_inds,:].detach() - z_base_2[0:paired_inds,:].detach())**2,dim=-1)).detach()\n",
    "            #n2 = paired_inds\n",
    "            #s_pooled = torch.sqrt(((n1-1)* s1**2 + (n2-1)* s2**2)/(n1+n2-2)).detach()\n",
    "            #effect_size = effect_size/s_pooled\n",
    "            cosineLoss = torch.nn.functional.cosine_similarity(z_base_1[0:paired_inds,:],z_base_2[0:paired_inds,:],dim=-1).mean()\n",
    "            \n",
    "            p_samples = res_un * pos_mask.float()\n",
    "            q_samples = res_un * neg_mask.float()\n",
    "    \n",
    "            Ep = log_2 - F.softplus(- p_samples)\n",
    "            Eq = F.softplus(-q_samples) + q_samples - log_2\n",
    "\n",
    "            Ep = (Ep * pos_mask.float()).sum() / pos_mask.float().sum()\n",
    "            Eq = (Eq * neg_mask.float()).sum() / neg_mask.float().sum()\n",
    "            mi_loss = Eq - Ep\n",
    "\n",
    "            #prior = torch.rand_like(torch.cat((z_1, z_2), 0))\n",
    "            prior = torch.rand_like(latent_base_vectors)\n",
    "\n",
    "            term_a = torch.log(prior_d(prior)).mean()\n",
    "            #term_b = torch.log(1.0 - prior_d(torch.cat((z_1, z_2), 0))).mean()\n",
    "            term_b = torch.log(1.0 - prior_d(latent_base_vectors)).mean()\n",
    "            prior_loss = -(term_a + term_b) * model_params['prior_beta']\n",
    "\n",
    "            # Classification loss\n",
    "            labels = classifier(latent_vectors)\n",
    "            true_labels = torch.cat((torch.ones(z_1.shape[0]),\n",
    "                                     torch.zeros(z_2.shape[0])),0).long().to(device)\n",
    "            entropy = class_criterion(labels,true_labels)\n",
    "            _, predicted = torch.max(labels, 1)\n",
    "            predicted = predicted.cpu().numpy()\n",
    "            cf_matrix = confusion_matrix(true_labels.cpu().numpy(),predicted)\n",
    "            tn, fp, fn, tp = cf_matrix.ravel()\n",
    "            f1_latent = 2*tp/(2*tp+fp+fn)\n",
    "            \n",
    "            # Remove signal from z_basal\n",
    "            labels_adv = adverse_classifier(latent_base_vectors)\n",
    "            true_labels = torch.cat((torch.ones(z_base_1.shape[0]),\n",
    "                                     torch.zeros(z_base_2.shape[0])),0).long().to(device)\n",
    "            adv_entropy = class_criterion(labels_adv,true_labels)\n",
    "            _, predicted = torch.max(labels_adv, 1)\n",
    "            predicted = predicted.cpu().numpy()\n",
    "            cf_matrix = confusion_matrix(true_labels.cpu().numpy(),predicted)\n",
    "            tn, fp, fn, tp = cf_matrix.ravel()\n",
    "            f1_basal = 2*tp/(2*tp+fp+fn)\n",
    "            \n",
    "            #loss = loss_1 + loss_2 + 100*mi_loss + prior_loss + 10*silimalityLoss-100*cosineLoss + 1000*entropy +\\\n",
    "            #       classifier.L2Regularization(1e-2) + Vsp.Regularization() - 1000*adv_entropy \n",
    "            loss = loss_1 + loss_2 + model_params['similarity_reg'] * silimalityLoss +\\\n",
    "                   model_params['lambda_mi_loss']*mi_loss + prior_loss +\\\n",
    "                   model_params['reg_classifier']*entropy - model_params['reg_adv']*adv_entropy +\\\n",
    "                   classifier.L2Regularization(model_params['state_class_reg']) +\\\n",
    "                   Vsp.Regularization(model_params['v_reg']) - model_params['cosine_loss'] * cosineLoss\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        \n",
    "            pearson_1 = pearson_r(y_pred_1.detach().flatten(), X_1.detach().flatten())\n",
    "            r2_1 = r_square(y_pred_1.detach().flatten(), X_1.detach().flatten())\n",
    "            mse_1 = torch.mean(torch.mean((y_pred_1.detach() - X_1.detach())**2,dim=1))\n",
    "        \n",
    "            pearson_2 = pearson_r(y_pred_2.detach().flatten(), X_2.detach().flatten())\n",
    "            r2_2 = r_square(y_pred_2.detach().flatten(), X_2.detach().flatten())\n",
    "            mse_2 = torch.mean(torch.mean((y_pred_2.detach() - X_2.detach())**2,dim=1))\n",
    "            \n",
    "                #iteration += iteration\n",
    "            \n",
    "            \n",
    "        #if iteration % model_params[\"adversary_steps\"] == 0 and model_params['schedule_step_adv'] is not None:\n",
    "        if model_params['schedule_step_adv'] is not None:\n",
    "            scheduler_adv.step()\n",
    "        if (e>=0):\n",
    "            scheduler.step()\n",
    "            outString = 'Split {:.0f}: Epoch={:.0f}/{:.0f}'.format(i+1,e+1,NUM_EPOCHS)\n",
    "            outString += ', r2_1={:.4f}'.format(r2_1.item())\n",
    "            outString += ', pearson_1={:.4f}'.format(pearson_1.item())\n",
    "            outString += ', MSE_1={:.4f}'.format(mse_1.item())\n",
    "            outString += ', r2_2={:.4f}'.format(r2_2.item())\n",
    "            outString += ', pearson_2={:.4f}'.format(pearson_2.item())\n",
    "            outString += ', MSE_2={:.4f}'.format(mse_2.item())\n",
    "            outString += ', MI Loss={:.4f}'.format(mi_loss.item())\n",
    "            outString += ', Prior Loss={:.4f}'.format(prior_loss.item())\n",
    "            outString += ', Entropy Loss={:.4f}'.format(entropy.item())\n",
    "            outString += ', Adverse Entropy={:.4f}'.format(adv_entropy.item())\n",
    "            outString += ', Cosine Loss={:.4f}'.format(cosineLoss.item())\n",
    "            outString += ', loss={:.4f}'.format(loss.item())\n",
    "            outString += ', F1 latent={:.4f}'.format(f1_latent)\n",
    "            outString += ', F1 basal={:.4f}'.format(f1_basal)\n",
    "            #if e % model_params[\"adversary_steps\"] == 0 and e>0:\n",
    "            outString += ', F1 basal trained={:.4f}'.format(f1_basal_trained)\n",
    "            #else:\n",
    "            #    outString += ', F1 basal trained= %s'%f1_basal_trained\n",
    "            #outString += ', Cohens d={:.4f}'.format(effect_size.item())\n",
    "        if (e==0 or (e%250==0 and e>0)):\n",
    "            print(outString)\n",
    "    print(outString)\n",
    "    #trainLoss.append(splitLoss)\n",
    "    decoder_1.eval()\n",
    "    decoder_2.eval()\n",
    "    encoder_1.eval()\n",
    "    encoder_2.eval()\n",
    "    prior_d.eval()\n",
    "    local_d.eval()\n",
    "    classifier.eval()\n",
    "    adverse_classifier.eval()\n",
    "    Vsp.eval()\n",
    "    #model.eval()\n",
    "    #master_encoder.eval()\n",
    "    \n",
    "    paired_val_inds = len(valInfo_paired)\n",
    "    x_1 = torch.tensor(np.concatenate((cmap.loc[valInfo_paired['sig_id.x']].values,\n",
    "                                          cmap.loc[valInfo_1.sig_id].values))).float().to(device)\n",
    "    x_2 = torch.tensor(np.concatenate((cmap.loc[valInfo_paired['sig_id.y']].values,\n",
    "                                          cmap.loc[valInfo_2.sig_id].values))).float().to(device)\n",
    "    \n",
    "    z_species_1 = torch.cat((torch.ones(x_1.shape[0],1),\n",
    "                             torch.zeros(x_1.shape[0],1)),1).to(device)\n",
    "    z_species_2 = torch.cat((torch.zeros(x_2.shape[0],1),\n",
    "                             torch.ones(x_2.shape[0],1)),1).to(device)\n",
    "\n",
    "    #z_latent_1 = encoder_1(x_1)\n",
    "    #z_latent_2 = encoder_2(x_2)\n",
    "    \n",
    "    z_latent_base_1 = encoder_1(x_1)\n",
    "    z_latent_base_2 = encoder_2(x_2)\n",
    "    \n",
    "    z_latent_1 = Vsp(z_latent_base_1,z_species_1)\n",
    "    z_latent_2 = Vsp(z_latent_base_2,z_species_2)\n",
    "    \n",
    "    labels = classifier(torch.cat((z_latent_1, z_latent_2), 0))\n",
    "    true_labels = torch.cat((torch.ones(z_latent_1.shape[0]).view(z_latent_1.shape[0],1),\n",
    "                             torch.zeros(z_latent_2.shape[0]).view(z_latent_2.shape[0],1)),0).long()\n",
    "    _, predicted = torch.max(labels, 1)\n",
    "    predicted = predicted.cpu().numpy()\n",
    "    cf_matrix = confusion_matrix(true_labels.numpy(),predicted)\n",
    "    tn, fp, fn, tp = cf_matrix.ravel()\n",
    "    class_acc = (tp+tn)/predicted.size\n",
    "    f1 = 2*tp/(2*tp+fp+fn)\n",
    "    \n",
    "    valF1.append(f1)\n",
    "    valClassAcc.append(class_acc)\n",
    "    \n",
    "    print('Classification accuracy: %s'%class_acc)\n",
    "    print('Classification F1 score: %s'%f1)\n",
    "\n",
    "    xhat_1 = decoder_1(z_latent_1)\n",
    "    xhat_2 = decoder_2(z_latent_2)\n",
    "\n",
    "    \n",
    "    r2_1 = r_square(xhat_1.detach().flatten(), x_1.detach().flatten())\n",
    "    pearson_1 = pearson_r(xhat_1.detach().flatten(), x_1.detach().flatten())\n",
    "    mse_1 = torch.mean(torch.mean((xhat_1 - x_1)**2,dim=1))\n",
    "    r2_2 = r_square(xhat_2.detach().flatten(), x_2.detach().flatten())\n",
    "    pearson_2 = pearson_r(xhat_2.detach().flatten(), x_2.detach().flatten())\n",
    "    mse_2 = torch.mean(torch.mean((xhat_2 - x_2)**2,dim=1))\n",
    "    rhos = []\n",
    "    for jj in range(xhat_1.shape[0]):\n",
    "        rho,p = spearmanr(x_1[jj,:].detach().cpu().numpy(),xhat_1[jj,:].detach().cpu().numpy())\n",
    "        rhos.append(rho)\n",
    "    valSpear_1.append(np.mean(rhos))\n",
    "    acc = pseudoAccuracy(x_1.detach().cpu(),xhat_1.detach().cpu(),eps=1e-6)\n",
    "    valAccuracy_1.append(np.mean(acc))\n",
    "    rhos = []\n",
    "    for jj in range(xhat_2.shape[0]):\n",
    "        rho,p = spearmanr(x_2[jj,:].detach().cpu().numpy(),xhat_2[jj,:].detach().cpu().numpy())\n",
    "        rhos.append(rho)\n",
    "    valSpear_2.append(np.mean(rhos))\n",
    "    acc = pseudoAccuracy(x_2.detach().cpu(),xhat_2.detach().cpu(),eps=1e-6)\n",
    "    valAccuracy_2.append(np.mean(acc))\n",
    "    \n",
    "    valR2_1.append(r2_1.item())\n",
    "    valPear_1.append(pearson_1.item())\n",
    "    valMSE_1.append(mse_1.item())\n",
    "    valR2_2.append(r2_2.item())\n",
    "    valPear_2.append(pearson_2.item())\n",
    "    valMSE_2.append(mse_2.item())\n",
    "    #print('R^2 1: %s'%r2_1.item())\n",
    "    print('Pearson correlation 1: %s'%pearson_1.item())\n",
    "    print('Spearman correlation 1: %s'%valSpear_1[i])\n",
    "    print('Pseudo-Accuracy 1: %s'%valAccuracy_1[i])\n",
    "    #print('R^2 2: %s'%r2_2.item())\n",
    "    print('Pearson correlation 2: %s'%pearson_2.item())\n",
    "    print('Spearman correlation 2: %s'%valSpear_2[i])\n",
    "    print('Pseudo-Accuracy 2: %s'%valAccuracy_2[i])\n",
    "    \n",
    "    \n",
    "    #x_1_equivalent = torch.tensor(cmap_val.loc[mask.index[np.where(mask>0)[0]],:].values).float().to(device)\n",
    "    #x_2_equivalent = torch.tensor(cmap_val.loc[mask.columns[np.where(mask>0)[1]],:].values).float().to(device)\n",
    "    x_1_equivalent = x_1[0:paired_val_inds,:]\n",
    "    x_2_equivalent = x_2[0:paired_val_inds,:]\n",
    "    \n",
    "    z_species_1_equivalent = z_species_1[0:paired_val_inds,:]\n",
    "    z_species_2_equivalent = z_species_2[0:paired_val_inds,:]\n",
    "    \n",
    "    pearDirect = pearson_r(x_1_equivalent.detach().flatten(), x_2_equivalent.detach().flatten())\n",
    "    rhos = []\n",
    "    for jj in range(x_1_equivalent.shape[0]):\n",
    "        rho,p = spearmanr(x_1_equivalent[jj,:].detach().cpu().numpy(),x_2_equivalent[jj,:].detach().cpu().numpy())\n",
    "        rhos.append(rho)\n",
    "    spearDirect = np.mean(rhos)\n",
    "    accDirect_2 = np.mean(pseudoAccuracy(x_2_equivalent.detach().cpu(),x_1_equivalent.detach().cpu(),eps=1e-6))\n",
    "    accDirect_1 = np.mean(pseudoAccuracy(x_1_equivalent.detach().cpu(),x_2_equivalent.detach().cpu(),eps=1e-6))\n",
    "\n",
    "    z_latent_base_1_equivalent  = encoder_1(x_1_equivalent)\n",
    "    z_latent_1_equivalent = Vsp(z_latent_base_1_equivalent,1.-z_species_1_equivalent)\n",
    "    x_hat_2_equivalent = decoder_2(z_latent_1_equivalent).detach()\n",
    "    r2_2 = r_square(x_hat_2_equivalent.detach().flatten(), x_2_equivalent.detach().flatten())\n",
    "    pearson_2 = pearson_r(x_hat_2_equivalent.detach().flatten(), x_2_equivalent.detach().flatten())\n",
    "    mse_2 = torch.mean(torch.mean((x_hat_2_equivalent - x_2_equivalent)**2,dim=1))\n",
    "    rhos = []\n",
    "    for jj in range(x_hat_2_equivalent.shape[0]):\n",
    "        rho,p = spearmanr(x_2_equivalent[jj,:].detach().cpu().numpy(),x_hat_2_equivalent[jj,:].detach().cpu().numpy())\n",
    "        rhos.append(rho)\n",
    "    rho_2 = np.mean(rhos)\n",
    "    acc_2 = np.mean(pseudoAccuracy(x_2_equivalent.detach().cpu(),x_hat_2_equivalent.detach().cpu(),eps=1e-6))\n",
    "    print('Pearson of direct translation: %s'%pearDirect.item())\n",
    "    print('Pearson correlation 1 to 2: %s'%pearson_2.item())\n",
    "    print('Pseudo accuracy 1 to 2: %s'%acc_2)\n",
    "\n",
    "    z_latent_base_2_equivalent  = encoder_2(x_2_equivalent)\n",
    "    z_latent_2_equivalent = Vsp(z_latent_base_2_equivalent,1.-z_species_2_equivalent)\n",
    "    x_hat_1_equivalent = decoder_1(z_latent_2_equivalent).detach()\n",
    "    r2_1 = r_square(x_hat_1_equivalent.detach().flatten(), x_1_equivalent.detach().flatten())\n",
    "    pearson_1 = pearson_r(x_hat_1_equivalent.detach().flatten(), x_1_equivalent.detach().flatten())\n",
    "    mse_1 = torch.mean(torch.mean((x_hat_1_equivalent - x_1_equivalent)**2,dim=1))\n",
    "    rhos = []\n",
    "    for jj in range(x_hat_1_equivalent.shape[0]):\n",
    "        rho,p = spearmanr(x_1_equivalent[jj,:].detach().cpu().numpy(),x_hat_1_equivalent[jj,:].detach().cpu().numpy())\n",
    "        rhos.append(rho)\n",
    "    rho_1 = np.mean(rhos)\n",
    "    acc_1 = np.mean(pseudoAccuracy(x_1_equivalent.detach().cpu(),x_hat_1_equivalent.detach().cpu(),eps=1e-6))\n",
    "    print('Pearson correlation 2 to 1: %s'%pearson_1.item())\n",
    "    print('Pseudo accuracy 2 to 1: %s'%acc_1)\n",
    "    \n",
    "    \n",
    "    valPear.append([pearson_2.item(),pearson_1.item()])\n",
    "    valSpear.append([rho_2,rho_1])\n",
    "    valAccuracy.append([acc_2,acc_1])\n",
    "    \n",
    "    valPearDirect.append(pearDirect.item())\n",
    "    valSpearDirect.append(spearDirect)\n",
    "    valAccDirect.append([accDirect_2,accDirect_1])\n",
    "    \n",
    "    torch.save(decoder_1,'../results/MI_results/models/CPA_approach/decoder_a375v3_%s.pt'%i)\n",
    "    torch.save(decoder_2,'../results/MI_results/models/CPA_approach/decoder_ht29v3_%s.pt'%i)\n",
    "    torch.save(prior_d,'../results/MI_results/models/CPA_approach/priorDiscrv3_%s.pt'%i)\n",
    "    torch.save(local_d,'../results/MI_results/models/CPA_approach/localDiscrv3_%s.pt'%i)\n",
    "    torch.save(encoder_1,'../results/MI_results/models/CPA_approach/encoder_a375v3_%s.pt'%i)\n",
    "    torch.save(encoder_2,'../results/MI_results/models/CPA_approach/encoder_ht29v3_%s.pt'%i)\n",
    "    torch.save(classifier,'../results/MI_results/models/CPA_approach/classifierv3_%s.pt'%i)\n",
    "    torch.save(Vsp,'../results/MI_results/models/CPA_approach/Vspecies3_%s.pt'%i)\n",
    "    torch.save(adverse_classifier,'../results/MI_results/models/CPA_approach/classifier_adversev3_%s.pt'%i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e07040e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss = loss_1 + loss_2 + 100*mi_loss + prior_loss + 10*silimalityLoss-100*cosineLoss + 1000*entropy +\\\n",
    "#                    classifier.L2Regularization(1e-2) + Vsp.Regularization() - 1000*adv_entropy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "465d75a5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<seaborn.axisgrid.FacetGrid at 0x1b12fa90c40>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.displot(torch.cat((z_1, z_2), 0).flatten().detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "822cedd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<seaborn.axisgrid.FacetGrid at 0x1b1b1da6f70>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.displot(torch.cat((z_base_1, z_base_2), 0).flatten().detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "77e43a7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([  3.,   5.,   6.,   7., 117., 425.,   7.,   3.,   5.,   6.]),\n",
       " array([-0.3950122 , -0.31679052, -0.23856887, -0.16034721, -0.08212554,\n",
       "        -0.00390388,  0.07431778,  0.15253945,  0.23076111,  0.3089828 ,\n",
       "         0.38720444], dtype=float32),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(Vsp.Vspecies.weight.flatten().detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bcea6576",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(autoencoder_a375,'../results/my_results/vae_a375.pt')\n",
    "# torch.save(autoencoder_ht29,'../results/my_results/vae_ht29.pt')\n",
    "# #torch.save(decoder_a375,'../results/my_results/decoder_a375.pt')\n",
    "# #torch.save(decoder_ht29,'../results/my_results/decoder_ht29.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e0664f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "valPear = np.array(valPear)\n",
    "valPearDirect = np.array(valPearDirect)\n",
    "crossCorrelation = np.array(crossCorrelation)\n",
    "valSpear = np.array(valSpear)\n",
    "valAccuracy= np.array(valAccuracy)\n",
    "valSpearDirect= np.array(valSpearDirect)\n",
    "valAccDirect= np.array(valAccDirect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "095f4069",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.69996376 0.6449095 ]\n",
      "0.45795060992240905\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(valPear,axis=0))\n",
    "print(np.mean(valPearDirect))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bfcd7c89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.60007563 0.56973341]\n",
      "0.40821883215419613\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(valSpear,axis=0))\n",
    "print(np.mean(valSpearDirect))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bf52abc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.71015849 0.70111452]\n",
      "[0.64107618 0.64107618]\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(valAccuracy,axis=0))\n",
    "print(np.mean(valAccDirect,axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c570de55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(valF1))\n",
    "print(np.mean(valClassAcc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4591319a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(np.mean(valPear,axis=0))\n",
    "# print(np.mean(valPearMean,axis=0))\n",
    "# print(np.mean(valPearUniform,axis=0))\n",
    "# print(np.mean(valPearScrampled,axis=0))\n",
    "# print(np.mean(valPearDirect))\n",
    "# print(np.mean(crossCorrelation,axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9847439a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>F1_score</th>\n",
       "      <th>ClassAccuracy</th>\n",
       "      <th>model_pearsonHT29</th>\n",
       "      <th>model_pearsonA375</th>\n",
       "      <th>model_spearHT29</th>\n",
       "      <th>model_spearA375</th>\n",
       "      <th>model_accHT29</th>\n",
       "      <th>model_accA375</th>\n",
       "      <th>recon_pear_ht29</th>\n",
       "      <th>recon_pear_a375</th>\n",
       "      <th>recon_spear_ht29</th>\n",
       "      <th>recon_spear_a375</th>\n",
       "      <th>recon_acc_ht29</th>\n",
       "      <th>recon_acc_a375</th>\n",
       "      <th>Direct_pearson</th>\n",
       "      <th>Direct_spearman</th>\n",
       "      <th>DirectAcc_ht29</th>\n",
       "      <th>DirectAcc_a375</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.748481</td>\n",
       "      <td>0.673342</td>\n",
       "      <td>0.662073</td>\n",
       "      <td>0.604682</td>\n",
       "      <td>0.732797</td>\n",
       "      <td>0.717510</td>\n",
       "      <td>0.831326</td>\n",
       "      <td>0.806153</td>\n",
       "      <td>0.777427</td>\n",
       "      <td>0.737934</td>\n",
       "      <td>0.797825</td>\n",
       "      <td>0.777215</td>\n",
       "      <td>0.520686</td>\n",
       "      <td>0.463031</td>\n",
       "      <td>0.662730</td>\n",
       "      <td>0.662730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.695696</td>\n",
       "      <td>0.672138</td>\n",
       "      <td>0.581036</td>\n",
       "      <td>0.566982</td>\n",
       "      <td>0.702531</td>\n",
       "      <td>0.702556</td>\n",
       "      <td>0.816990</td>\n",
       "      <td>0.789868</td>\n",
       "      <td>0.750484</td>\n",
       "      <td>0.704539</td>\n",
       "      <td>0.786057</td>\n",
       "      <td>0.764375</td>\n",
       "      <td>0.479417</td>\n",
       "      <td>0.420304</td>\n",
       "      <td>0.649847</td>\n",
       "      <td>0.649847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.764636</td>\n",
       "      <td>0.730406</td>\n",
       "      <td>0.661859</td>\n",
       "      <td>0.647950</td>\n",
       "      <td>0.730573</td>\n",
       "      <td>0.731161</td>\n",
       "      <td>0.790620</td>\n",
       "      <td>0.815008</td>\n",
       "      <td>0.726577</td>\n",
       "      <td>0.746654</td>\n",
       "      <td>0.770840</td>\n",
       "      <td>0.779772</td>\n",
       "      <td>0.500422</td>\n",
       "      <td>0.432608</td>\n",
       "      <td>0.644836</td>\n",
       "      <td>0.644836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.718573</td>\n",
       "      <td>0.669254</td>\n",
       "      <td>0.604124</td>\n",
       "      <td>0.574181</td>\n",
       "      <td>0.717101</td>\n",
       "      <td>0.704525</td>\n",
       "      <td>0.812668</td>\n",
       "      <td>0.803064</td>\n",
       "      <td>0.743098</td>\n",
       "      <td>0.722154</td>\n",
       "      <td>0.782515</td>\n",
       "      <td>0.772435</td>\n",
       "      <td>0.445524</td>\n",
       "      <td>0.393393</td>\n",
       "      <td>0.635890</td>\n",
       "      <td>0.635890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.746797</td>\n",
       "      <td>0.698562</td>\n",
       "      <td>0.627261</td>\n",
       "      <td>0.613357</td>\n",
       "      <td>0.720118</td>\n",
       "      <td>0.712193</td>\n",
       "      <td>0.835958</td>\n",
       "      <td>0.811065</td>\n",
       "      <td>0.781110</td>\n",
       "      <td>0.734417</td>\n",
       "      <td>0.805763</td>\n",
       "      <td>0.776593</td>\n",
       "      <td>0.542872</td>\n",
       "      <td>0.442503</td>\n",
       "      <td>0.653144</td>\n",
       "      <td>0.653144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.680350</td>\n",
       "      <td>0.637008</td>\n",
       "      <td>0.608584</td>\n",
       "      <td>0.559457</td>\n",
       "      <td>0.722060</td>\n",
       "      <td>0.697137</td>\n",
       "      <td>0.831959</td>\n",
       "      <td>0.811053</td>\n",
       "      <td>0.773386</td>\n",
       "      <td>0.745494</td>\n",
       "      <td>0.804025</td>\n",
       "      <td>0.785063</td>\n",
       "      <td>0.456026</td>\n",
       "      <td>0.433163</td>\n",
       "      <td>0.654525</td>\n",
       "      <td>0.654525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.664518</td>\n",
       "      <td>0.576398</td>\n",
       "      <td>0.564026</td>\n",
       "      <td>0.518973</td>\n",
       "      <td>0.694606</td>\n",
       "      <td>0.680368</td>\n",
       "      <td>0.828790</td>\n",
       "      <td>0.789486</td>\n",
       "      <td>0.771446</td>\n",
       "      <td>0.722204</td>\n",
       "      <td>0.800065</td>\n",
       "      <td>0.771881</td>\n",
       "      <td>0.391251</td>\n",
       "      <td>0.353916</td>\n",
       "      <td>0.618405</td>\n",
       "      <td>0.618405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.710810</td>\n",
       "      <td>0.625737</td>\n",
       "      <td>0.641991</td>\n",
       "      <td>0.584657</td>\n",
       "      <td>0.726253</td>\n",
       "      <td>0.708180</td>\n",
       "      <td>0.836766</td>\n",
       "      <td>0.767657</td>\n",
       "      <td>0.780334</td>\n",
       "      <td>0.693412</td>\n",
       "      <td>0.808310</td>\n",
       "      <td>0.759356</td>\n",
       "      <td>0.455651</td>\n",
       "      <td>0.433182</td>\n",
       "      <td>0.652428</td>\n",
       "      <td>0.652428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.629094</td>\n",
       "      <td>0.566340</td>\n",
       "      <td>0.498267</td>\n",
       "      <td>0.479480</td>\n",
       "      <td>0.667766</td>\n",
       "      <td>0.661529</td>\n",
       "      <td>0.796626</td>\n",
       "      <td>0.792136</td>\n",
       "      <td>0.735875</td>\n",
       "      <td>0.735458</td>\n",
       "      <td>0.779002</td>\n",
       "      <td>0.780240</td>\n",
       "      <td>0.373686</td>\n",
       "      <td>0.337688</td>\n",
       "      <td>0.610148</td>\n",
       "      <td>0.610148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.640684</td>\n",
       "      <td>0.599909</td>\n",
       "      <td>0.551536</td>\n",
       "      <td>0.547615</td>\n",
       "      <td>0.687781</td>\n",
       "      <td>0.695987</td>\n",
       "      <td>0.808007</td>\n",
       "      <td>0.785308</td>\n",
       "      <td>0.747484</td>\n",
       "      <td>0.735733</td>\n",
       "      <td>0.782590</td>\n",
       "      <td>0.779047</td>\n",
       "      <td>0.413972</td>\n",
       "      <td>0.372399</td>\n",
       "      <td>0.628809</td>\n",
       "      <td>0.628809</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   F1_score  ClassAccuracy  model_pearsonHT29  model_pearsonA375  \\\n",
       "0       1.0            1.0           0.748481           0.673342   \n",
       "1       1.0            1.0           0.695696           0.672138   \n",
       "2       1.0            1.0           0.764636           0.730406   \n",
       "3       1.0            1.0           0.718573           0.669254   \n",
       "4       1.0            1.0           0.746797           0.698562   \n",
       "5       1.0            1.0           0.680350           0.637008   \n",
       "6       1.0            1.0           0.664518           0.576398   \n",
       "7       1.0            1.0           0.710810           0.625737   \n",
       "8       1.0            1.0           0.629094           0.566340   \n",
       "9       1.0            1.0           0.640684           0.599909   \n",
       "\n",
       "   model_spearHT29  model_spearA375  model_accHT29  model_accA375  \\\n",
       "0         0.662073         0.604682       0.732797       0.717510   \n",
       "1         0.581036         0.566982       0.702531       0.702556   \n",
       "2         0.661859         0.647950       0.730573       0.731161   \n",
       "3         0.604124         0.574181       0.717101       0.704525   \n",
       "4         0.627261         0.613357       0.720118       0.712193   \n",
       "5         0.608584         0.559457       0.722060       0.697137   \n",
       "6         0.564026         0.518973       0.694606       0.680368   \n",
       "7         0.641991         0.584657       0.726253       0.708180   \n",
       "8         0.498267         0.479480       0.667766       0.661529   \n",
       "9         0.551536         0.547615       0.687781       0.695987   \n",
       "\n",
       "   recon_pear_ht29  recon_pear_a375  recon_spear_ht29  recon_spear_a375  \\\n",
       "0         0.831326         0.806153          0.777427          0.737934   \n",
       "1         0.816990         0.789868          0.750484          0.704539   \n",
       "2         0.790620         0.815008          0.726577          0.746654   \n",
       "3         0.812668         0.803064          0.743098          0.722154   \n",
       "4         0.835958         0.811065          0.781110          0.734417   \n",
       "5         0.831959         0.811053          0.773386          0.745494   \n",
       "6         0.828790         0.789486          0.771446          0.722204   \n",
       "7         0.836766         0.767657          0.780334          0.693412   \n",
       "8         0.796626         0.792136          0.735875          0.735458   \n",
       "9         0.808007         0.785308          0.747484          0.735733   \n",
       "\n",
       "   recon_acc_ht29  recon_acc_a375  Direct_pearson  Direct_spearman  \\\n",
       "0        0.797825        0.777215        0.520686         0.463031   \n",
       "1        0.786057        0.764375        0.479417         0.420304   \n",
       "2        0.770840        0.779772        0.500422         0.432608   \n",
       "3        0.782515        0.772435        0.445524         0.393393   \n",
       "4        0.805763        0.776593        0.542872         0.442503   \n",
       "5        0.804025        0.785063        0.456026         0.433163   \n",
       "6        0.800065        0.771881        0.391251         0.353916   \n",
       "7        0.808310        0.759356        0.455651         0.433182   \n",
       "8        0.779002        0.780240        0.373686         0.337688   \n",
       "9        0.782590        0.779047        0.413972         0.372399   \n",
       "\n",
       "   DirectAcc_ht29  DirectAcc_a375  \n",
       "0        0.662730        0.662730  \n",
       "1        0.649847        0.649847  \n",
       "2        0.644836        0.644836  \n",
       "3        0.635890        0.635890  \n",
       "4        0.653144        0.653144  \n",
       "5        0.654525        0.654525  \n",
       "6        0.618405        0.618405  \n",
       "7        0.652428        0.652428  \n",
       "8        0.610148        0.610148  \n",
       "9        0.628809        0.628809  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_result = pd.DataFrame({'F1_score':valF1,'ClassAccuracy':valClassAcc,\n",
    "                          'model_pearsonHT29':valPear[:,0],'model_pearsonA375':valPear[:,1],\n",
    "                          'model_spearHT29':valSpear[:,0],'model_spearA375':valSpear[:,1],\n",
    "                          'model_accHT29':valAccuracy[:,0],'model_accA375':valAccuracy[:,1],\n",
    "                          'recon_pear_ht29':valPear_2 ,'recon_pear_a375':valPear_1,\n",
    "                          'recon_spear_ht29':valSpear_2 ,'recon_spear_a375':valSpear_1,\n",
    "                          'recon_acc_ht29':valAccuracy_2 ,'recon_acc_a375':valAccuracy_1,\n",
    "                          'Direct_pearson':valPearDirect,'Direct_spearman':valSpearDirect,\n",
    "                          'DirectAcc_ht29':valAccDirect[:,0],'DirectAcc_a375':valAccDirect[:,1]})\n",
    "df_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a0902183",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result.to_csv('../results/MI_results/landmarks_10foldvalidation_withCPA_1000ep512bs_a375_ht29_v3.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02d479df",
   "metadata": {},
   "source": [
    "## Evaluate embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a0de4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    # Network\n",
    "    encoder_1 = torch.load('../results/MI_results/models/CPA_approach/encoder_a375v3_%s.pt'%i)\n",
    "    encoder_2 = torch.load('../results/MI_results/models/CPA_approach/encoder_ht29v3_%s.pt'%i)\n",
    "    Vsp = torch.load('../results/MI_results/models/CPA_approach/Vspecies3_%s.pt'%i)\n",
    "    \n",
    "    #autoencoder_a375 = torch.load('../results/my_results/models/notvae/autoencoder_mmd_a375_%s.pt'%i)\n",
    "    #autoencoder_ht29 = torch.load('../results/my_results/models/notvae/autoencoder_mmd_ht29_%s.pt'%i)\n",
    "    \n",
    "    trainInfo_paired = pd.read_csv('../preprocessing/preprocessed_data/10fold_validation_spit/train_paired_%s.csv'%i,index_col=0)\n",
    "    trainInfo_1 = pd.read_csv('../preprocessing/preprocessed_data/10fold_validation_spit/train_a375_%s.csv'%i,index_col=0)\n",
    "    trainInfo_2 = pd.read_csv('../preprocessing/preprocessed_data/10fold_validation_spit/train_ht29_%s.csv'%i,index_col=0)\n",
    "    \n",
    "    valInfo_paired = pd.read_csv('../preprocessing/preprocessed_data/10fold_validation_spit/val_paired_%s.csv'%i,index_col=0)\n",
    "    valInfo_1 = pd.read_csv('../preprocessing/preprocessed_data/10fold_validation_spit/val_a375_%s.csv'%i,index_col=0)\n",
    "    valInfo_2 = pd.read_csv('../preprocessing/preprocessed_data/10fold_validation_spit/val_ht29_%s.csv'%i,index_col=0)\n",
    "    \n",
    "    encoder_1.eval()\n",
    "    encoder_2.eval()\n",
    "    Vsp.eval()\n",
    "    #autoencoder_a375.eval()\n",
    "    #autoencoder_ht29.eval()\n",
    "    \n",
    "    paired_val_inds = len(valInfo_paired)\n",
    "    x_1 = torch.tensor(np.concatenate((cmap.loc[valInfo_paired['sig_id.x']].values,\n",
    "                                          cmap.loc[valInfo_1.sig_id].values))).float().to(device)\n",
    "    x_2 = torch.tensor(np.concatenate((cmap.loc[valInfo_paired['sig_id.y']].values,\n",
    "                                          cmap.loc[valInfo_2.sig_id].values))).float().to(device)\n",
    "    \n",
    "    z_base_1 = encoder_1(x_1)\n",
    "    z_base_2 = encoder_2(x_2)\n",
    "    \n",
    "    z_species_1 = torch.cat((torch.ones(x_1.shape[0],1),\n",
    "                             torch.zeros(x_1.shape[0],1)),1).to(device)\n",
    "    z_species_2 = torch.cat((torch.zeros(x_2.shape[0],1),\n",
    "                             torch.ones(x_2.shape[0],1)),1).to(device)\n",
    "    \n",
    "    z_latent_1 = Vsp(z_base_1,z_species_1)\n",
    "    z_latent_2 = Vsp(z_base_2,z_species_2)\n",
    "    \n",
    "    valEmbs_1 = pd.DataFrame(z_latent_1.detach().cpu().numpy())\n",
    "    valEmbs_1.index = np.concatenate((valInfo_paired['sig_id.x'].values,valInfo_1.sig_id.values))\n",
    "    valEmbs_2 = pd.DataFrame(z_latent_2.detach().cpu().numpy())\n",
    "    valEmbs_2.index = np.concatenate((valInfo_paired['sig_id.y'].values,valInfo_2.sig_id.values))\n",
    "    \n",
    "    valEmbs_base_1 = pd.DataFrame(z_base_1.detach().cpu().numpy())\n",
    "    valEmbs_base_1.index = np.concatenate((valInfo_paired['sig_id.x'].values,valInfo_1.sig_id.values))\n",
    "    valEmbs_base_2 = pd.DataFrame(z_base_2.detach().cpu().numpy())\n",
    "    valEmbs_base_2.index = np.concatenate((valInfo_paired['sig_id.y'].values,valInfo_2.sig_id.values))\n",
    "    \n",
    "    # Training embeddigns\n",
    "    paired_inds = len(trainInfo_paired)\n",
    "    x_1 = torch.tensor(np.concatenate((cmap.loc[trainInfo_paired['sig_id.x']].values,\n",
    "                                          cmap.loc[trainInfo_1.sig_id].values))).float().to(device)\n",
    "    x_2 = torch.tensor(np.concatenate((cmap.loc[trainInfo_paired['sig_id.y']].values,\n",
    "                                          cmap.loc[trainInfo_2.sig_id].values))).float().to(device)\n",
    "    z_base_1 = encoder_1(x_1)\n",
    "    z_base_2 = encoder_2(x_2)\n",
    "    \n",
    "    z_species_1 = torch.cat((torch.ones(x_1.shape[0],1),\n",
    "                             torch.zeros(x_1.shape[0],1)),1).to(device)\n",
    "    z_species_2 = torch.cat((torch.zeros(x_2.shape[0],1),\n",
    "                             torch.ones(x_2.shape[0],1)),1).to(device)\n",
    "    \n",
    "    z_latent_1 = Vsp(z_base_1,z_species_1)\n",
    "    z_latent_2 = Vsp(z_base_2,z_species_2)\n",
    "    \n",
    "    trainEmbs_1 = pd.DataFrame(z_latent_1.detach().cpu().numpy())\n",
    "    trainEmbs_1.index = np.concatenate((trainInfo_paired['sig_id.x'].values,trainInfo_1.sig_id.values))\n",
    "    trainEmbs_2 = pd.DataFrame(z_latent_2.detach().cpu().numpy())\n",
    "    trainEmbs_2.index = np.concatenate((trainInfo_paired['sig_id.y'].values,trainInfo_2.sig_id.values))\n",
    "    \n",
    "    trainEmbs_base_1 = pd.DataFrame(z_base_1.detach().cpu().numpy())\n",
    "    trainEmbs_base_1.index = np.concatenate((trainInfo_paired['sig_id.x'].values,trainInfo_1.sig_id.values))\n",
    "    trainEmbs_base_2 = pd.DataFrame(z_base_2.detach().cpu().numpy())\n",
    "    trainEmbs_base_2.index = np.concatenate((trainInfo_paired['sig_id.y'].values,trainInfo_2.sig_id.values))\n",
    "    \n",
    "    valEmbs_1.to_csv('../results/MI_results/embs/CPA_approach/validation/valEmbsv3_%s_a375.csv'%i)\n",
    "    valEmbs_2.to_csv('../results/MI_results/embs/CPA_approach/validation/valEmbsv3_%s_ht29.csv'%i)\n",
    "    trainEmbs_1.to_csv('../results/MI_results/embs/CPA_approach/train/trainEmbsv3_%s_a375.csv'%i)\n",
    "    trainEmbs_2.to_csv('../results/MI_results/embs/CPA_approach/train/trainEmbsv3_%s_ht29.csv'%i)\n",
    "    \n",
    "    valEmbs_base_1.to_csv('../results/MI_results/embs/CPA_approach/validation/valEmbs_basev3_%s_a375.csv'%i)\n",
    "    valEmbs_base_2.to_csv('../results/MI_results/embs/CPA_approach/validation/valEmbs_basev3_%s_ht29.csv'%i)\n",
    "    trainEmbs_base_1.to_csv('../results/MI_results/embs/CPA_approach/train/trainEmbs_basev3_%s_a375.csv'%i)\n",
    "    trainEmbs_base_2.to_csv('../results/MI_results/embs/CPA_approach/train/trainEmbs_basev3_%s_ht29.csv'%i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f02eb3b",
   "metadata": {},
   "source": [
    "## Train on all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7db78a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NUM_EPOCHS = 1000\n",
    "# #bs = 512\n",
    "# bs_a375 = 178\n",
    "# bs_ht29 = 154\n",
    "# bs_paired = 90\n",
    "# bs_paired_ctrl = 1000\n",
    "# beta=1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "420146b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# valR2 = []\n",
    "# valPear = []\n",
    "# valMSE =[]\n",
    "# valSpear = []\n",
    "# valAccuracy = []\n",
    "\n",
    "# valPear_ctrl = []\n",
    "# valSpear_ctrl = []\n",
    "# valAccuracy_ctrl = []\n",
    "\n",
    "\n",
    "# valPearDirect = []\n",
    "# valSpearDirect = []\n",
    "# valAccDirect = []\n",
    "\n",
    "# valR2_a375 = []\n",
    "# valPear_a375 = []\n",
    "# valMSE_a375 =[]\n",
    "# valSpear_a375 = []\n",
    "# valAccuracy_a375 = []\n",
    "# valPear_ctrl_a375 = []\n",
    "# valSpear_ctrl_a375 = []\n",
    "# valAccuracy_ctrl_a375 = []\n",
    "\n",
    "# valR2_ht29 = []\n",
    "# valPear_ht29 = []\n",
    "# valMSE_ht29 =[]\n",
    "# valSpear_ht29 = []\n",
    "# valAccuracy_ht29 = []\n",
    "# valPear_ctrl_ht29 = []\n",
    "# valSpear_ctrl_ht29 = []\n",
    "# valAccuracy_ctrl_ht29 = []\n",
    "\n",
    "# crossCorrelation = []\n",
    "\n",
    "# #model_mi.eval()\n",
    "# #trainLoss = []\n",
    "# for i in range(1):\n",
    "#     # Network\n",
    "#     decoder_a375 = Decoder(292,[384,640],gene_size,dropRate=0.2, activation=torch.nn.ELU()).to(device)\n",
    "#     decoder_ht29 = Decoder(292,[384,640],gene_size,dropRate=0.2, activation=torch.nn.ELU()).to(device)\n",
    "    \n",
    "#     # Infomax\n",
    "#     #master_encoder = SimpleEncoder(gene_size,[640,384],292,dropRate=0.1, activation=torch.nn.ELU())#.to(device)\n",
    "#     encoder_a375 = SimpleEncoder(gene_size,[640,384],292,dropRate=0.1, activation=torch.nn.ELU()).to(device)\n",
    "#     encoder_ht29 = SimpleEncoder(gene_size,[640,384],292,dropRate=0.1, activation=torch.nn.ELU()).to(device)\n",
    "#     prior_d = PriorDiscriminator(292).to(device)\n",
    "#     local_d = LocalDiscriminator(292,292).to(device)\n",
    "#     #model = EmbInfomax(292,master_encoder)\n",
    "    \n",
    "#     #model = MultiEncInfomax(292,[encoder_a375,encoder_ht29])    \n",
    "#     #model = model.to(device)\n",
    "    \n",
    "#     #encoder_a375.load_state_dict(model_mi.encoder.state_dict())\n",
    "#     #encoder_ht29.load_state_dict(model_mi.encoder.state_dict())\n",
    "#     #model.encoders[0].load_state_dict(model_mi.encoder.state_dict())\n",
    "#     #model.encoders[1].load_state_dict(model_mi.encoder.state_dict())\n",
    "#     #local_d.load_state_dict(model_mi.local_d.state_dict())\n",
    "#     #prior_d.load_state_dict(model_mi.prior_d.state_dict())\n",
    "#     #model.load_state_dict(model_mi.state_dict())\n",
    "#     #master_encoder.load_state_dict(model_mi.encoder.state_dict())\n",
    "    \n",
    "#     #autoencoder_a375.load_state_dict(master_autoencoder.state_dict())\n",
    "#     #autoencoder_ht29.load_state_dict(master_autoencoder.state_dict())\n",
    "#     #encoder_a375 = Encoder(gene_size,[4096,2048],1024,dropRate=0.1, activation=torch.nn.ELU())\n",
    "#     #decoder_a375 = Decoder(1024,[2048,4096],gene_size,dropRate=0.1, activation=torch.nn.ELU())\n",
    "#     #autoencoder_a375 = VAE(encoder_a375,decoder_a375,device).to(device)\n",
    "    \n",
    "#     #encoder_ht29 = Encoder(gene_size,[4096,2048],1024,dropRate=0.1, activation=torch.nn.ELU())\n",
    "#     #decoder_ht29 = Decoder(1024,[2048,4096],gene_size,dropRate=0.1, activation=torch.nn.ELU())\n",
    "#     #autoencoder_ht29 = VAE(encoder_ht29,decoder_ht29,device).to(device)\n",
    "    \n",
    "#     trainInfo_paired = pd.read_csv('../preprocessing/preprocessed_data/10fold_validation_spit/alldata/paired_pc3_ha1e.csv',index_col=0)\n",
    "#     trainInfo_a375 = pd.read_csv('../preprocessing/preprocessed_data/10fold_validation_spit/alldata/pc3_unpaired.csv',index_col=0)\n",
    "#     trainInfo_ht29 = pd.read_csv('../preprocessing/preprocessed_data/10fold_validation_spit/alldata/ha1e_unpaired.csv',index_col=0)\n",
    "    \n",
    "    \n",
    "    \n",
    "#     N_paired = len(trainInfo_paired)\n",
    "#     N_a375 = len(trainInfo_a375)\n",
    "#     N_ht29 = len(trainInfo_ht29)\n",
    "#     N = N_a375\n",
    "#     if N_ht29>N:\n",
    "#         N=N_ht29\n",
    "    \n",
    "#     #cmap_train = cmap.loc[trainInfo.sig_id,:]\n",
    "#     #cmap_val = cmap.loc[valInfo.sig_id,:]\n",
    "#     #xtrain = torch.tensor(cmap_train.values).float().to(device)\n",
    "#     #xval = torch.tensor(cmap_val.values).float().to(device)\n",
    "    \n",
    "#     #allParams = list(master_encoder.parameters()) +list(decoder_a375.parameters()) + list(decoder_ht29.parameters())\n",
    "#     #allParams = list(model.parameters())+list(decoder_a375.parameters()) +list(decoder_ht29.parameters())\n",
    "#     allParams = list(decoder_a375.parameters()) +list(decoder_ht29.parameters())\n",
    "#     allParams = allParams + list(encoder_a375.parameters()) +list(encoder_ht29.parameters())\n",
    "#     allParams = allParams + list(prior_d.parameters()) + list(local_d.parameters())\n",
    "#     optimizer = torch.optim.Adam(allParams, lr= 0.001, weight_decay=0)\n",
    "#     scheduler = torch.optim.lr_scheduler.StepLR(optimizer,step_size=300,gamma=0.8)\n",
    "    \n",
    "#     for e in range(0, NUM_EPOCHS):\n",
    "#         decoder_a375.train()\n",
    "#         decoder_ht29.train()\n",
    "#         encoder_a375.train()\n",
    "#         encoder_ht29.train()\n",
    "#         prior_d.train()\n",
    "#         local_d.train()\n",
    "#         #model.train()\n",
    "#         #master_encoder.train()\n",
    "        \n",
    "#         trainloader_a375 = getSamples(N_a375, bs_a375)\n",
    "#         len_a375 = len(trainloader_a375)\n",
    "#         trainloader_ht29 = getSamples(N_ht29, bs_ht29)\n",
    "#         len_ht29 = len(trainloader_ht29)\n",
    "#         trainloader_paired = getSamples(N_paired, bs_paired)\n",
    "#         len_paired = len(trainloader_paired)\n",
    "\n",
    "#         lens = [len_a375,len_ht29,len_paired]\n",
    "#         maxLen = np.max(lens)\n",
    "\n",
    "#         if maxLen>lens[0]:\n",
    "#             while maxLen>len(trainloader_a375):\n",
    "#                 trainloader_suppl = getSamples(N_a375, bs_a375)\n",
    "#                 trainloader_a375 = trainloader_a375 + trainloader_suppl\n",
    "        \n",
    "#         if maxLen>lens[1]:\n",
    "#             while maxLen>len(trainloader_ht29):\n",
    "#                 trainloader_suppl = getSamples(N_ht29, bs_ht29)\n",
    "#                 trainloader_ht29 = trainloader_ht29 + trainloader_suppl\n",
    "        \n",
    "#         if maxLen>lens[2]:\n",
    "#             while maxLen>len(trainloader_paired):\n",
    "#                 trainloader_suppl = getSamples(N_paired, bs_paired)\n",
    "#                 trainloader_paired = trainloader_paired + trainloader_suppl\n",
    "        \n",
    "#         for j in range(maxLen):\n",
    "#             dataIndex_a375 = trainloader_a375[j]\n",
    "#             dataIndex_ht29 = trainloader_ht29[j]\n",
    "#             dataIndex_paired = trainloader_paired[j]\n",
    "            \n",
    "#             df_pairs = trainInfo_paired.iloc[dataIndex_paired,:]\n",
    "#             df_a375 = trainInfo_a375.iloc[dataIndex_a375,:]\n",
    "#             df_ht29 = trainInfo_ht29.iloc[dataIndex_ht29,:]\n",
    "#             paired_inds = len(df_pairs)\n",
    "            \n",
    "#             a375_inds = len(df_a375) + paired_inds\n",
    "            \n",
    "            \n",
    "#             X_a375 = torch.tensor(np.concatenate((cmap.loc[df_pairs['sig_id.x']].values,\n",
    "#                                                  cmap.loc[df_a375.sig_id].values))).float().to(device)\n",
    "#             X_ht29 = torch.tensor(np.concatenate((cmap.loc[df_pairs['sig_id.y']].values,\n",
    "#                                                  cmap.loc[df_ht29.sig_id].values))).float().to(device)\n",
    "            \n",
    "#             conditions = np.concatenate((df_pairs.conditionId.values,\n",
    "#                                          df_a375.conditionId.values,\n",
    "#                                          df_pairs.conditionId.values,\n",
    "#                                          df_ht29.conditionId.values))\n",
    "            \n",
    "#             size = conditions.size\n",
    "#             conditions = conditions.reshape(size,1)\n",
    "#             conditions = conditions == conditions.transpose()\n",
    "#             conditions = conditions*1\n",
    "#             mask = torch.tensor(conditions).to(device).detach()\n",
    "#             pos_mask = mask\n",
    "#             neg_mask = 1 - mask\n",
    "#             log_2 = math.log(2.)\n",
    "#             optimizer.zero_grad()\n",
    "            \n",
    "#             z_a375 = encoder_a375(X_a375)\n",
    "#             z_ht29 = encoder_ht29(X_ht29)\n",
    "            \n",
    "#             z_un = local_d(torch.cat((z_a375, z_ht29), 0))\n",
    "#             res_un = torch.matmul(z_un, z_un.t())\n",
    "            \n",
    "#             y_pred_a375 = decoder_a375(z_a375)\n",
    "#             fitLoss_a375 = torch.mean(torch.sum((y_pred_a375 - X_a375)**2,dim=1))\n",
    "#             L2Loss_a375 = decoder_a375.L2Regularization(0.01) + encoder_a375.L2Regularization(0.01)\n",
    "#             loss_a375 = fitLoss_a375 + L2Loss_a375\n",
    "            \n",
    "#             y_pred_ht29 = decoder_ht29(z_ht29)\n",
    "#             fitLoss_ht29 = torch.mean(torch.sum((y_pred_ht29 - X_ht29)**2,dim=1))\n",
    "#             L2Loss_ht29 = decoder_ht29.L2Regularization(0.01) + encoder_ht29.L2Regularization(0.01)\n",
    "#             loss_ht29 = fitLoss_ht29 + L2Loss_ht29\n",
    "            \n",
    "#             #modelL2 = model.L2Regularization(0.01)\n",
    "#             silimalityLoss = torch.mean(torch.sum((z_a375[0:paired_inds ,:] - z_ht29[0:paired_inds,:])**2,dim=-1))\n",
    "            \n",
    "#             p_samples = res_un * pos_mask\n",
    "#             q_samples = res_un * neg_mask\n",
    "\n",
    "#             Ep = log_2 - F.softplus(- p_samples)\n",
    "#             Eq = F.softplus(-q_samples) + q_samples - log_2\n",
    "\n",
    "#             Ep = (Ep * pos_mask).sum() / pos_mask.sum()\n",
    "#             Eq = (Eq * neg_mask).sum() / neg_mask.sum()\n",
    "#             mi_loss = Eq - Ep\n",
    "\n",
    "#             prior = torch.rand_like(torch.cat((z_a375, z_ht29), 0))\n",
    "\n",
    "#             term_a = torch.log(prior_d(prior)).mean()\n",
    "#             term_b = torch.log(1.0 - prior_d(torch.cat((z_a375, z_ht29), 0))).mean()\n",
    "#             prior_loss = -(term_a + term_b) * beta\n",
    "            \n",
    "#             #Direct translate\n",
    "#             #x_a375_translated = decoder_a375(z_ht29[0:paired_inds,:])\n",
    "#             #x_ht29_translated = decoder_ht29(z_a375[0:paired_inds,:])\n",
    "#             #directLoss = torch.mean(torch.sum((x_a375_translated - X_a375[0:paired_inds,:])**2,dim=1)) + torch.mean(torch.sum((x_ht29_translated - X_ht29[0:paired_inds,:])**2,dim=1))\n",
    "        \n",
    "#             #ZeroEmbPenalnty = 1e-7*torch.sum(1 - torch.abs(z_latent))\n",
    "#             #LossCtrl = torch.mean(torch.sum(torch.abs(z_a375[paired_inds:paired_ctrl_inds,:]),dim=-1))\n",
    "#             #LossCtrl = LossCtrl + torch.mean(torch.sum(torch.abs(z_ht29[paired_inds:paired_ctrl_inds,:]),dim=-1))\n",
    "#             loss = loss_a375 + loss_ht29 + mi_loss + prior_loss + silimalityLoss #+ LossCtrl#modelL2 \n",
    "#             #loss = loss_a375 + loss_ht29 + directLoss\n",
    "\n",
    "#             loss.backward()\n",
    "\n",
    "#             optimizer.step()\n",
    "        \n",
    "#             pearson_a375 = pearson_r(y_pred_a375.detach().flatten(), X_a375.detach().flatten())\n",
    "#             r2_a375 = r_square(y_pred_a375.detach().flatten(), X_a375.detach().flatten())\n",
    "#             mse_a375 = torch.mean(torch.mean((y_pred_a375.detach() - X_a375.detach())**2,dim=1))\n",
    "        \n",
    "#             pearson_ht29 = pearson_r(y_pred_ht29.detach().flatten(), X_ht29.detach().flatten())\n",
    "#             r2_ht29 = r_square(y_pred_ht29.detach().flatten(), X_ht29.detach().flatten())\n",
    "#             mse_ht29 = torch.mean(torch.mean((y_pred_ht29.detach() - X_ht29.detach())**2,dim=1))            \n",
    "            \n",
    "#         scheduler.step()\n",
    "#         outString = 'Split {:.0f}: Epoch={:.0f}/{:.0f}'.format(i+1,e+1,NUM_EPOCHS)\n",
    "#         outString += ', r2_a375={:.4f}'.format(r2_a375.item())\n",
    "#         outString += ', pearson_a375={:.4f}'.format(pearson_a375.item())\n",
    "#         outString += ', MSE_a375={:.4f}'.format(mse_a375.item())\n",
    "#         outString += ', r2_ht29={:.4f}'.format(r2_ht29.item())\n",
    "#         outString += ', pearson_ht29={:.4f}'.format(pearson_ht29.item())\n",
    "#         outString += ', MSE_ht29={:.4f}'.format(mse_ht29.item())\n",
    "#         outString += ', MI Loss={:.4f}'.format(mi_loss.item())\n",
    "#         outString += ', Prior Loss={:.4f}'.format(prior_loss.item())\n",
    "#         outString += ', loss={:.4f}'.format(loss.item())\n",
    "#         if (e%250==0):\n",
    "#             print(outString)\n",
    "#     print(outString)\n",
    "#     #trainLoss.append(splitLoss)\n",
    "#     decoder_a375.eval()\n",
    "#     decoder_ht29.eval()\n",
    "#     encoder_a375.eval()\n",
    "#     encoder_ht29.eval()\n",
    "#     prior_d.eval()\n",
    "#     local_d.eval()\n",
    "    \n",
    "#     torch.save(decoder_a375,'../results/MI_results/models/AllData_Model/decoder_pc3.pt')\n",
    "#     torch.save(decoder_ht29,'../results/MI_results/models/AllData_Model/decoder_ha1e.pt')\n",
    "#     torch.save(prior_d,'../results/MI_results/models/AllData_Model/priorDiscr.pt')\n",
    "#     torch.save(local_d,'../results/MI_results/models/AllData_Model/localDiscr.pt')\n",
    "#     torch.save(encoder_a375,'../results/MI_results/models/AllData_Model/encoder_pc3.pt')\n",
    "#     torch.save(encoder_ht29,'../results/MI_results/models/AllData_Model/encoder_ha1e.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5eea5fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
